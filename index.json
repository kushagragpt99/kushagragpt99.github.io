[{"authors":["Kushagra"],"categories":null,"content":"I am a final year undergraduate in the Department of Mathematics and Statistics, IIT Kanpur. I am genuinely interested in maths, with statistics being my newfound passion. Recently, I\u0026rsquo;ve been working on Bayesian statistics, and its power to simplify problems significantly continues to amaze me (given you have the right belief!). I am currently working with Prof. Dootika Vats on output analysis of multiple Markov chains.\nI\u0026rsquo;ve also worked extensively on deep learning in college from a research as well as industrial perspective. My recent experiences on developing trading strategies for a quantitative trading firm introduced me to the power and wide applicability of statistics in practical problems. I am always looking for opportunities to work on interesting and challenging problems to keep my brain churning.\nI have explored the field of Computer Architecture in the past few months. The aggressive hardware optimizations employed in current processors interest me. I'm working with Biswa in CAR3S Group to play around and see if they can be exploited by an attacker.\nIf you're interested, I've found a Medium to present my experiences in words. I'm also in the process of compiling a fairly comprehensive set of resources to introduce interested folks to Computer Systems, the details of which are available here: Systems Reading Group -- ","date":1578096000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1578096000,"objectID":"0147088b0bad3f329b27707805a8a018","permalink":"https://kushagragpt99.github.io/authors/kushagra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kushagra/","section":"authors","summary":"I am a final year undergraduate in the Department of Mathematics and Statistics, IIT Kanpur. I am genuinely interested in maths, with statistics being my newfound passion. Recently, I\u0026rsquo;ve been working on Bayesian statistics, and its power to simplify problems significantly continues to amaze me (given you have the right belief!","tags":null,"title":"","type":"authors"},{"authors":["Kushagra Gupta, Dootika Vats"],"categories":null,"content":"","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"9114e0c8b74b3dd9e11e5171294c75fe","permalink":"https://kushagragpt99.github.io/publication/preprints/2020-replicated-batch-means/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/publication/preprints/2020-replicated-batch-means/","section":"publication","summary":"We propose a multivariate replicated batch means (RBM) estimator that utilizes information across multiple chains in order to estimate the asymptotic covariance matrix.","tags":["replicated batch means","MCMC","Output Analysis","Operations Research"],"title":"Estimating Monte Carlo variance from multiple Markov chains","type":"publication"},{"authors":[],"categories":[],"content":"A few weeks ago, I wrapped up teaching tidymodels for the third time. We say third time\u0026rsquo;s the charm, right? Even during a global pandemic? I don\u0026rsquo;t know, but this time around was fun in new ways and hard in new ways, so I wanted to take the time to write some thoughts down about how it went.\nBut why? Why teach tidymodels virtually? I\u0026rsquo;ve already taught it as a 2-day workshop at rstudio::conf(2020). I had already agreed to teach intro to machine learning with tidymodels as a full-day workshop for the Cascadia R Conf (which unfortunately was cancelled due to COVID), and the R / Medicine conference (still on, and 100% virtual!).\nI had three main goals:\n First, I wanted to do a good job for the R / Medicine conference workshop in August. This seemed like an interesting teaching challenge. One of the best compliments I\u0026rsquo;ve ever gotten from a colleague is that I \u0026ldquo;teach with heart\u0026rdquo; ‚Äî so the challenge was, can a virtual workshop have a ‚ù§Ô∏è?    Second, given current events, I saw an opportunity to document a good system (tooling + pedagogy + logistics) around virtual workshops. I don\u0026rsquo;t see them going away anytime soon. My colleague, Greg Wilson, had also been giving our RStudio certified instructor workshops virtually for over a year, so I trusted that he could help me navigate.\n  Third, help out the R / Medicine conference organizers. My partner-in-crime for this particular workshop, Dr. Stephan Kadauke, wanted to join me for some reconnaissance work. He is one of the conference organizers and will be leading his own workshop there, so he wanted to test out the tooling and the pedagogy too.\n  So we thought, three birds, one stone: I get to pilot a much shorter version of my conf workshop materials, we get to test doing it virtually, and the R / Medicine conference organizers learn how this could all work in August.\nHow we planned it Starting out, I knew I had two main hurdles:\n Shaving two days of workshop content into X days Logistics (like figuring out what X should be!)  The first decision based on conversations with Greg was to offer the workshop across two half-days. This is how the RStudio instructor training is timed, and it works well because there are few people who can (or want to) be tied up for a full day, especially if under stay-at-home orders. We asked Stephan if his group at the Children\u0026rsquo;s Hospital of Philadelphia (CHOP) would be up for two 4-hour sessions. Stephan\u0026rsquo;s feedback was yes, but given that these folks at CHOP typically have weekly schedules, having two consecutive days would not work. So we opted for:\n Two 4-hour sessions that were\u0026hellip; exactly one week apart.  I\u0026rsquo;m in Oregon, they were in Philly, so we decided to start at 9am my time (1pm for them) and wrap up at 1pm my time (5pm for them). After making this call, and again with Greg\u0026rsquo;s sage advice to take a break every hour, I started working on a rough schedule. I had about 8 hours total to work with; about half of my conf workshop. We decided on:\n 50 minute chunks, and 10 minute breaks at 10 till the hour every single hour.  I didn\u0026rsquo;t try to make sure my materials for each session filled exactly 50 minutes. Instead, I promised the group that I would break wherever I was at the same time, and after each break we just picked up where left off.\nHere was the new topic outline:\nDay 1   Session 00: Intro (include a tooling tour- orient to Google doc + Zoom)  Session 01: Build a model (mainly parsnip package)  Session 02: Resample a model (add rsample package, plus tune package for fit_resamples())  Session 03: Build a better training set (add recipes and workflows packages)  Day 2   Session 04: Build an ensemble model (back to parsnip, now with model arguments)  Session 05: Tune a model (heavy tune package)  Session 06: The Great Model-Off (a Kaggle-like group activity)  The final decision was about tooling. Luckily, Greg had advice here too. We went with:\n Google doc as the \u0026ldquo;home page\u0026rdquo; + chat (no workshop website!) RStudio Cloud for all exercises Zoom for video  Pre-workshop launch list  Make shared Google doc and prepopulate with: Zoom link, RStudio Cloud link, bulleted list of participant names (be sure to make this editable for anyone!) Email everyone with a Google calendar invite that includes the Zoom link and a link to a shared Google doc (be sure to make this editable for anyone!) Ask everyone to fill in their 2-sentence bios ahead of time to ensure that you\u0026rsquo;ve done this correctly!  The Zoom link and the Google doc link should be the only links that attendees see ahead of time. Then, the Google doc is the one true source for everything. Too many links at first leads to confusion later. I also prepopulated the doc with my session outline with HTML links to each session\u0026rsquo;s slide deck.\nActual launch In Zoom, I set it up to mute everyone as they joined. We started with an orientation of the tools, the schedule, and the general plan for how they would work and interact with each other. We tried to keep questions per session in the Google doc, which my TA Stephan fielded in real time, which now is a great resource for me as I prepare for this again in August.\nWhat I should have done:\n Have a Code of Conduct. I will next time. We didn\u0026rsquo;t have any issues, but what came up later was that I asked participants to turn on their cameras. I wished that part of my Code of Conduct was that we would not videotape or take screenshots at any point to protect the privacy of all participants. Asked folks to make sure they had an updated version of Zoom. In particular, some newer security issues have been addressed recently, so this is nice for everyone to take advantage of. Plus the interface looked different for some. Locked down the Zoom room. After a few minutes of starting, it was super distracting to have late arrivals who kept sending me personal chats asking for the links. In addition to locking down after the first 10-15 minutes, I also should have assigned my TA as a co-host, so that he could have helped me manage that. We did this on day 2 and it worked great.  How did it go? So, I\u0026rsquo;m not going to lie here. The first session of teaching spooked me a bit. This was because all attendees left their videos off and I felt like:\nIt was eerie to teach to a silent void. On day 2, I asked for two volunteers to turn on their cameras for a single 50 minute block each because it really helped me to see faces. Participants probably kept their own view as \u0026ldquo;speaker only\u0026rdquo;, but for me it really helped to be able to have some human feedback, even if they were muted. Much love to the head nodders out there. This conversation on day 2 broke my heart though, because several participants indicated they felt they couldn\u0026rsquo;t turn on their cameras because they had young children at home. It is hard.\nInterestingly, the participants didn\u0026rsquo;t sense my discomfort at all. In fact, I heard from several that it was nice to see me up close and so personal. It actually felt more personal than a large in-person workshop, to my surprise.\nOn day 1, I started by asking participants to use Zoom reactions (like thumbs up) to answer questions, give me progress updates, etc. I ended up retiring this- it was distracting and the reactions disappear so it wasn\u0026rsquo;t actually useful. Instead, I asked people to use the Zoom chat to indicate \u0026ldquo;done\u0026rdquo; or give quick one-word answers (a or b, yes or no).\nStephan also had a great idea for the breaks. On day 2, I started using Garrick Aden-Buie\u0026rsquo;s countdown app to show the 10 minute break countdown full-screen. I used the hosted version here.\nWhat can I do better? Logistically, I got really frustrated because I kept losing my Zoom meeting controls. Later I found out about this accessibility setting, which would have helped!\nMore substantively, as I mentioned, I would have a Code of Conduct at the very beginning. I also think virtual workshops offer a unique opportunity to include some more creative exercise types. Here are a few I brainstormed with Greg Wilson after the fact‚Äîexpect to see these at R / Medicine if you attend with me!\n  Spot the bug- do in groups\n  Unscramble code\n  Predict what is going to happen\n  Fill in the blank with the flair package\n  Verdict Transitioning from primarily teaching in person to teaching virtually is hard, and I\u0026rsquo;m in awe of all the instructors I know who have had to do this on very short notice. But, can it be done with heart? Yes, I think so ‚ù§Ô∏è\nThanks Thanks to the participants, who were the loveliest guinea pigs. It is a hard time to learn and a hard time to find time. I appreciate that you took time out of your lives to spend 8 hours with me.\nThanks also to Greg Wilson for his support, and Stephan Kadauke for being an awesome co-pilot. Extra special thanks to Desir√©e De Leon, who has the biggest heart of all. Knowing that I didn\u0026rsquo;t have time for creativity with my slides, she surprised me with the most beautiful xaringan slide deck theme based on tidymodels.org. I merged in her PR with glee and delight, I know the participants felt those same emotions too (while learning about machine learning, no less!) üåº\n      Feedback If you are curious, here is some of the feedback I collected at the bottom of our Google doc:\nTwo half days?  worked well for me. A full day would be tough. Worked well for me also and I also think having all in one day would be a lot. worked well. worked well for me too.  Separated by one week?  seemed fine. I liked this! good. Enough time to digest the previous session yes.  Pace: too fast, too slow?  good pace I liked the pace. It felt like we covered a lot of ground quickly, but also like we have great resources to come back to for refreshing on what we learned. good pace, except at ‚Äúdata leakage‚Äù. I still have hard time to understand that part. good pace.  Scope: too small, too big?  nice scope Nice scope very nice and practical. nice scope.  Timing in 50 min chunks: too many or too few breaks?  The 50 min chunks were great! I thought perfect length. Perfect break up. And the 10 min breaks gave enough time to make tea, grab a snack, etc. I am fine with that. works well.  Timed code exercises: too easy, too hard?  good Neither, I thought they were appropriate good. good.  In-between homework/reading?  maybe some light reading or practice In theory I think I would have liked some ‚Äòhomework‚Äô, but the week turned out to be so busy that I don‚Äôt know if I would‚Äôve made time to complete it before the workshop. will be helpful to give some reading about different model descriptions in-between.  Final take-home project (+/- feedback)?  optional I think this could be fun if we had access to a few different datasets to choose from. With maybe some pointers on what to look out for as we explore the data. Is it unbalanced? Could any two variables be collinear? Etc. i would vote for take-home project to play with.  Zoom: video on/off?  I find video distracting while working/listening/learning. I like being able to see the presenter/speaker‚Äôs video. I tend to choose ‚ÄúSpeaker View‚Äù so that I don‚Äôt see all the participant videos (which I would find distracting). i am fine with either. I know, from the speaker‚Äôs perspective, it will be good to have video on. I think it was nice and respectful to ask volunteers to turn video on part time! (Kudos to Stephan for having it on the whole time!)  Google doc as our ‚Äúhome page‚Äù and ‚Äúdiscussion forum‚Äù?  Great! Yes! I thought it was a really helpful tool and ‚Äòhomebase‚Äô to come to for all of the things (slides, R Studio Cloud project, etc.) very good! Much easier to track. works!  Rstudio.cloud?  pretty good except for crashing at end. ditto. I really enjoyed using R Studio Cloud. It makes it easier when I don‚Äôt have to worry about pre-installing packages or updating my version of R in order to go into a workshop. Having everything already set up for you in there is super convenient. I like Rstudio cloud generally. Just not suitable for complicated model training.  ","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"e55fbb1e1fbfd9c7d301525253850c35","permalink":"https://kushagragpt99.github.io/post/2020-06-02-tidymodels-virtually/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/post/2020-06-02-tidymodels-virtually/","section":"post","summary":"Some reflections on my third time teaching tidymodels for machine learning, and doing it virtually.","tags":["rmarkdown"],"title":"How I Taught Tidymodels, Virtually","type":"post"},{"authors":[],"categories":[],"content":"  Ah, R Markdown. I love teaching R Markdown. But this was not a love at first sight story. When I first started teaching with R at Oregon Health \u0026amp; Science University many moons ago, my students used R Markdown for their homework assignments and in-class labs. This is a great way to start for beginners, to be honest- just use it! I found that most people are not very disoriented by a file that mixes text and code; a concern I had at first and one that I‚Äôve heard voiced by other educators. It seems natural to use if it is your first exposure to all the data science things, especially if you‚Äôve never used an R script.\nI did notice, though, that over the course of even just a few weeks, many of my students would reach unbearable levels of R Markdown curiosity. I would see signs of growing pains in their assignments, like how can I turn off these messages and warnings when I load packages? This is so long- how can I make it easier to find things? How can I make it look better?\nFor all learners, there comes an uncomfortable point where you simply cannot just continue to use this awesome tool without learning how to use it. And I realized when I was teaching that ultimately it was unfair of me to assume that they would just learn an entire ecosystem while I stood idly by, clutching the secrets of being an ‚ÄúR Markdown Whisperer‚Äù close to my chest.\nLike, why can‚Äôt you just find the pi√±ata, kid? But more importantly, what happens when you can‚Äôt? I think the risk is that learners either won‚Äôt like or won‚Äôt appreciate a tool that I genuinely think would make their working lives better. If I believed it was worth their time to use it, it was worth my time to teach it.\nSo like any good educator, I evolved. I decided that the next year, I would teach R Markdown. And I did. I went from:\n ‚ÄúJust use it‚Äù\n To:\n ‚ÄúJust tell them how to use it already‚Äù\n I waited until a few weeks into a quarter, and I spent a full lab period walking students through ‚ÄúHey- this bit where you put the title of each assignment and your name? That‚Äôs called YAML! Here is what it does‚Ä¶‚Äù I showed them how to use bootswatch themes, make a floating table of contents, control their output using the power of knitr code chunks and chunk options.\nGuess what happened? The next batch of assignment my TA team and I received were‚Ä¶stunning. Creative. Beautiful. Thoughtful and thought-provoking. Students had taken care with their markdown headers, they played with flatly/yeti/united, they cared about not printing the giant output of a code chunk. It was a thing of beauty. I remember one of my beloved TAs, Grace Lawley, commenting that one star student‚Äôs homework ‚Äúbrought tears to her eyes‚Äù (and yes, I trained Grace too as she was also my research assistant, so she was already baptized into the R Markdown family).\nBut I have to admit, the first time I taught it, I had a hard time teaching it! I realized that I lacked vocabulary around things I used everyday, but had never really talked about out loud with anyone in words.\nNow, in my role at RStudio, I‚Äôve devoted a lot of time and energy trying to figure out how we can make R Markdown easier- easier to discover, easier to debug, easier to use, and easier to talk about.\nSo without further ado, here are some of my guiding principles when introducing R Markdown to beginners, for those who are ready to go beyond casual knitter:\nMake it. Make it again. Knit early. Knit often. That means starting with a pre-filled Rmd document usually that you know will knit. How do you motivate repeated knitting and make it satisfying? Teach the basics of output formats and options by editing mainly the YAML. Your goal is to show how small effort ‚Äì\u0026gt; high polish. For single docs, I love html_document() with a floating table of contents and a theme (like this), switching quickly to bookdown::html_document2() and distill::distill_article(). The latter two also enable automated figure numbering, which for scientific audiences is quite nice!\nYou can do something like this with #rmarkdown. Ask students to knit to HTML, then use the bootswatch themes https://t.co/J9ucdRWnIl\nBonus: you get the prettiest #rstats homeworks after this! üíêüñçÔ∏è\n---\ntitle: \u0026quot;Habits\u0026quot; output: html_document: theme: flatly\n--- https://t.co/wMQ3Uqhc7s\n\u0026mdash; Alison Presmanes Hill (@apreshill) March 2, 2019  Bonus when using a theme with html_document(): show off the code_download output option!\nTIL you can embed a \u0026quot;code download\u0026quot; button in an HTML #rmarkdown doc so that users can click to download your source .Rmd from the rendered HTML version...without GitHub ü§© #rstats\nYAML:\n---\noutput:\nhtml_document:\ncode_download: true\n---\nTest: https://t.co/bp7w7XKF8b pic.twitter.com/uMQK0mvYcF\n\u0026mdash; Alison Presmanes Hill (@apreshill) March 22, 2019   Make it pretty. The starting Rmd should have a nice ggplot2 graphic in there, and maybe a pretty gt table too. This is a motivational, aspirational document! I also try to use local data sets, so they can see how that actually works, as opposed to using a data package.\nNever underestimate the power of being able to make pretty #rstats things. ‚¨ÜÔ∏è polish/effort ratio ‚û°Ô∏è happier users ü¶ö https://t.co/LMbFRUiuLa\n\u0026mdash; Alison Presmanes Hill (@apreshill) March 2, 2019   Make it snappy. I aim to get to a shareable link in the first 20 minutes (at most!). I like to use Netlify Drop for this. No account sign-up needed, and everyone knows how to drag-and-drop (see video below). It is very satisfying to get a link they can share with their mom/best friend/arch nemesis (kidding). I like to have everyone drop their links in a chat too, like a Slack, Google Doc, or a Gitter channel if doing a workshop. My favorite motto: ‚Äúif it knits, it ships‚Äù üö¢\n Make it real. Teach folks what they need to know to actually use the tool productively in real life. If you are an avid R Markdown user, this means that you know without a doubt that file paths will eventually be painful, for example. At the end of an intro, I go back and highlight things I just used to make sure they notice them like R Projects and the here package for data importing. I also love a good ‚ÄúYour Turn‚Äù exercise where you get a fresh data dump and all they have to do is re-knit. As in, ‚ÄúSurprise! Now instead of 3 sites, you have data from all 6 sites- what do you do?‚Äù\n Make it easy. People will only keep using R Markdown if they see it making their life easier. So show them how. For example, the RStudio IDE has some very nice built-in features that make it much easier to be an R Markdown user. I point out things having a global setup chunk, and IDE features like:\n  What do I save until later?\nRendering with render(). I think knitting in the RStudio IDE can get you very far- this I consider an intermediate to advanced concept that is confusing if introduced too early. I‚Äôve never heard someone say ‚ÄúWell there is this simple button, but how can I do the same thing from the command line?‚Äù\n Parameterized reports. I actually do use parameters though! I‚Äôll have at least one or two often in one of my Rmds, and mention them briefly in my wrap-up. I teach parameters explicitly with actual exercises when I have \u0026gt; 1 hour.\n Multiple Rmd output formats. I tend to start with single output formats first. If I have \u0026gt; 1.5 hours and it matches my learning objectives, then I happily oblige- I love teaching bookdown, blogdown, and distill websites. But to start, I stay single.\n Markdown. I skim markdown, and always provide a link to an interactive tutorial. This one is my favorite. Teaching markdown can be‚Ä¶dry. I show some bits on the slides because I cannot count on everyone doing the interactive tutorial ahead of time, but I do not linger much.\n  If you are curious to see some of the materials I‚Äôve used to teach R Markdown, click on the rmarkdown button just below this post! üöÄ\nBut remember: there is no one way to learn R Markdown, and no one way to teach it either. I love seeing the creativity of the community when introducing the R Markdown family- so keep them coming!\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590723957,"objectID":"726d799b72309e718780e6fc80d1b9f4","permalink":"https://kushagragpt99.github.io/post/2020-05-28-how-i-teach-r-markdown/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/post/2020-05-28-how-i-teach-r-markdown/","section":"post","summary":"A handful of guiding principles for introducing beginners to the R Markdown family of packages.","tags":["rmarkdown"],"title":"How I Teach R Markdown","type":"post"},{"authors":null,"categories":["machine learning","tidymodels"],"content":"A few years ago, I did a talk called \u0026ldquo;Take a Sad Plot \u0026amp; Make it Better,\u0026quot; where I showed how I took a single sad plot and tried to make it better. The process of making that plot better taught me a lot about data visualization, and about the ggplot2 package.\nFast-forward to 2019 when I started learning tidymodels, and I have accumulated some pretty sad predictive modeling scripts! And my sad plots are not so lonely anymore. Specifically, my old scripts for doing cross-validation with tidymodels are particularly sad. But, I\u0026rsquo;ve been able to make them better (one might even call them happy), primarily due to changes in the tune package and the addition of the fit_resamples() function. The process of making these scripts better taught me a lot about predictive modeling, and about the (evolving) tidymodels ecosystem. So, why write a blog post with outdated code?\n I want to remember that I did this \u0026ldquo;by hand.\u0026rdquo; I want to remember how I did this \u0026ldquo;by hand.\u0026rdquo; The code still works, even if there is now a happier path to doing the same thing. I want to share cute penguin art and gifs.  Let\u0026rsquo;s start with some cute penguin art by Rohan Chakravarty\u0026hellip;\n\nMy objective here is not to provide an introduction to using tidymodels, cross-validation, or to machine learning. If that is what you came for, check out the project button at the top of this post for my workshop materials for learners, and my associated blog post on the RStudio education site.\n Bottom line: If you are stumbling upon this blog post in the year 2020 or beyond, know that there is a better way!   A sad script symphony üéª üé∑ üéπ I\u0026rsquo;m not the first person to write sad tidymodels scripts- there are many out in the wild. Here were the blog posts that I found most helpful when trying to solve this particular coding conundrum:\n   Modelling with Tidymodels and Parsnip: A Tidy Approach to a Classification Problem by Diego Usai\n   A tutorial on tidy cross-validation with R by Bruno Rodrigues\n   Modeling with parsnip and tidymodels by Benjamin Sorensen\n  Packages library(tidyverse) library(tidymodels) library(rpart) # for decision tree library(ranger) # for random forest  Data I\u0026rsquo;m going to use data that Allison Horst helped me source on penguins from the Palmer Station (Antarctica) Long Term Ecological Research Network.\n \u0026ldquo;sooo now I\u0026rsquo;m just looking at penguin pictures\u0026rdquo;\n  Allison Horst after slacking me this penguin data    Update! We have bundled the Palmer Station penguins data into an R data package named palmerpenguins. Enjoy üêß Here is the package website: https://allisonhorst.github.io/palmerpenguins/   install.packages(\u0026quot;remotes\u0026quot;) # to install from github remotes::install_github(\u0026quot;allisonhorst/palmerpenguins\u0026quot;)  After you\u0026rsquo;ve installed the package, load it and read about the variables with ?penguins. We\u0026rsquo;ll modify this dataset lightly by:\n casting all characters as factors, dropping any observations with missing data, and dropping the island variable.  library(palmerpenguins) tidypenguins \u0026lt;- penguins %\u0026gt;% select(-island) %\u0026gt;% drop_na() glimpse(tidypenguins) #\u0026gt; Rows: 333 #\u0026gt; Columns: 6 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ade‚Ä¶ #\u0026gt; $ culmen_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.‚Ä¶ #\u0026gt; $ culmen_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.‚Ä¶ #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 1‚Ä¶ #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 380‚Ä¶ #\u0026gt; $ sex \u0026lt;fct\u0026gt; MALE, FEMALE, FEMALE, FEMALE, MALE, FEMALE, MALE, F‚Ä¶  Penguins Figure 1: Artwork by @allisonhorst\n This data included structural size measurements of penguins like their bill length, flipper length, and body mass. It also included each penguin\u0026rsquo;s species and sex. I\u0026rsquo;m going to use this data to try to predict penguin body mass. Sadly, we only have data for three distinct penguin species:\ntidypenguins %\u0026gt;% count(species) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Adelie 146 #\u0026gt; 2 Chinstrap 68 #\u0026gt; 3 Gentoo 119  Here is a lineup:\nFrom: https://www.bas.ac.uk/about/antarctica/wildlife/penguins/\nLooks like we have data for 3 of the smaller penguin species (of those pictured here).\nFirst, let\u0026rsquo;s build a simple linear regression model to predict body mass from flipper length.\nggplot(tidypenguins, aes(x = flipper_length_mm, y = body_mass_g)) + geom_point(color = \u0026quot;salmon\u0026quot;, size = 3, alpha = .9) + geom_smooth(method = \u0026quot;lm\u0026quot;) + theme_penguin()  Not bad! Looks promising. To actually fit a linear regression model, you might be used to something like this in R:\npenguin_mod \u0026lt;- lm(body_mass_g ~ flipper_length_mm, data = tidypenguins) summary(penguin_mod) #\u0026gt; #\u0026gt; Call: #\u0026gt; lm(formula = body_mass_g ~ flipper_length_mm, data = tidypenguins) #\u0026gt; #\u0026gt; Residuals: #\u0026gt; Min 1Q Median 3Q Max #\u0026gt; -1057.33 -259.79 -12.24 242.97 1293.89 #\u0026gt; #\u0026gt; Coefficients: #\u0026gt; Estimate Std. Error t value Pr(\u0026gt;|t|) #\u0026gt; (Intercept) -5872.09 310.29 -18.93 \u0026lt;2e-16 *** #\u0026gt; flipper_length_mm 50.15 1.54 32.56 \u0026lt;2e-16 *** #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #\u0026gt; #\u0026gt; Residual standard error: 393.3 on 331 degrees of freedom #\u0026gt; Multiple R-squared: 0.7621,\tAdjusted R-squared: 0.7614 #\u0026gt; F-statistic: 1060 on 1 and 331 DF, p-value: \u0026lt; 2.2e-16  But we aren\u0026rsquo;t going to stick with this. We are going to use tidymodels, with the goal of generating accurate predictions for future, yet-to-be-seen penguins.\ntidymodels 101 The code provided in the section below is not particularly sad üêß. If you are embarking on learning tidymodels, you\u0026rsquo;ll need to use this same kind of code as the building blocks for any predictive modeling pipeline.\nParsnip: build the model This step is really three, using only the parsnip package:\nlm_spec \u0026lt;- linear_reg() %\u0026gt;% # pick model set_engine(\u0026quot;lm\u0026quot;) %\u0026gt;% # set engine set_mode(\u0026quot;regression\u0026quot;) # set mode lm_spec #\u0026gt; Linear Regression Model Specification (regression) #\u0026gt; #\u0026gt; Computational engine: lm  Things that are missing: data (we haven\u0026rsquo;t touched it yet) and a formula (no data, no variables, no twiddle ~). This is an abstract model specification. See other possible parsnip models here.\nRecipe: not happening here, folks This is where you would normally insert some code for feature engineering using the recipes package. But previously this required functions named prep(), bake(), juice()- so I\u0026rsquo;m willfully ignoring that for now. There will be no recipes involving penguins.\nRsample: initial split We\u0026rsquo;ll use the rsample package to split (ayee! I promise no penguins were hurt in the writing of this blog post) the penguins up into two datasets: training and testing. If you are unfamiliar with this practice, read up on the holdout method.\npenguin_split \u0026lt;- initial_split(tidypenguins, strata = species) penguin_train \u0026lt;- training(penguin_split) penguin_test \u0026lt;- testing(penguin_split)  Fitting the model once Fitting a single model once is\u0026hellip;not exactly the hardest part.\nThis is essentially the workflow from this early blog post.\nset.seed(0) lm_spec %\u0026gt;% # train: get fitted model fit(body_mass_g ~ ., data = penguin_train) %\u0026gt;% # test: get predictions predict(new_data = penguin_test) %\u0026gt;% # compare: get metrics bind_cols(penguin_test) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 292.  Fitting the model with a function If you squint, you might see that I could make this into a function like below:\nget_rmse \u0026lt;- function(model_spec, split) { model_spec %\u0026gt;% # train: get fitted model fit(body_mass_g ~ ., data = training(split)) %\u0026gt;% # test: get predictions predict(new_data = testing(split)) %\u0026gt;% # compare: get metrics bind_cols(testing(split)) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) }  And I could use it to fit a linear regression model:\nset.seed(0) get_rmse(model_spec = lm_spec, split = penguin_split) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 292.  I could also build up a tibble that includes the results, if I wanted to save the predicted values, for example:\nget_preds \u0026lt;- function(model_spec, split){ # train: get fitted model fit_model \u0026lt;- model_spec %\u0026gt;% fit(body_mass_g ~ ., data = training(split)) # test: get predictions preds \u0026lt;- fit_model %\u0026gt;% predict(new_data = testing(split)) %\u0026gt;% bind_cols(testing(split) %\u0026gt;% select(body_mass_g, species)) preds } set.seed(0) penguin_preds \u0026lt;- get_preds(model_spec = lm_spec, split = penguin_split)  Then I can work with the predicted values, like plotting the fitted body mass estimates against the residuals.\nggplot(penguin_preds, aes(x = .pred, y = (.pred - body_mass_g))) + geom_point(aes(colour = species), size = 3, alpha = .8) + geom_smooth(method = \u0026quot;lm\u0026quot;) + theme_penguin() + scico::scale_colour_scico_d(end = .8) + ggtitle(\u0026quot;Residuals vs Fitted\u0026quot;) #\u0026gt; `geom_smooth()` using formula 'y ~ x'  # compare: get metrics penguin_preds %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 292.  Or I could fit a regression tree model with a new model spec:\n# regression tree model spec rt_spec \u0026lt;- decision_tree() %\u0026gt;% set_engine(\u0026quot;rpart\u0026quot;) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;) # get rmse set.seed(0) get_preds(model_spec = rt_spec, split = penguin_split) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 301.  Or a random forest:\n# random forest model spec rf_spec \u0026lt;- rand_forest() %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;) # get rmse set.seed(0) get_preds(model_spec = rf_spec, split = penguin_split) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; .metric .estimator .estimate #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 294.  But, unfortunately, I shouldn\u0026rsquo;t be predicting with the test set over and over again like this. It isn\u0026rsquo;t good practice to predict with the test set \u0026gt; 1 time. What is a good predictive modeler to do? I should be saving (holding out) the test set and use it to generate predictions exactly once, at the very end ‚Äî after I\u0026rsquo;ve compared different models, selected my features, and tuned my hyperparameters. How do you do this? You do cross-validation with the training set, and you leave the testing set for the very last fit you do.\nHey Jude, don\u0026rsquo;t make it sad üé∂ Now, for the üò≠ part- let\u0026rsquo;s add cross-validation! To do this, we\u0026rsquo;ll use a function called rsample::vfold_cv().\n# add the cv step here set.seed(0) penguin_folds \u0026lt;- vfold_cv(data = penguin_train, strata = \u0026quot;species\u0026quot;) penguin_folds #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 2 #\u0026gt; splits id #\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10  The process of training, testing, and computing metrics gets a lot harder when you need to do this across 10 folds, each with a different data split. I eventually worked out three approaches, which I show below. All require some level of comfort with iteration using the purrr package.\nFunction with minimal purrr-ing This approach is essentially a mega-function, that we then use purrr to map across each fold.\nI\u0026rsquo;m going to change a few things from my previous get_preds() function:\n training(split) -\u0026gt; analysis(split) testing(split) -\u0026gt; assessment(split) I also added the rsample::add_resample_id() function to keep track of the fold number. I saved the predictions now as a list column.  To build up this function, my strategy was to figure out how to work with one fold, then I knew I\u0026rsquo;d be able to use purrr::map_df() to apply it across multiple folds.\n# Figure it out for one fold get_fold_results \u0026lt;- function(model_spec, split){ # train: get fitted model for each fold fits \u0026lt;- model_spec %\u0026gt;% fit(body_mass_g ~ ., data = analysis(split)) # test: get predictions on for each fold preds \u0026lt;- fits %\u0026gt;% predict(new_data = assessment(split)) %\u0026gt;% bind_cols(assessment(split)) # compare: compute metric for each fold rmse \u0026lt;- assessment(split) %\u0026gt;% summarize(rmse = rmse_vec(truth = body_mass_g, estimate = preds$.pred)) rmse %\u0026gt;% # add fold identifier column rsample::add_resample_id(split = split) %\u0026gt;% as_tibble() %\u0026gt;% # add predictions mutate(preds = list(preds)) }  I tried this function with a single fold first:\nset.seed(0) get_fold_results( split = penguin_folds$splits[[1]], model_spec = rt_spec ) #\u0026gt; # A tibble: 1 x 3 #\u0026gt; rmse id preds #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 349. Fold01 \u0026lt;tibble [26 √ó 7]\u0026gt;  Next, I used purrr- but just once. The function get_fold_results is doing most of the work for us, but I needed purrr to map it across each fold.\nset.seed(0) kfold_results \u0026lt;- map_df( penguin_folds$splits, ~get_fold_results(.x, model = rt_spec)) kfold_results #\u0026gt; # A tibble: 10 x 3 #\u0026gt; rmse id preds #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 349. Fold01 \u0026lt;tibble [26 √ó 7]\u0026gt; #\u0026gt; 2 304. Fold02 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 3 290. Fold03 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 4 313. Fold04 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 5 403. Fold05 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 6 383. Fold06 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 7 330. Fold07 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 8 374. Fold08 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 9 319. Fold09 \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 10 298. Fold10 \u0026lt;tibble [25 √ó 7]\u0026gt;  Here we are still left with 10 RMSE values- one for each of the 10 folds. We don\u0026rsquo;t care too much about by fold- the power is in the aggregate. Specifically, we mainly care about the central tendency and spread of these RMSE values. Let\u0026rsquo;s finish by combining (or aggregating) these metrics.\nkfold_results %\u0026gt;% summarize(mean_rmse = mean(rmse), sd_rmse = sd(rmse)) #\u0026gt; # A tibble: 1 x 2 #\u0026gt; mean_rmse sd_rmse #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 336. 39.0  So, this works. But, can you imagine doing it again? Without errors? Can you imagine teaching it?\nPurrr-to-the-max This approach is purrr::map() (and friends) on steriods. We use vanilla map(), map2(), and map2_dbl() here. We also use anonymous functions as a formula, and the pipe operator within those anonymous functions.\nset.seed(0) penguin_res \u0026lt;- penguin_folds %\u0026gt;% mutate( # train: get fitted model for each fold train_set = map(splits, analysis), fit_models = map(train_set, ~rt_spec %\u0026gt;% fit(body_mass_g ~ ., data = .x)), # test: get predictions for each fold test_set = map(splits, assessment), estimates = map2(fit_models, test_set, ~.x %\u0026gt;% predict(.y)), # compare: compute metric for each fold rmse = map2_dbl(test_set, estimates, ~rmse_vec(truth = .x$body_mass_g, estimate = .y$.pred)) ) penguin_res #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 7 #\u0026gt; splits id train_set fit_models test_set estimates rmse #\u0026gt; * \u0026lt;named lis\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named lis\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 \u0026lt;split [22‚Ä¶ Fold01 \u0026lt;tibble [225 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26‚Ä¶ \u0026lt;tibble [26‚Ä¶ 349. #\u0026gt; 2 \u0026lt;split [22‚Ä¶ Fold02 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 304. #\u0026gt; 3 \u0026lt;split [22‚Ä¶ Fold03 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 290. #\u0026gt; 4 \u0026lt;split [22‚Ä¶ Fold04 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 313. #\u0026gt; 5 \u0026lt;split [22‚Ä¶ Fold05 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 403. #\u0026gt; 6 \u0026lt;split [22‚Ä¶ Fold06 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 383. #\u0026gt; 7 \u0026lt;split [22‚Ä¶ Fold07 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 330. #\u0026gt; 8 \u0026lt;split [22‚Ä¶ Fold08 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 374. #\u0026gt; 9 \u0026lt;split [22‚Ä¶ Fold09 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 319. #\u0026gt; 10 \u0026lt;split [22‚Ä¶ Fold10 \u0026lt;tibble [226 ‚Ä¶ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25‚Ä¶ \u0026lt;tibble [25‚Ä¶ 298. penguin_res %\u0026gt;% summarise(mean_rmse = mean(rmse), sd_rmse = sd(rmse)) #\u0026gt; # A tibble: 1 x 2 #\u0026gt; mean_rmse sd_rmse #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 336. 39.0  The purrr mash-up Another way I worked out was largely after reviewing Max\u0026rsquo;s slides from previous workshops. This is basically a mash-up of my previous two approaches, where we write laser-focused functions that each do one thing, then use purrr to apply those functions across the folds. This way is nice(r) for showing in slides as you can incrementally build up the results table. Let\u0026rsquo;s see this sad script in action\u0026hellip;\nRound 1 set.seed(0) # for reproducibility # train: get fitted model for a split get_fits \u0026lt;- function(split, model_spec){ model_spec %\u0026gt;% fit(body_mass_g ~ ., data = analysis(split)) } # train: get fitted models across folds penguin_purrr \u0026lt;- penguin_folds %\u0026gt;% mutate(rt_fits = map(splits, get_fits, rt_spec)) penguin_purrr #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 3 #\u0026gt; splits id rt_fits #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt;  Round 2 # test: get predictions for a split get_preds \u0026lt;- function(split, fit_df) { fit_df %\u0026gt;% predict(new_data = assessment(split)) %\u0026gt;% bind_cols(assessment(split)) } # test: get predictions across folds penguin_purrr \u0026lt;- penguin_purrr %\u0026gt;% mutate(rt_preds = map2(splits, rt_fits, get_preds)) penguin_purrr #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 4 #\u0026gt; splits id rt_fits rt_preds #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 √ó 7]\u0026gt; #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt;  aaaand Round 3 # compare: compute metric for a split get_rmse \u0026lt;- function(pred_df) { pred_df %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) %\u0026gt;% pluck(\u0026quot;.estimate\u0026quot;) } # compare: compute metric across folds penguin_purrr \u0026lt;- penguin_purrr %\u0026gt;% mutate(rt_rmse = map_dbl(rt_preds, get_rmse)) penguin_purrr #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 5 #\u0026gt; splits id rt_fits rt_preds rt_rmse #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 √ó 7]\u0026gt; 349. #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 304. #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 290. #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 313. #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 403. #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 383. #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 330. #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 374. #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 319. #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 298.  Finally, summarizing as I did before:\npenguin_purrr %\u0026gt;% summarize(mean_rmse = mean(rt_rmse), sd_rmse = sd(rt_rmse)) #\u0026gt; # A tibble: 1 x 2 #\u0026gt; mean_rmse sd_rmse #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 336. 39.0  In practice, if you did all these at once instead of incrementally, it would look like:\nset.seed(0) penguin_folds %\u0026gt;% # train: get fitted model for a split mutate(rt_fits = map(splits, get_fits, rt_spec)) %\u0026gt;% # test: get predictions on for each fold mutate(rt_preds = map2(splits, rt_fits, get_preds)) %\u0026gt;% # compare: compute metric for each fold mutate(rt_rmse = map_dbl(rt_preds, get_rmse)) #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 5 #\u0026gt; splits id rt_fits rt_preds rt_rmse #\u0026gt; * \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 √ó 7]\u0026gt; 349. #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 304. #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 290. #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 313. #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 403. #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 383. #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 330. #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 374. #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 319. #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 √ó 7]\u0026gt; 298.  When you put it like that, it doesn\u0026rsquo;t look like so much work! But, this way hides how much work it takes to write those 3 custom functions: get_fits(), get_preds(), and get_rmse(). And we still had to use vanilla map(), map2(), and map2_dbl().\nMake it better I kept a learning log while working through the all the above code, and I wrote down these notes to myself:\n  It is very easy to do the wrong thing; it is very hard to do the right thing.\n  I lost sight many times of what the code I was writing was doing, because I was using up so much cognitive energy on getting the code to just work.\n  I thought I knew how to use purrr\u0026hellip;\n  If you have made it this far, I\u0026rsquo;m pretty sure I don\u0026rsquo;t need to convince you that a better way to do cross-validation using tidymodels would be more pleasant to do more than once. It would also be less prone to error due to me copying-and-pasting repeatedly, and making stupid mistakes that would be difficult to spot with so much cluttered code. Luckily, tune::fit_resamples() came along to take a sad script and make it better:\npenguin_party \u0026lt;- tune::fit_resamples( rt_spec, body_mass_g ~ ., resamples = penguin_folds )  Here is the beautiful output from that function:\npenguin_party #\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 4 #\u0026gt; splits id .metrics .notes #\u0026gt; \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; #\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;tibble [2 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt;  Now, to see all the stuff inside this penguin_party, we can use tune\u0026rsquo;s collect_* functions.\npenguin_party %\u0026gt;% collect_metrics() #\u0026gt; # A tibble: 2 x 5 #\u0026gt; .metric .estimator mean n std_err #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 rmse standard 336. 10 12.3 #\u0026gt; 2 rsq standard 0.829 10 0.0159  To see the predictions, we need to add use control_resamples():\npenguin_party \u0026lt;- tune::fit_resamples( rt_spec, body_mass_g ~ ., resamples = penguin_folds, control = control_resamples(save_pred = TRUE) # add this line )  Then we collect the predictions.\npenguin_party %\u0026gt;% collect_predictions() #\u0026gt; # A tibble: 251 x 4 #\u0026gt; id .pred .row body_mass_g #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Fold01 3417 4 3625 #\u0026gt; 2 Fold01 3417 11 3325 #\u0026gt; 3 Fold01 3971. 14 3950 #\u0026gt; 4 Fold01 3971. 41 3800 #\u0026gt; 5 Fold01 3417 54 3700 #\u0026gt; 6 Fold01 3971. 65 4300 #\u0026gt; 7 Fold01 3971. 70 4100 #\u0026gt; 8 Fold01 3971. 71 4725 #\u0026gt; 9 Fold01 3971. 76 3900 #\u0026gt; 10 Fold01 3417 85 3350 #\u0026gt; # ‚Ä¶ with 241 more rows  Now, isn\u0026rsquo;t that better?\n","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582761600,"objectID":"3f6d21755f6210ccc450575422f58ff0","permalink":"https://kushagragpt99.github.io/post/2020-02-27-better-tidymodels/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/post/2020-02-27-better-tidymodels/","section":"post","summary":"Taking a sad script and making it better for model cross-validation.","tags":["tidymodels"],"title":"Take a Sad Script \u0026 Make it Better: Tidymodels Edition","type":"post"},{"authors":["","Aadil Hayat"],"categories":null,"content":"   This talk was part of Programming Club, IIT Kanpur\u0026rsquo;s talk on machine learning frameworks. This talk was targeted for sophomores and junior undergraduates with some background on deep learning. It explained dataflow graphs (which represent TensorFlow computations) and its basic terminologies and functions. Next, it covered basic implementation in TensorFlow through a toy problem on MNIST dataset, including the neural network and the dataflow graph for the problem. It also covered the installation procedure of the GPU and the CPU-only version.\n","date":1578096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578096000,"objectID":"ab75b4604f9d86033f76addc3de83d38","permalink":"https://kushagragpt99.github.io/talk/2019-tensorflow/","publishdate":"2020-01-04T00:00:00Z","relpermalink":"/talk/2019-tensorflow/","section":"talk","summary":"This talk introduces tensorflow for deep learning  and explains its underlying computation methodology.","tags":["tensorflow","deep learning","dataflow graphs"],"title":"Introduction to tensorflow","type":"talk"},{"authors":[],"categories":[],"content":" I‚Äôm excited to be teaching a new workshop at the upcoming rstudio::conf in January called ‚ÄúIntroduction to Machine Learning with the Tidyverse‚Äù, with my colleague Garrett Grolemund. Our workshop just sold out over the weekend! üéâ\nIt is always hard to develop an entirely new workshop, especially if you are doing it at the same time as learning how to use a new API. It is even harder when that API is under active development like the tidymodels ecosystem! I‚Äôve been so lucky to be able to work with the tidymodels team at RStudio, Max Kuhn and Davis Vaughan, to help shape how we tell the tidymodels story to ML beginners. But my favorite part of developing a new workshop like this has been studying how others teach machine learning. Spoiler alert: there are a lot of materials intended for learners that make things seem harder than they actually are! Below, I‚Äôm sharing my bookmarked resources, organized roughly in the order I think they are most helpful for beginners.\nMachine Learning for Everyone. In simple words. With real-world examples. Yes, again. In my experience, the biggest hurdle to getting started is sifting through both the hype and the math. This is a readable illustrated introduction to key concepts that will help you start building your own mental model of this space. For example, ‚Äúthe only goal of machine learning is to predict results based on incoming data. That‚Äôs it.‚Äù There you go! Start here.\n\n A Visual Introduction to Machine Learning by r2d3. This is a wonderful two-part series (that I wish would be extended!):\n Part I: A Decision Tree Part II: Model Tuning and the Bias-Variance Tradeoff   Supervised Machine Learning course by Julia Silge Taught with R and the caret package (the precursor to the in-development tidymodels ecosystem), this is a great next step in your machine learning journey as you‚Äôll start doing ML right away in your browser using an innovative course delivery platform. You‚Äôll also get to play with data that is not iris, titanic, or AmesHousing. This will be sweet relief because you‚Äôll find the rest of my recommended resources all basically build models to predict home prices in Ames, Iowa.\n\n Hands-on Machine Learning with R by Bradley Boehmke \u0026amp; Brandon Greenwell. Another great way to learn concepts plus code, although another one that focuses on the caret package (pre-tidymodels). Each chapter maps onto a new learning algorithm, and provides a code-through with real data from building to tuning. The authors also offer practical advice for each algorithm, and the ‚Äúfinal thoughts‚Äù sections at the end of each chapter will help you tie it all together.\n\nDon‚Äôt skip the ‚ÄúFundamentals‚Äù section, even if you feel like you‚Äôve got that down by now. The second chapter on the modeling process is especially good.\n\n Interpretable Machine Learning: A Guide for Making Black Box Models Explainable by Christoph Molnar. If you only have time to read a single chapter, skip ahead to Chapter 4: Interpretable Models. I also appreciated the introduction section on terminology. But the whole book is excellent and well-written.\n\n Model evaluation, model selection, and algorithm selection in machine learning- a 4-part series by Sebastian Raschka. I found this to be a great evidence-based, thorough overview of the methods for machine learning. I especially liked how he walks you step-by-step from the simplest methods like the holdout method up to nested cross-validation:\n Part I: The Basics Part II: Bootstrapping \u0026amp; uncertainties Part III: Cross-validation and hyperparameter tuning Part IV: Comparing the performance of machine learning models and algorithms using statistical tests and nested cross-validation   At this point, if you can read through the above resources and you are no longer feeling awash in new terminology, I think your vocabulary and mental model are in pretty good shape! That means you are ready for the next step, which is to read Max Kuhn and Kjell Johnson‚Äôs new book Feature Engineering and Selection: A Practical Approach for Predictive Models\n\nIn my experience, the later chapters in this book filled in a lot of lingering questions I had about certain methods, like whether to use factor or dummy variables in tree-based models. But also don‚Äôt miss the section on ‚Äúimportant concepts‚Äù at the beginning- this should feel like a nice review if you‚Äôve gotten this far!\n Elements of Statistical Learning. The entire PDF of the book is available online. A great resource for those with a strong statistics background, and for those looking for more math and formulas.\n  Other note-worthy resources  For the highly visual learner, you may want to cue up some YouTube videos from Udacity‚Äôs ‚ÄúMachine Learning for Trading‚Äù course. I found these illustrations especially helpful:\n Cross-validation Overfitting Ensemble learners Bootstrap aggregating (bagging) Boosting   Chris Albon‚Äôs Machine Learning Flashcards ($12)\n Shirin Elsinghorst‚Äôs blog (free! and so good).\n\nI love her sketchnotes.\n I also found Rafael Irizarry‚Äôs ‚ÄúIntroduction to Machine Learning‚Äù, a chapter from his Introduction to Data Science book, to have some helpful discussion.\n Machine Learning: A primer by Danilo Bzdok, Martin Krzywinski \u0026amp; Naomi Altman, from the Nature Methods Points of Significance collection- this collection in general is always straight-forward with great visuals. Start with the primer, then skim these:\n Statistics versus machine learning Machine learning: supervised methods Classification and regression trees (decision trees are the ‚Äúbase learner‚Äù for many ensemble methods - this is a good intro) Ensemble methods: bagging and random forests    That‚Äôs all for now- if you are taking my workshop in January I look forward to meeting you in person! If not, rest assured that all code and materials will be shared openly after the workshop. Until then, happy learning ü§ñ\n ","date":1577059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119901,"objectID":"416351d525a4abd88ecf05ec88076d80","permalink":"https://kushagragpt99.github.io/post/2019-12-23-learning-to-teach-machines-to-learn/","publishdate":"2019-12-23T00:00:00Z","relpermalink":"/post/2019-12-23-learning-to-teach-machines-to-learn/","section":"post","summary":"I‚Äôm excited to be teaching a new workshop at the upcoming rstudio::conf in January called ‚ÄúIntroduction to Machine Learning with the Tidyverse‚Äù, with my colleague Garrett Grolemund. Our workshop just sold out over the weekend!","tags":[],"title":"Learning to Teach Machines to Learn","type":"post"},{"authors":["","Raunak Shah"],"categories":null,"content":"   This talk was part of Programming Club, IIT Kanpur\u0026rsquo;s talk on introduction to deep learning. This talk was targeted for freshers and sophomores (both UG and PG) who are starting with machine learning. It covered topics like the most popular convex cost functions, activation functions and gradient descent, along with hands-on coding experience of these topics. It served as a medium of introducing deep learning to the campus community and as a starting step to explore more complex topics.\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"97855ba07b4bb6c611bebddb47a00173","permalink":"https://kushagragpt99.github.io/talk/2019-basic_dl/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/talk/2019-basic_dl/","section":"talk","summary":"Part of introduction to deep learning lecture series, covering topics like convex cost functions and gradient descent.","tags":["deep learning"],"title":"Introduction to Deep Learning","type":"talk"},{"authors":["Kushagra Gupta","Dootika Vats"],"categories":["stats"],"content":"Sufficient research has been done on estimating the asymptotic covariance matrix in a Markov chain central limit theorem for applications in Markov chain Monte Carlo (MCMC). However, almost all of it, including the efficient batch means (BM) estimator, focuses on a single-chain MCMC run. We demonstrate that simply averaging covariance matrix estimators from different chains (average BM) can yield critical underestimates of the variance in small sample sizes, especially for slow mixing Markov chains. We propose a multivariate replicated batch means (RBM) estimator that utilizes information across all chains in order to estimate the asymptotic covariance matrix, thereby correcting for the underestimation. Under weak conditions on the mixing rate of the process, strong consistency of the RBM estimator follows from the strong consistency of the BM estimators. Further, we show that the large-sample bias and variance of the RBM estimator mimics that of the average BM estimator. Thus, for large MCMC runs, the RBM and average BM yield equivalent performance, but for small MCMC runs, the RBM estimator can be dramatically superior. This superiority is demonstrated through a variety of examples, including a two-variable Gibbs sampler for a bivariate Gaussian target distribution. Here we obtain a closed-form expression for the asymptotic covariance matrix of the Monte Carlo estimator, a result vital in itself, as it allows for benchmarking implementations in the future.\n","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"bf6ccd8edd51a1f62595b1b7212a01a4","permalink":"https://kushagragpt99.github.io/project/rbm/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/project/rbm/","section":"project","summary":"Replicated batch means estimator for covariance estimation from multiple Markov chains.","tags":["stats"],"title":"Replicated Batch Means for Parallel MCMC","type":"project"},{"authors":["Kushagra Gupta","Aditya Gulati","Vikrant Malik","Aviral Aggarwal"],"categories":["misc"],"content":"  -- Fully Homomorphic Encryption (FHE) allows computation directly on the encrypted data, generating an encrypted result that matches the result of the computation on decrypted data. FHE removes the insecurity introduced by the requirement of decrypting data to perform computations on it. We build a levelled FHE library (which can evaluate circuits with a bounded depth) based on the GSW scheme using libtorch and CUDA. Our library is capable of performing parallel computations and automatic differentiation, making it significantly fast and easy to use, particularly for machine learning models.\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"5ffe1686c8e696f3f8c4595597b72ab7","permalink":"https://kushagragpt99.github.io/project/fhe/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/project/fhe/","section":"project","summary":"Implemented a C++ FHE library based on GSW encryption system using libtorch with CUDA to enable parallel computation with Automatic Differentiation.","tags":["misc"],"title":"Fully Homomorphic Encryptio Library","type":"project"},{"authors":["Kushagra Gupta","Arshad Rahman"],"categories":["stats"],"content":"In Fall 2019, I was working with Prof.Vats and Prof. Arshad Rahman (in Economics department) on developing Bayesian models for interval data. I was introduced to interval regression in my summer project on Bayesian estimation of Weibull distribution while reading Prof.Kundu\u0026rsquo;s paper on interval-censored data with Weibull lifetime. I researched on various paradigms that solve interval regression, covering quantile regression (QR), meta-heuristic algorithms, information theory, convex analysis, and set arithmetic linear models. I found QR to be a promising approach as traditional QR settings can extend to interval data in the following way - we can modify the quantile loss function to have three parts based on the partition by the interval endpoints, as opposed to a binary partition by traditional QR, with certain added assumptions. The change is that the dot product of parameters and covariates lying in the interval has a zero loss as the dependent variable can lie anywhere in the interval. The assumptions can be modelled in a Bayesian setting with appropriate priors. As part of my project, I built Bayesian frameworks for various paradigms of interval regression.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"b76bcc6ebceb6f5bd69068769f9917f2","permalink":"https://kushagragpt99.github.io/project/qr/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/project/qr/","section":"project","summary":"Developed Bayesian models in multiple paradigms for interval regression.","tags":["stats"],"title":"Interval Regression using Bayesian Inference","type":"project"},{"authors":["Kushagra Gupta","Medha Agarwal","Debasis Kundu"],"categories":["stats"],"content":"In the summer of 2019, I (in a team of 2) was working with Prof.Kundu on developing Bayesian methodologies for parameter estimation of a 3-Weibull distribution \\cite{weibull}. Maximum likelihood estimators are inconsistent and inefficient for this problem when the shape parameter is less than one due to the likelihood function exploding to infinity. However, the Bayesian setting with gamma priors solves this problem as the posterior breaks into a product of proper densities, which is convenient for MCMC sampling. We found the posterior to be log-concave and similar to a gamma distribution and, thus, approximated the parameters and confidence intervals using this observation. Our Bayesian implementation reduced the mean square errors significantly as compared to the maximum likelihood method.\n","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"5a4b4031cc7e0e43e7ad77f75cb1a97c","permalink":"https://kushagragpt99.github.io/project/weibull/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/project/weibull/","section":"project","summary":"Developed algorithm for parameter estimation of 3-Weibull distribution using bayesian inference.","tags":["stats"],"title":"Bayesian Inference on 3 parameter Weibull Distribution","type":"project"},{"authors":["Kushagra Gupta","Utsav Singh"],"categories":["ml"],"content":" I worked on multi-agent self-play in atari games in collaborative and competitive settings. I used variational autoencoders to disentangle multiple near-optimal policies extracted using latent code. Our initial results on the model gave win probability of 72%, which is close to 80% SOTA values, and much better than the human score of 40% in multi-agent CTF. I worked on developing a generative model for InfoRL to maintain unsupervised setting for latent code generation to allow all standard MARL algorithms to be used with InfoRL.  ","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"6bd13f3caa1ba927eb609b13da077e28","permalink":"https://kushagragpt99.github.io/project/mrl/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/project/mrl/","section":"project","summary":"Multi-agent self-play in atari games in collaborative and competetive settings.","tags":["ml"],"title":"Multi-Agent Reinforcement Learning","type":"project"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"  #1: Update Hugo #2: Change the baseurl #3: Netlify drag-and-drop #4: Torch public/ #5: Peruse public/ #6: Back to the future    ‚ÄúJust a spoonful of Hugo helps the blog go down.‚Äù - me, only somewhat kidding\n In this series, I‚Äôm sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated). You can read the previous posts about my ‚ÄúSpoonful of Hugo‚Äù series about Hugo archetypes, Hugo versions, and Hugo page bundles.\nThe following are a few steps that I always start with to troubleshoot any blogdown/Hugo/Netlify problems. These steps would solve what I would anecdotally estimate as ~50% of blogdown problems that I see posted in the GitHub repository and on the community site.\n#1: Update Hugo  Figure 1: Don‚Äôt be like this  If things have gone south and you are getting Hugo errors when you use the ‚ÄúServe Site‚Äù Addin locally, it is possible that you need to update your version of Hugo. From R, you can check your Hugo version with blogdown:\nblogdown::hugo_version() Then you can reference your Hugo theme to find the minimum version of Hugo required by your theme:\n Figure 2: Check your theme‚Äôs minimum Hugo version  You can go higher than the minimum version though, so it‚Äôs good practice to update your Hugo, again from within R:\nblogdown:: update_hugo() Check your version again post-update:\nblogdown::hugo_version() ## [1] \u0026#39;0.55.6\u0026#39; If you are using Netlify to build your site using Hugo, you‚Äôll want this version to match that- the best way to do that is with a netlify.toml file.\n #2: Change the baseurl Open up your config.toml file and look for the baseurl field, usually pretty close to the top. Here is mine1:\nbaseurl = \u0026quot;https://alison.rbind.io\u0026quot; Now if you are just starting with Hugo and don‚Äôt actually have a domain name yet, try taking the advice that blogdown automatically prints out for you:\nWarning: You should change the \u0026quot;baseurl\u0026quot; option in config.toml from https://example.org to your actual domain; if you do not have a domain, set \u0026quot;baseurl\u0026quot; to \u0026quot;/\u0026quot; But be careful here- you shouldn‚Äôt leave it as ‚Äú/‚Äù- once you do have your domain name you should update the baseurl as ‚Äú/‚Äù is a not a valid URL.\nCare to know more? Here is a quote from the person who writes the Hugo docs:\n ‚Äú‚Ä¶the only purpose for the baseurl field in the config is to define the full base URL of your website for deployment purposes.‚Äù - @rdwatters\n The main error that would happen without the trailing slash in the past is that you would end up with a site where the theme‚Äôs CSS would be all wrong. This was probably because the theme designer used code like this buried in a layout file:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ .Site.BaseURL }}css/style.css\u0026quot;/\u0026gt; Now, if you set baseurl = \u0026quot;http://mysite.com\u0026quot; but only rendered locally, things would look just peachy, because the default local server already included the trailing slash. So, the link in the html file would be2:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;http://localhost:1313/css/style.css\u0026quot;\u0026gt; But, at build, the link in the html file would turn into:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;http://mysite.comcss/style.css\u0026quot;\u0026gt; Which creates sites that look like this:\n Figure 3: Hugo tranquil peak theme  GitHub issue #369\n Figure 4: Hugo universal theme  GitHub issue #131\nGitHub issue #114\nHowever, Hugo authors and theme developers have largely been moving towards using relative URLs instead of the baseurl to build paths. This was based on public advice voiced by the Hugo authors on the discourse forum. For example:\n ‚ÄúThe recommended way to reference resources is to use either relURL or absURL template funcs, which handles the slash issues.‚Äù- @bep\n Following that advice, a more up-to-date theme would have code that looks like this buried in a layout file:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ \u0026quot;css/style.css\u0026quot; | relURL }}\u0026quot;/\u0026gt; ‚Üí  Bottom line? If your theme uses relURL or absURL to link to site resources like CSS, JavaScript, or static images, then whether or not you include a trailing slash in your baseurl should not matter at all.\nAnd here is some tough love about your theme: if the most recent version does still require the trailing slash in the baseurl to ‚Äúwork‚Äù out of the box, I would seriously consider switching themes. This is a pretty good ‚Äúcanary in the coal mine‚Äù test regarding how up-to-date the theme author is, and how well the theme you have chosen adheres to Hugo templating best practices. If you are having pain with this now, it is likely not the only thing that will be painful about working with your theme.\n  #3: Netlify drag-and-drop If you can render your site locally but your published site looks different, try the drag-and-drop method:\nUse the ‚ÄúServe Site‚Äù Addin, then drag-and-drop the public/ folder straight into Netlify. What does this do? You can now see your public site‚Ä¶that you built‚Ä¶with your local version of Hugo. Netlify is doing none of the site building here.\nOne of the first benefits of this approach is that it ensures that you are able to actually generate a public/ folder locally! I have seen folks struggle to deploy the wrong repo. This simple step can force you to make sure to use the ‚ÄúServe Site‚Äù Addin to generate the public/ folder, and that the repo you are trying to link to Netlify actually contains a Hugo site because you must physically move the public/ folder. But this method can also help you diagnose other problems too.\nIf your public/ folder does not render on Netlify, you have work to do locally. I can‚Äôt tell you what it is as it can be a number of things, but you can be sure that your problem is not just the Netlify build- it is your local build too.\nIf your public/ folder does render perfectly on Netlify, but you are getting a Netlify build error, then you likely have a Hugo version problem. It might be that the version you are running locally is more recent than the version run by Netlify by default to actually build your site. The good news is there is a quick fix for this! The solution is to upgrade the Hugo version Netlify is using- see my advice here for how to do that.\nIf you are happy with how your site looks but you are missing content and/or seeing old deleted content, then you may need the next few strategies to troubleshoot.\n #4: Torch public/ When you are seeing very weird things locally, try deleting your local public/ folder. Then serve site again. Sometimes it can get ‚Äújunked up‚Äù. I‚Äôve found that sometimes deleted content can be a little sticky. As recommended in the blogdown book:\n ‚Äúyou are strongly recommended to delete the /public/ directory before you rebuild the site for publishing every time, because Hugo never deletes it‚Äù\n Also, this has a bonus of reinforcing for you exactly what the ‚ÄúServe Site‚Äù Addin does - it regenerates the public/ folder. This is also the folder that, if you are using Netlify to build your site, is in your .gitignore file because Netlify (+ Hugo) generates this file ‚Äúfresh‚Äù with each push to your GitHub repository.\n #5: Peruse public/ When you notice weird things, try actually looking inside public/- don‚Äôt be afraid to spelunk around in there! If you are seeing something wrong with your site, try to figure out how blogdown/Hugo is processing and rendering your content. This folder can tell you a lot! Keep in mind that your local public/ folder will still contain future/draft/expired content if you used the ‚ÄúServe Site‚Äù Addin.\n #6: Back to the future  Figure 5: Where are my posts?  If your site renders beautifully locally, and your drag-and-drop site from public/ looks the same, but you are missing key content when you actually deploy to Netlify using a Hugo build, you may have inadvertently stumbled into a Hugo date time warp. This is a fairly common gotcha. Try using the drag-and-drop method again, this time first delete public/, then instead of using the ‚ÄúServe Site‚Äù Addin, run this in your console:\nblogdown::build_site(local = FALSE) Plop this new public folder in Netlify to see what your site will look like when it is actually published. What does this show you? Your local Hugo build (read: your public/ folder generated by ‚ÄúServe Site‚Äù) differs by design in 3 important ways from your deployed site built by Netlify/Hugo. By default, Hugo will not publish:\n Content with a future publishDate value\n Content with draft: true status\n Content with a past expiryDate value\n  You can see that these are defaults. The behavior of the ‚ÄúServe Site‚Äù Addin is also documented in the blogdown book:\n ‚ÄúThis is for you to preview draft and future posts locally.‚Äù\n Blogdown‚Äôs build_site(local = FALSE) differs from the ‚ÄúServe Site‚Äù Addin in that it will not render draft, future, or expired content. So your public/ folder from build_site(local = FALSE) shows you exactly what Netlify should publish. Seeing it can help you troubleshoot why some content was showing up locally but not when you publish.\nThe defaults are pretty sensible and nice to have, as you can still put these kinds of content under version control, and hence collaborate with other team members on the content without having the content publish (or expire) until you say so.\nTo show content that Hugo was hiding, you‚Äôll want to edit some YAML fields in the individual offending content files. For example, in the YAML of an individual content file (like a blog post), if you want to un-draft it, add or change this key/value:\ntitle: \u0026#39;A Spoonful of Hugo: Troubleshooting your Build\u0026#39; author: \u0026quot;Alison Hill\u0026quot; date: \u0026#39;2019-03-04\u0026#39; draft: false Alternatively, if you want to date something in the future (like to advertise the date of an upcoming talk) but publish now, you can use the publishDate field. The publishDate field is a newer addition to Hugo (\u0026gt;= v0.54.0) which, if left unset, will default to the date field, which means in the individual content file YAML you can do:\ntitle: \u0026#39;A Spoonful of Hugo: Get excited!!\u0026#39; author: \u0026quot;Alison Hill\u0026quot; date: \u0026#39;2025-03-04\u0026#39; publishDate: \u0026#39;2019-03-04\u0026#39; Hopefully these 6 things can help you get unstuck. If not, the RStudio community forums are a great place to ask questions!\n  Yes that‚Äôs right, I don‚Äôt have a trailing slash- read on for why I can get away with this.‚Ü©\n https://discourse.gohugo.io/t/how-not-to-specify-url-site/5691/5‚Ü©\n   ","date":1551657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551657600,"objectID":"0ba13389195341f445a9a6be1766a7ec","permalink":"https://kushagragpt99.github.io/post/2019-03-04-hugo-troubleshooting/","publishdate":"2019-03-04T00:00:00Z","relpermalink":"/post/2019-03-04-hugo-troubleshooting/","section":"post","summary":"A few troubleshooting strategies to save your sanity","tags":["blogdown"],"title":"A Spoonful of Hugo: Troubleshooting Your Build","type":"post"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"  ‚ÄúJust a spoonful of Hugo helps the blog go down.‚Äù - me, only somewhat kidding\n In this series, I‚Äôm sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated). You can read the previous posts about my ‚ÄúSpoonful of Hugo‚Äù series about Hugo archetypes and Hugo versions.\nThis is my third post in this series and it is breaking news.\nHugo Page Bundles Well, not really breaking news, but you still may not know about it! Hugo v0.32 introduced a new feature called Page Bundles, as a way to organize the content files. Blogdown users rejoice that Davis Vaughn posted an issue on the rstudio/blogdown repo to enable this option, which Yihui added shortly before rstudio::conf 2019 üéâ. Here is the snippet from the NEWS.md:\n ‚ÄúOne benefit of using a page bundle instead of a normal page is that you can put resource files associated with the post (such as images) under the same directory of the post itself. This means you no longer have to put them under the static/ directory, which has been quite confusing to Hugo beginners.‚Äù\n What does a blogdown/Hugo site begin to look like without page bundles? I think here is a representative example from tidyverse.org (sorry tidyverse team- it‚Äôs not you, it‚Äôs the old Hugo).\nFor this team, they need an image for every post, which gets out of control pretty fast. Also, some ended up in static/ too, organized by post (which I have done on my own blog, though not well or consistently).\nWhat would it look like to use page bundles?\ncontent/ ‚îú‚îÄ‚îÄ about ‚îÇ ‚îú‚îÄ‚îÄ index.md ‚îú‚îÄ‚îÄ posts ‚îÇ ‚îú‚îÄ‚îÄ 2015-07-23-hi-world ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ bakers.csv ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ image1.jpg ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ image2.png ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.Rmd ‚îÇ ‚îî‚îÄ‚îÄ 2015-07-24-bye-world ‚îÇ ‚îî‚îÄ‚îÄ index.Rmd One could call this bundled file structure ‚Äútidier‚Äù üç±.\nIn the above, after serving site, index.html files also get added to the bundle. In Hugo‚Äôs terms, these are leaf bundles. The resource files allowed in a bundle include page and non-page items like images, pdf, .csv files, etc.\nThis is instead of:\ncontent/ ‚îú‚îÄ‚îÄ about ‚îÇ ‚îú‚îÄ‚îÄ index.md ‚îú‚îÄ‚îÄ posts ‚îÇ ‚îú‚îÄ‚îÄ 2015-07-23-hi-world.Rmd ‚îÇ ‚îú‚îÄ‚îÄ bakers.csv ‚îÇ ‚îú‚îÄ‚îÄ image1.jpg ‚îÇ ‚îú‚îÄ‚îÄ image2.png ‚îÇ ‚îî‚îÄ‚îÄ 2015-07-24-bye-world.Rmd When you create a new bundled post, the actual content of the post goes in the index file of a page bundle. So:\n# not bundled post post/2015-07-23-hi-world.Rmd # bundled post post/2015-07-24-bye-world/index.Rmd  Bundle Me, blogdown! First, read the previous post on setting up a netlify.toml file. Since using Hugo page bundles depends on Hugo v0.32 or higher, you should go ahead and update hugo then update your netlify.toml with your updated version:\nblogdown::update_hugo() blogdown::hugo_version() Now, let‚Äôs use the usethis package.\nProject-specific .Rprofile First, I‚Äôm going to demo here how to create a project-specific .Rprofile file- but know that you can do a user-level .Rprofile file too.\n# install.packages(\u0026quot;usethis\u0026quot;) # uncomment this to install usethis::edit_r_profile(scope = \u0026quot;project\u0026quot;) These helpful messages should print to your console: please note the ‚Äúrestart‚Äù reminder‚Ä¶\n\u0026gt; usethis::edit_r_profile(scope = \u0026quot;project\u0026quot;) ‚óè Restart R for changes to take effect ‚úî Setting active project to \u0026#39;/Users/alison/rprojs/alison.rbind.io\u0026#39; ‚óè Modify \u0026#39;.Rprofile\u0026#39; Now you could add this to your file:\n# in .Rprofile of the website project if (file.exists(\u0026quot;~/.Rprofile\u0026quot;)) { base::sys.source(\u0026quot;~/.Rprofile\u0026quot;, envir = environment()) } options(blogdown.new_bundle = TRUE) The first code chunk above is from the blogdown book, where we describe a workaround for loading both user and project .Rprofile files (since R technically only reads one startup profile file).\nIf you don‚Äôt want this, you could add the blogdown options to your user .Rprofile instead using:\nusethis::edit_r_profile(scope = \u0026quot;user\u0026quot;) Heck, while you are at it, you could set a bunch of options to make your blogdown life easier:\n# in .Rprofile of the website project if (file.exists(\u0026quot;~/.Rprofile\u0026quot;)) { base::sys.source(\u0026quot;~/.Rprofile\u0026quot;, envir = environment()) } options( blogdown.author = \u0026quot;Alison Hill\u0026quot;, blogdown.ext = \u0026quot;.Rmd\u0026quot;, blogdown.subdir = \u0026quot;post\u0026quot;, blogdown.yaml.empty = TRUE, blogdown.new_bundle = TRUE, blogdown.title_case = TRUE ) For the blogdown-specific options, any of these prepopulate content in your ‚ÄúNew Post‚Äù Addin (I told you to use this here). There is a handy table from the blogdown book, summarized here:\n blogdown.author = author of new posts blogdown.ext = default extension of new posts (can also be ‚Äú.md‚Äù or ‚Äú.Rmarkdown‚Äù) blogdown.subdir = theme-specific, you need to know your theme and content folder here blogdown.yaml.empty = I told you to do that here blogdown.new_bundle = what this whole post is about! blogdown.title_case = ‚ÄúnEed More coFFee‚Äù ‚Äì\u0026gt; ‚ÄúNeed More Coffee‚Äù (it tidies all your post titles to title case)   The Newline Thing Here is a massive .Rprofile gotcha: this file must end with a blank line. So make sure you add an empty line at the end of the file, then save it, and restart your R session.\nWant to make your general R life easier in the future? Follow Yihui‚Äôs advice and do this in RStudio to ensure that all source files end with a newline:\n  Use Bundles After restarting R, try using the ‚ÄúNew Post‚Äù Addin, this time with feeling. There is still one more gotcha though. Use the Addin to create your new bundled post. The only catch is that once you are looking at your exciting new post, you should delete the slug in the YAML (I posted an issue about this here).\nThe reason is that you want the link to your post to be:\nhttp://alison.rbind.io/post/2019-02-21-hugo-page-bundles/\nIf you include the slug, the link to your post will be:\nhttp://alison.rbind.io/post/2019-02-21-hugo-page-bundles/hugo-page-bundles\nAnother option is to update your config.toml file with permalinks like Yihui suggests (but beware: this will change all your past links as well, requiring some Netlify redirects):\n[permalinks] post = \u0026quot;/:year/:month/:day/:slug/\u0026quot; The default here from Hugo was /post/:year-:month-:day-:slug/:slug/.\nA small note: if you want to add relative links from a blog post to another post in your same blog. So [this](/post/2019-02-19-hugo-archetypes/) becomes this.\nNow, add images and data files to your ‚ù§Ô∏è‚Äôs content! But you may want to do one more thing‚Ä¶\n Update Metadata If you are anything like me, you may draft a blog post then come back to it later. For example, I started this post 2 days ago, but want to publish it today, 2019-10-03. The cool thing that was already built-in to blogdown is the ‚ÄúUpdate Metadata‚Äù Addin. With your blog post open (it should be called index.Rmd)1, click on Addins and select ‚ÄúUpdate Metadata‚Äù. You should see a window like this:\nCheck the box to rename the file if the date has changed. RStudio will tell you your file has been deleted- which is technically true since the folder was renamed, but don‚Äôt panic!\nClick YES. The index.Rmd file that is now open should have an updated date field in the YAML. In your RStudio file viewer, you may want to click on ‚Äúcontent‚Äù at this point then navigate back to view your post- then you will then see that the folder name now has an updated date too.\n  If no post is open, you will get an error: Warning message: The current document does not seem to contain YAML metadata‚Ü©Ô∏é\n   ","date":1550707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550707200,"objectID":"57794aaa1dd7911236dd044032a33bf8","permalink":"https://kushagragpt99.github.io/post/2019-02-21-hugo-page-bundles/","publishdate":"2019-02-21T00:00:00Z","relpermalink":"/post/2019-02-21-hugo-page-bundles/","section":"post","summary":"Why (and how) you should use Hugo's new page bundles feature","tags":["blogdown"],"title":"A Spoonful of Hugo: Page Bundles","type":"post"},{"authors":["alison"],"categories":["blogdown","netlify","hugo"],"content":"  ‚ÄúJust a spoonful of Hugo helps the blog go down.‚Äù - me, only somewhat kidding\n You can read the previous post about my ‚ÄúSpoonful of Hugo‚Äù series here. In this series, I‚Äôm sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated).\nThis is my second post in this series, and it is a relatively quick one. Just do this. This one is a no-brainer.\n Thanks to Mara Averick for alerting me that with Hugo version 0.54.0 and onward, there is a trailing zero at the end of Hugo versions now. So for versions before 0.54.0, use the format: 0.53; for later versions use 0.54.0 (0.54 will not work).   Use Netlify to Deploy First, you‚Äôll need to use Netlify! I am a very happy Netlify user and currently have approximately 33 sites deployed. To setup a new account, navigate to Netlify and click on the Sign Up link.\nSign up with GitHub to connect your GitHub and Netlify accounts (as shown below).\nIf you use a different version control service, select GitLab or BitBucket instead.\nThe last step is to use the Netlify UI in browser do New Site from Git \u0026gt; pick your repo. You‚Äôll be prompted to fill in these fields, they are probably already filled in correctly for you:\nThe next part is the advanced build settings:\nSee that pro tip about the netlify.toml? Let‚Äôs do that! You can leave these fields as is.\n Why netlify.toml? In their Build Gotchas:\n ‚ÄúIf your build works locally, the next debugging step is to ensure the package versions we use to build match yours. You can find the settings for these in the Build Settings doc. That‚Äôs the leading cause of build failure.‚Äù\n Yes that is right- package version mismatches are the leading cause of build failure with Netlify. What does this look like for blogdown users? This means that you are running a version of Hugo locally that doesn‚Äôt match the version that Netlify is using to build your site. Most of the time, you are using a more recent version of Hugo than the one Netlify uses. This means that the files your theme relies on may be using newer Hugo functions that were introduced in later Hugo versions- functions that Netlify won‚Äôt be able to find working from an older Hugo version. You‚Äôll get all the build errors.\nYou can check your local Hugo version by running this code in your R console:\nblogdown::hugo_version() ## [1] \u0026#39;0.57.2\u0026#39; Now, we want Netlify to use this same version of Hugo when it builds your site. You can do this two ways:\nDo this in your browser (üëé) Do this in your project root directory in a netlify.toml file (üëç)   Add the netlify.toml File Adding this file means that team members can see for themselves what version of Hugo you are running- if it is buried in the Netlify UI, you can‚Äôt see that information unless you sift through the public build logs (no thanks). Making the file as plain text in the root of your blogdown project directory means that:\n it is version controlled (yay!) and other people who use/learn from/contribute to your blog can actually reproduce your site with the same site configuration. Bonus: you can set the Hugo versions for branch deploys too.  Here is an example from my own netlify.toml file1:\n[build] publish = \u0026quot;public\u0026quot; command = \u0026quot;hugo\u0026quot; [context.production.environment] HUGO_VERSION = \u0026quot;0.54.0\u0026quot; # if older, use format: 0.53 (no trailing zero) HUGO_ENV = \u0026quot;production\u0026quot; HUGO_ENABLEGITINFO = \u0026quot;true\u0026quot; [context.branch-deploy.environment] HUGO_VERSION = \u0026quot;0.54.0\u0026quot; # if older, use format: 0.53 (no trailing zero) [context.deploy-preview.environment] HUGO_VERSION = \u0026quot;0.54.0\u0026quot; You can leave off the last two chunk if you don‚Äôt want to use branch deploys or preview deploys, but I ‚ù§Ô∏è these two Netlify features and encourage you to try them out. I‚Äôve starting drafting individual blog posts and tutorials in branches, and then I can see them rendered and share them for feedback without asking collaborators to clone and build the repository locally. It is lovely. Every branch and pull request gets a link üéâ.\nSo add this file to your blogdown site repo and push to GitHub.\nNote that, according to the Netlify docs:\n ‚ÄúDuring a build, the following ordering determines which context covers a particular deploy: UI settings are overridden if a netlify.toml file is present in the root folder of the repo and there exists a setting for the same property/redirect/header in the toml file.‚Äù\n If you look in your site‚Äôs Netlify deploy log, you should see entries like this:\n7:47:13 PM: Found netlify.toml. Overriding site configuration 7:47:13 PM: Starting build script 7:47:13 PM: Installing dependencies 7:47:14 PM: Started restoring cached node version 7:47:17 PM: Finished restoring cached node version 7:47:18 PM: v8.15.0 is already installed. 7:47:19 PM: Now using node v8.15.0 (npm v6.4.1) 7:47:19 PM: Attempting ruby version 2.3.6, read from environment 7:47:20 PM: Using ruby version 2.3.6 7:47:20 PM: Using PHP version 5.6 7:47:20 PM: Installing Hugo 0.54.0 Success!\n  the leading zero matters for Hugo versions, so 0.53 works but .53 will not. For versions \u0026gt;= 0.54.0, the trailing zero also matters, so 0.54.0 works but 0.54 will not.‚Ü©\n   ","date":1550620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550620800,"objectID":"8a82d0bfce9f49b7c489323259c476b4","permalink":"https://kushagragpt99.github.io/post/2019-02-19-hugo-netlify-toml/","publishdate":"2019-02-20T00:00:00Z","relpermalink":"/post/2019-02-19-hugo-netlify-toml/","section":"post","summary":"Why you should use a netlify.toml file in your blogdown site","tags":["blogdown","netlify","hugo"],"title":"A Spoonful of Hugo: The netlify.toml File","type":"post"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"  ‚ÄúJust a spoonful of Hugo helps the blog go down.‚Äù - me, only somewhat kidding\n As a happy blogdown user, a common story I hear from other #rstats users is that you try to change one little thing in Hugo, and the whole site breaks. Here be dragons for folks who aren‚Äôt web developers.\nI‚Äôm here to tell you that there are small spoonfuls of Hugo that can help you get your site UP (and even better- more efficient, more streamlined, more automated), even if you are not in the least bit interested in transitioning into a career in web development üòè.\nMy Project The education team at RStudio needs a website and we have a short wishlist:\n We want something we can maintain ourselves, We want to look consistent with other RStudio sites on the outside, and We want to be consistent on the inside so that we can get help if/when we need it.  This led me to the current tidyverse.org blogdown site. I wanted to make a copy of the site then customize for the education team, but I noticed that the source code for the site didn‚Äôt make it easy for me to copy the structure of the site and edit only the content of the site. This is one of the real strengths of Hugo, so I embarked on a learning adventure.\n   via GIPHY  As a result, I have been living and breathing Hugo lately. As in, my husband now recognizes Mike Dane‚Äôs voice. You may not have have met Mike yet, but he appears in all the video tutorials in the Hugo docs. His screencasts have been really helpful to me, like this one on templating. I‚Äôve also spent a lot of time actually reading the docs (which are pretty good!), reading posts and answers on the Hugo discourse community site, and spelunking around inside the actual source code for two very well structured Hugo sites:\nThe actual Hugo site: https://github.com/gohugoio/hugoDocs The rOpenSci site: https://github.com/ropensci/roweb2  I‚Äôll be using this post and other later posts to share some of the things I‚Äôve learned about Hugo along the way. Mainly breadcrumbs to myself, but I hope these help other people too.\nFor reference, I‚Äôm using Hugo via the blogdown R package, and within the RStudio IDE. These are my blogdown and Hugo versions:\npackageVersion(\u0026quot;blogdown\u0026quot;) ## [1] \u0026#39;0.12.1\u0026#39; blogdown::hugo_version() ## [1] \u0026#39;0.55.6\u0026#39;  tl;dr: A Teaspoon of Archetypes Add custom archetypes as .md files to your project root directory (do not touch the archetypes folder in your themes/archetypes folder).  If you don‚Äôt have that as an empty folder in your project root, make one, then add your archetype files to it. If you are making a new blogdown site, I recommend using these options to keep your empty directories1:  library(blogdown) new_site(theme = \u0026quot;jpescador/hugo-future-imperfect\u0026quot;, sample = TRUE, theme_example = TRUE, empty_dirs = TRUE, # this! to_yaml = TRUE)  Figure 1: Using the RStudio Project Wizard  Use the ‚ÄúNew Post‚Äù Addin in RStudio to create any and all new content for your site (not just posts!). Be sure to use the handy dropdown menu to select from all the possible archetypes. Also, careful about the subdirectory here- some themes use blog, others use news, articles, or posts.\n Your archetypes, while only markdown files, can include R code. When you use the Addin, be sure to choose R Markdown (.Rmd) as the format so that you can run the code.  Don‚Äôt miss this great blog post by my friend and the great educator Leo Collado-Torres on archetypes.    A Tablespoon of Archetypes One of the easiest things you can do for yourself is customize your site‚Äôs archetypes. From the Hugo docs:\n ‚ÄúArchetypes are templates used when creating new content.‚Äù\n Right away when I cloned the tidyverse site, I noticed that there were instructions for how to contribute a new article (or blog post) in the README.md and in a separate CONTRIBUTING.md file. Then I noticed this open GitHub issue from Mara Averick (the tidyverse developer advocate) titled ‚ÄúFix README/CONTRIBUTING so there‚Äôs one source of mechanical info?‚Äù.\nI also noticed that there was no project root folder called archetypes, which is where you would store your custom site archetype files as .md files. In fact, there is no theme folder as you might expect either, which is where you could view the default theme archetypes. Let‚Äôs look at some from other Hugo themes:\nThe default Hugo theme for blogdown, Lithium, has just one archetype: default.md\n--- title: \u0026#39;\u0026#39; date: \u0026#39;\u0026#39; --- In contrast, the Hugo Academic theme has A LOT: https://github.com/gcushen/hugo-academic/tree/master/archetypes; here is the content of the one for new posts:\n+++ title = \u0026quot;{{ replace .Name \u0026quot;-\u0026quot; \u0026quot; \u0026quot; | title }}\u0026quot; subtitle = \u0026quot;\u0026quot; # Add a summary to display on homepage (optional). summary = \u0026quot;\u0026quot; date = {{ .Date }} draft = false # Authors. Comma separated list, e.g. `[\u0026quot;Bob Smith\u0026quot;, \u0026quot;David Jones\u0026quot;]`. authors = [] # Tags and categories # For example, use `tags = []` for no tags, or the form `tags = [\u0026quot;A Tag\u0026quot;, \u0026quot;Another Tag\u0026quot;]` for one or more tags. tags = [] categories = [] # Projects (optional). # Associate this post with one or more of your projects. # Simply enter your project\u0026#39;s folder or file name without extension. # E.g. `projects = [\u0026quot;deep-learning\u0026quot;]` references # `content/project/deep-learning/index.md`. # Otherwise, set `projects = []`. # projects = [\u0026quot;internal-project\u0026quot;] # Featured image # To use, add an image named `featured.jpg/png` to your page\u0026#39;s folder. [image] # Caption (optional) caption = \u0026quot;\u0026quot; # Focal point (optional) # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight focal_point = \u0026quot;\u0026quot; +++  A quick note: you may have noticed differences in both the content between these two files but also the structure. The first is a YAML file, the second is a TOML file. For blogdown users, you may want to use YAML. This is also why I recommend when you set up your site to use the to_yaml = TRUE option (in the Project Wizard from figure 1, check the ‚ÄúConvert all metadata to YAML‚Äù box; otherwise, the exampleSite will contain TOML instead of YAML)2.\nIf you read the original tidyverse CONTRIBUTING.md file, the instructions include a fair bit of R code that I would guess means a lot of copying and pasting into new posts. For example, the R Markdown setup chunk and the code for using usethis::use_tidy_thanks() for package releases. I studied the contributing guidelines, and parsed three different ‚Äúkinds‚Äù of articles that are commonly contributed, each with a different archetype:\nThe default.md- this is just for plain old markdown posts and basically sets up the YAML of the post to be the same as it is now (currently, there is no archetype dictating the content- it is pulling from a project-level .Rprofile).\n A default-rmarkdown.md which should only be used with an R Markdown post and provides only the setup chunk at the top.\n A package-release.md which also should only be used with an R Markdown post and adds the usethis::use_tidy_thanks() code chunk (this is pseudo-code so the default chunk option is set to eval = FALSE).\n  So I drafted a pull request that adds these three archetypes to the GitHub repository for the tidyverse.org. Here is the ‚Äúafter‚Äù Addin view:\nHere‚Äôs hoping Hugo archetypes make some things about adding new content to your site easier. There is no Hugo involved, other than realizing that Hugo will look first in your themes/\u0026lt;THEME-NAME\u0026gt;/archetypes/ folder, then in your project root archetypes/ folder next. DO NOT TOUCH any files in your themes/ directory.3\nYou may want to set up archetypes for your blogdown site if you have a ‚Äúsignature‚Äù R setup chunk that loads your preferred knitr chunk options, common libraries you always load at setup like tidyverse, ggplot2 themes you prefer (theme_minimal() FTW), etc. This may be especially helpful if you have multiple team members contributing to a single site and you want their posts to have a uniform setup. Then archetypes can be a real time- and sanity-saver. Get more ideas from Leo‚Äôs blog post on archetypes. You can also make directory based archetypes if you use Hugo page bundles, which is a topic of a future post.\n  These setup options are newish to the blogdown package: https://github.com/rstudio-education/arm-workshop-rsc2019/issues/8‚Ü©\n If you end up with TOML in your content files, run this R code: hugo_convert(to = \u0026quot;YAML\u0026quot;, unsafe = TRUE)‚Ü©\n Trust me on this one- if you ever want to update your site this will make that process way harder.‚Ü©\n   ","date":1550534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550534400,"objectID":"a160260aa45ac1699baf5f119b4c4e60","permalink":"https://kushagragpt99.github.io/post/2019-02-19-hugo-archetypes/","publishdate":"2019-02-19T00:00:00Z","relpermalink":"/post/2019-02-19-hugo-archetypes/","section":"post","summary":"Why you should use Hugo archetypes in your blogdown site","tags":["blogdown"],"title":"A Spoonful of Hugo: Archetypes","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"bd516ebe91dc31b2b1f084d003009111","permalink":"https://kushagragpt99.github.io/accolades/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/accolades/","section":"","summary":"The highs in my journey","tags":null,"title":"Accolades","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://kushagragpt99.github.io/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"How to get in touch","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://kushagragpt99.github.io/experience/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"My journey so far","tags":null,"title":"Experience","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://kushagragpt99.github.io/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://kushagragpt99.github.io/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3960dd3bdc6f629fb800d1d2aaa7224f","permalink":"https://kushagragpt99.github.io/resume/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/resume/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://kushagragpt99.github.io/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks","type":"widget_page"},{"authors":["Kushagra Gupta","Aditya Gulati","Esha Gupta","Abhinav Airan"],"categories":["misc"],"content":"Our team won the silver medal in the competition. The problem involved data analysis and inference from a dataset for a globular star cluster. The datasets had the coordinates, color index (for different wavelengths) and magnitude of every star in the cluster. The task involved using the datasets appropriately to analyze the globular cluster to determine some parameters and peculiarities of the same.\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"e379e491aa91ff7245148d6c5f4cfe21","permalink":"https://kushagragpt99.github.io/project/techmeet/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/project/techmeet/","section":"project","summary":"Data science hackathon on analysing globular star clusters.","tags":["misc"],"title":"Data Science Hackathon, Inter IIT Tech Meeth 2018","type":"project"},{"authors":["Kushagra Gupta","Harish Rajagopal","Harsh Sinha","Apurv Gupta"],"categories":["ml"],"content":"   The problem was part of the machine learning competition in inter IIT tech meet, 2018, where we won the silver medal in the competition. The task was to classify all pixels in a given satellite image, as belonging to one of nine given classes. Our approach towards solving this challenge involved three approaches, with each approach performing better than the last one. The Ô¨Årst model involved the implementation of a basic U-Net architecture with custom metrics designed for this problem. The second approach involved a P-Net architecture with extensive hyperparameter tuning to improve model accuracy. The third approach utilizes nine distinct U-Net architectures for the segmentation, giving results comparable to the state of the art model for multi-class image segmentation.\nMy contributions to the project :\n I designed and implemented a U-Net architecture for image segmentation of high-quality satellite images by using context-based representations. I improved the existing accuracy from 84% to 91% on just 25 images by developing a new algorithm based on 9 U-Nets using \u0026lsquo;one vs all\u0026rsquo; classification approach. I used localized optimization of parameters with high frequency to break the bottleneck of a small dataset.  ","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"5e988f48affaca82772d2d76feabc6bc","permalink":"https://kushagragpt99.github.io/project/eits/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/project/eits/","section":"project","summary":"Designed and implemented a U-Net architecture for image segmentation of high quality satellite images by using context-based representations.","tags":["ml"],"title":"Eye in the Sky","type":"project"},{"authors":["alison"],"categories":["readr","readxl","data import"],"content":"  A shorter version of this blog post now appears as an article vignette for the readxl package, thank you to Jenny Bryan for the invitation!   A problem I run up against a lot when working with other people‚Äôs data is having multiple header rows in the source data file. I like to use readr functions to read in rectangular data like .csv and .tsv files, but if you skip rows at import using the skip argument, you lose the header row as well, which usually has column names. The problem I often have is that the header row has column names that I want to keep, but I‚Äôd like to skip the second row (or more), which has some junk in it. Usually this row is some kind of data dictionary inserted between the row of column names and the actual data.\nIn this post, I‚Äôll walk through a solution to this problem, using the readr package. You can also watch along in the video.\n  Warning!: I made a mistake when I said readr uses the first 100 rows of your data to predict column types- it uses the first 1000 rows.\n Download stickers.csv Being sticker rich This dataset is from an article published in PLOS ONE called ‚ÄúBeing Sticker Rich: Numerical Context Influences Children‚Äôs Sharing Behavior‚Äù. In this study, children (ages 3‚Äì11) received a small (12, ‚Äústicker poor‚Äù) or large (30, ‚Äústicker rich‚Äù) number of stickers, and were then given the opportunity to share their windfall with either one or multiple anonymous recipients. This type of experimental design is a version of the Dictator Game.\nThe main research questions the authors explored were: do the number of available resources and/or the number of potential recipients alter the likelihood of a child donating and/or the amount they donate? But, in order to answer this question, we have to be able to read in the data! Luckily, these lovely developmental psychologists opted to share their data on the Harvard Dataverse as a tab-delimited file.\nIf you download the file, you can open it up in a plain text editor. You can also open it with Microsoft Excel.  Read in the file Let‚Äôs start by creating a variable called link to store the link to the data file.\n# create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; The file has a .tab extension, so we know it is tab-delimited. This means that the right readr function for reading this file is read_tsv. Since we stored our link already as a character string, that is the only argument to the read_tsv function.\n#install.packages(\u0026quot;readr\u0026quot;) library(readr) # load the readr package stickers \u0026lt;- read_tsv(link) # spec() Now, we know the second row of data is wonky, but how can we see that in R? There are a number of ways we can go spelunking around into our data file. The easiest to print it. Since we used readr, we have a tibble, which nicely prints to screen.\nstickers # # A tibble: 402 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 [Included Sa‚Ä¶ 1=12:1; ‚Ä¶ 1=12; 2=30 1=1 recipient;‚Ä¶ 1=fem‚Ä¶ NA # 2 1 1 1 1 1 36 # 3 2 1 1 1 2 36 # 4 3 1 1 1 2 36 # 5 4 1 1 1 1 36 # 6 5 1 1 1 2 36 # 7 6 1 1 1 2 36 # 8 7 2 1 2 1 36 # 9 8 2 1 2 2 36 # 10 9 3 2 1 2 36 # # ‚Ä¶ with 392 more rows, and 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, # # Agegroups \u0026lt;chr\u0026gt;, `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, # # RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; Unfortunately, dplyr::glimpse can‚Äôt help us much, because we have one variable name that is ridiculously long (absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)). We‚Äôll fix that with dplyr::rename.\nlibrary(dplyr) glimpse(stickers) # Observations: 402 # Variables: 18 # $ SubjectNumber \u0026lt;chr\u0026gt; ‚Ä¶ # $ Condition \u0026lt;chr\u0026gt; ‚Ä¶ # $ NumberStickers \u0026lt;chr\u0026gt; ‚Ä¶ # $ NumberEnvelopes \u0026lt;chr\u0026gt; ‚Ä¶ # $ Gender \u0026lt;chr\u0026gt; ‚Ä¶ # $ Agemonths \u0026lt;dbl\u0026gt; ‚Ä¶ # $ Ageyears \u0026lt;dbl\u0026gt; ‚Ä¶ # $ Agegroups \u0026lt;chr\u0026gt; ‚Ä¶ # $ `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt; ‚Ä¶ # $ LeftEnvelope \u0026lt;chr\u0026gt; ‚Ä¶ # $ RightEnvelope \u0026lt;chr\u0026gt; ‚Ä¶ # $ `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt; ‚Ä¶ # $ `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt; ‚Ä¶ # $ Giveornot \u0026lt;chr\u0026gt; ‚Ä¶ # $ LargerEnvelopeabs \u0026lt;chr\u0026gt; ‚Ä¶ # $ LargeEnvelopepercent \u0026lt;chr\u0026gt; ‚Ä¶ # $ SmallerEnvelopeabs \u0026lt;chr\u0026gt; ‚Ä¶ # $ SmallEnvelopepercent \u0026lt;chr\u0026gt; ‚Ä¶ More options:\nhead(stickers) # # A tibble: 6 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 [Included Sa‚Ä¶ 1=12:1; ‚Ä¶ 1=12; 2=30 1=1 recipient;‚Ä¶ 1=fem‚Ä¶ NA # 2 1 1 1 1 1 36 # 3 2 1 1 1 2 36 # 4 3 1 1 1 2 36 # 5 4 1 1 1 1 36 # 6 5 1 1 1 2 36 # # ‚Ä¶ with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;, # # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; tail(stickers) # # A tibble: 6 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 396 1 1 1 2 136 # 2 397 4 2 2 1 136 # 3 398 1 1 1 1 137 # 4 399 1 1 1 2 137 # 5 400 4 2 2 2 139 # 6 401 3 2 1 1 143 # # ‚Ä¶ with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;, # # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; names(stickers) # [1] \u0026quot;SubjectNumber\u0026quot; # [2] \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; # [4] \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; # [6] \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; # [8] \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; # [10] \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; # [12] \u0026quot;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026quot; # [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; # [14] \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; # [16] \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; # [18] \u0026quot;SmallEnvelopepercent\u0026quot; # View() Now we are ready to diagnose the problem!\nProblem: the first row is not really data. It is metadata about the variables, and it is screwing up readr‚Äôs ability to predict our column types.\nSolution: we‚Äôll use readr and the read_tsv() function to read in the data twice. In Step 1, we‚Äôll create a character vector of the column names only. In Step 2, we‚Äôll read in the actual data and skip the multiple header rows at the top. When we do this, we lose the column names, so we use the character vector of column names we created in Step 1 instead.\n Read in the file (again) Step 1 Goal: we want to read in the first row only and save it as a character vector called sticker_names. This row contains the correct column names that we‚Äôll need in Step 2.\nsticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() sticker_names # [1] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; \u0026quot;stickersgiven\u0026quot; # [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; \u0026quot;SmallEnvelopepercent\u0026quot; glimpse(sticker_names) # chr [1:18] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; \u0026quot;NumberStickers\u0026quot; ...  Step 2 Goal: we want to read in all the rows except for the first two rows, which contained the variable names and variable descriptions. We want to save this as stickers, and set the column names to the sticker_names object we created in Step 1.\nstickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names) glimpse(stickers) # Observations: 401 # Variables: 18 # $ SubjectNumber \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1‚Ä¶ # $ Condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3,‚Ä¶ # $ NumberStickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,‚Ä¶ # $ NumberEnvelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,‚Ä¶ # $ Gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2,‚Ä¶ # $ Agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, ‚Ä¶ # $ Ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,‚Ä¶ # $ Agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶ # $ `Subject\u0026#39;sEnvelope` \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 3‚Ä¶ # $ LeftEnvelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18‚Ä¶ # $ RightEnvelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA‚Ä¶ # $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18‚Ä¶ # $ `PercentGiven(Outof100percent)` \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.‚Ä¶ # $ Giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,‚Ä¶ # $ LargerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA‚Ä¶ # $ LargeEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.500000‚Ä¶ # $ SmallerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA‚Ä¶ # $ SmallEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.500000‚Ä¶   Fin! All together now: the final solution!\n# load packages library(readr) library(dplyr) # create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; # read in column names only sticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() # read in data, set column names stickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names)  Addendum For good measure, I would add a final step to everything above and use janitor::clean_names() to put all the variable names into snake case. So my final final solution is here:\n# load packages library(readr) library(dplyr) library(janitor) # create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; # read in column names only sticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() # read in data, set column names stickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names) %\u0026gt;% clean_names() stickers # # A tibble: 401 x 18 # subject_number condition number_stickers number_envelopes gender # \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; # 1 1 1 1 1 1 # 2 2 1 1 1 2 # 3 3 1 1 1 2 # 4 4 1 1 1 1 # 5 5 1 1 1 2 # 6 6 1 1 1 2 # 7 7 2 1 2 1 # 8 8 2 1 2 2 # 9 9 3 2 1 2 # 10 10 3 2 1 2 # # ‚Ä¶ with 391 more rows, and 13 more variables: agemonths \u0026lt;dbl\u0026gt;, # # ageyears \u0026lt;dbl\u0026gt;, agegroups \u0026lt;dbl\u0026gt;, subjects_envelope \u0026lt;dbl\u0026gt;, # # left_envelope \u0026lt;dbl\u0026gt;, right_envelope \u0026lt;dbl\u0026gt;, stickersgiven \u0026lt;dbl\u0026gt;, # # percent_given_outof100percent \u0026lt;dbl\u0026gt;, giveornot \u0026lt;dbl\u0026gt;, # # larger_envelopeabs \u0026lt;dbl\u0026gt;, large_envelopepercent \u0026lt;dbl\u0026gt;, # # smaller_envelopeabs \u0026lt;dbl\u0026gt;, small_envelopepercent \u0026lt;dbl\u0026gt; glimpse(stickers) # Observations: 401 # Variables: 18 # $ subject_number \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,‚Ä¶ # $ condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3‚Ä¶ # $ number_stickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2‚Ä¶ # $ number_envelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1‚Ä¶ # $ gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1‚Ä¶ # $ agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, 36‚Ä¶ # $ ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3‚Ä¶ # $ agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶ # $ subjects_envelope \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 30,‚Ä¶ # $ left_envelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18, ‚Ä¶ # $ right_envelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, ‚Ä¶ # $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18, ‚Ä¶ # $ percent_given_outof100percent \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.33‚Ä¶ # $ giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1‚Ä¶ # $ larger_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA, ‚Ä¶ # $ large_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000,‚Ä¶ # $ smaller_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, ‚Ä¶ # $ small_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000,‚Ä¶  Bonus data dictionary As an extra bonus, when you do have extra header rows, you can create a data dictionary using the gather() function from the tidyr package.\nlibrary(tidyr) stickers_dict \u0026lt;- read_tsv(link, n_max = 1) %\u0026gt;% rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% clean_names() %\u0026gt;% gather(variable_name, variable_description) stickers_dict # # A tibble: 18 x 2 # variable_name variable_description # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; # 1 subject_number [Included Sample Only] # 2 condition 1=12:1; 2=12:2, 3=30:1, 4=30:2 # 3 number_stickers 1=12; 2=30 # 4 number_envelopes 1=1 recipient; 2=2 recipients # 5 gender 1=female; 2=male # 6 agemonths \u0026lt;NA\u0026gt; # 7 ageyears \u0026lt;NA\u0026gt; # 8 agegroups 1=3-4yrs; 2=5-6yrs; 3=7-8yrs; 4=9-11yrs # 9 subjects_envelope How many stickers did the child keep for themse‚Ä¶ # 10 left_envelope 1 recipient conditions: How many stickers the s‚Ä¶ # 11 right_envelope 1 recipient conditions: N/A; 2 recipient condit‚Ä¶ # 12 stickersgiven Regardless of condition, the number of stickers‚Ä¶ # 13 percent_given_outof100‚Ä¶ Regardless of condition, the proportion of stic‚Ä¶ # 14 giveornot 1=Donated 1 or more stickers to the recipient(s‚Ä¶ # 15 larger_envelopeabs Raw number of stickers (out of 30: Condition 2 ‚Ä¶ # 16 large_envelopepercent Proportion of stickers (out of 100%; Condition ‚Ä¶ # 17 smaller_envelopeabs Raw number of stickers (out of 30: Condition 2 ‚Ä¶ # 18 small_envelopepercent Proportion of stickers (out of 100%; Condition ‚Ä¶  Useful resources  Great blog post from Lisa DeBruine using readxl to read in data with multiple header rows (including those with merged cells!): https://debruine.github.io/multirow_headers.html This GitHub issue with Hadley‚Äôs response that solved all my problems: https://github.com/tidyverse/readr/issues/179 My original tweet when I discovered this trick!  Neat #rstats #readr #tidyverse solution to read data when 1st row is header + 2nd row is junk, thanks @hadleywickham https://t.co/5TuH7vNaID pic.twitter.com/woZ3HuECge\n\u0026mdash; Alison Presmanes Hill (@apreshill) September 4, 2017   ","date":1531008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531008000,"objectID":"c18d02feb96be2ee747ed7aa57c7bcc4","permalink":"https://kushagragpt99.github.io/post/2018-02-23-read-multiple-header-rows/","publishdate":"2018-07-08T00:00:00Z","relpermalink":"/post/2018-02-23-read-multiple-header-rows/","section":"post","summary":"Using the readr package to sidestep a common problem","tags":null,"title":"Read Data with Multiple Header Rows into R","type":"post"},{"authors":["Kushagra Gupta","Avik Pal"],"categories":["ml"],"content":"   Deep learning has revolutionized many areas of machine learning, and it is poised to do so with recommender systems as well. This project shows how deep autoencoders can be successfully trained even on relatively small amounts of data by using both well established (dropout) and scaled exponential linear units deep learning techniques. We used iterative output re-feeding - a technique which allows dense updates in collaborative Ô¨Åltering, increases the learning rate and further improves generalization performance of the model.\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"a06bd57ad79c53a7a81a511ee7d9a39f","permalink":"https://kushagragpt99.github.io/project/kvpy/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/project/kvpy/","section":"project","summary":"Developed online collaborative filtering based deep learning algorithm recommender based on AutoEncoder using tensorflow.","tags":["ml"],"title":"Autoencoder Recommendation Engine","type":"project"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://kushagragpt99.github.io/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":["Kushagra Gupta","Avik Pal","Ahsan Barkati","Yatin Dandi"],"categories":["ml"],"content":"Sub-project : ‚Äã‚ÄãAn online recommendation system based on collaborative filtering for implicit data using sentiment and frequency dependent weighting schemes. Technical details :\n Implemented a state of the art algorithm for online collaborative filtering based on Fast Matrix Factorization for Online Recommendation with Implicit Feedback (He et al.) using Numpy. Integrated element-wise Alternating Least Squares (eALS) based incremental update strategy for online learning. Developed an online collaborative filtering based deep recommender algorithm based on AutoEncoder in tensorflow. Used the VADER model in NLTK for sentiment analysis of comments. Improved results of algorithm by using interaction count and sentiment dependent weighting scheme for the observed data and a frequency aware weighting scheme for the missing data. Built multiple Kafka consumers and producer for parallely consuming real time interaction data of comments, likes and views to produce online recommendations for users. Used locust to simulate parallel user interaction to test recommendation algorithm. Used an eventually consistent engagement database (Couchbase) for storing user and item based data.  Sub-Project: ‚Äã‚ÄãIdentification and Classification of toxic comments. Technical Details:\n Implemented a Bidirectional LSTM based model using Keras for flagging toxic comments based on six metrics. Built Kafka consumer and producer data-pipelines for recording and processing new comments.  ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"3b21211a6efc492e8e09c5e163210a65","permalink":"https://kushagragpt99.github.io/project/nyo/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/project/nyo/","section":"project","summary":"Developed collaborative filtering based online recommendation engine and a toxic comment classifier for a logistics platform.","tags":["ml"],"title":"Machine Learning for Large Scale Logistics Platform","type":"project"},{"authors":["alison"],"categories":["rladies","xaringan"],"content":" So, you are doing an R-Ladies presentation‚Ä¶that‚Äôs awesome!\nThe short version I made an R-Ladies theme for xaringan slides. My original tweet about it:\nif you want to use @xieyihui\u0026#39;s awesome #xaringan package for #rstats slides but want more #Rladies flavor, there is now a built-in theme for that (with code highlighting)! Thanks to the awesome @RLadiesGlobal starter kit. Update the CSS in your YAML to use üßôüèΩ‚Äç‚ôÄÔ∏èüßû‚Äç‚ôÄÔ∏è pic.twitter.com/YnlGSVAMsl\n\u0026mdash; Alison Presmanes Hill (@apreshill) November 29, 2017  The way to use the theme is to update the YAML like so:\noutput: xaringan::moon_reader: css: [\u0026quot;default\u0026quot;, \u0026quot;rladies\u0026quot;, \u0026quot;rladies-fonts\u0026quot;] Make sure your version of xaringan is up-to-date.\nBelow is a demo slide deck using the theme.\n (view the source .Rmd on GitHub)\n The longer story I recommend Yihui‚Äôs xaringan package for slides. This is an R package, available through GitHub, for creating slideshows with remark.js through R Markdown. This means that you can:\n write all your slides in Markdown text include chunks of R code and rendered output like plots, results, tables, etc. in your slides use git for version control and share your GitHub repository  This makes xaringan ideal for an R-Ladies presentation!1\nTo use the package, you‚Äôll need the devtools package installed so that you can use the install_github function. Then do:\ndevtools::install_github(\u0026#39;yihui/xaringan\u0026#39;) As Yihui points out in the documentation, if you use RStudio, you can use the menu to navigate to File -\u0026gt; New File -\u0026gt; R Markdown -\u0026gt; From Template -\u0026gt; Ninja Presentation, and you will see an R Markdown example.\nI first used xaringan a few months ago. I was working with Yihui on the blogdown book, and had signed up to lead a workshop for the Portland R User group. Obviously, such a workshop could not have powerpoint slides, so it seemed like the perfect time to learn xaringan.\nFor my workshop, I made a simple website for the newly founded R-Ladies PDX using blogdown (Thanks to Augustina and Deeksha, our fearless organizers). So naturally, my slides needed more purple.\nLuckily, the R-Ladies run a tight ship- they have a starter kit on GitHub that details all the pretty purples they like.\nAbout a month after I did the R-Ladies blogdown workshop, I saw this blog post by Yihui:\nFirst, I thought this was such a cool idea and I hope more people make and submit themes. Then I realized, I had already made a theme! I submitted a pull request2, Yihui helped me make some edits to the CSS files to make them more parsimonious with the default theme, I electronically signed a contributor agreement, and now the theme is there for you all to enjoy and use! You use the theme by editing the YAML:\noutput: xaringan::moon_reader: css: [\u0026quot;default\u0026quot;, \u0026quot;rladies\u0026quot;, \u0026quot;rladies-fonts\u0026quot;] If you use the theme and you are on twitter, I‚Äôd love to see it- please mention me on twitter!\nExamples!\n My blogdown workshop slides: ‚ÄúUp and running with blogdown‚Äù (view the source .Rmd on GitHub)    Jessica Minnier‚Äôs slides for ‚ÄúBuilding Shiny Apps: With Great Power Comes Great Responsibility‚Äù     If you are new to xaringan, don‚Äôt miss the wiki!‚Ü©\n Yihui‚Äôs technical instructions for contributors section of that blog post has been revised and is very detailed‚Ü©\n   ","date":1513555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513555200,"objectID":"4bae6dbc2d30b9c31fdd99c5dcdd6a81","permalink":"https://kushagragpt99.github.io/post/2017-12-18-r-ladies-presentation-ninja/","publishdate":"2017-12-18T00:00:00Z","relpermalink":"/post/2017-12-18-r-ladies-presentation-ninja/","section":"post","summary":"A guide to using the R-Ladies xaringan slide theme","tags":["xaringan"],"title":"R-Ladies Presentation Ninja","type":"post"},{"authors":["alison"],"categories":["blogdown","hugo","netlify"],"content":"  1 Read up on blogdown 2 Caveats, disclaimers, etc. 3 In GitHub 4 In terminal 5 In RStudio 6 Build your site in RStudio 7 Deploy in Netlify 8 Going further   1 Read up on blogdown Before you start, I recommend reading the following:\n blogdown: Creating Websites with R Markdown by Yihui Xie and Amber Thomas Making a Website Using blogdown, Hugo, and GitHub pages also by Amber Thomas  I also found this comment by Eric Nantz, the creator of the R-Podcast, in the rbind/support issues section on GitHub to be helpful:\n https://github.com/rbind/support/issues/12   2 Caveats, disclaimers, etc. Even with all the great resources I listed above, getting myself up and running took a few tries, so in this post I‚Äôm passing along what ended up working for me. Everyone‚Äôs mileage may vary, though, depending on your operating system and your approach. About me: I am a macOS user, and I use R, RStudio, Git (usually via GitLab, sometimes via GitHub), and terminal regularly, so I‚Äôm assuming familiarity here with all of these. If that is not you, here are some places to get started:\n For Git: Happy Git with R by Jenny Bryan et al. For RStudio: DataCamp‚Äôs Working with the RStudio IDE (free) by Garrett Grolemund For Terminal: The Command Line Murder Mystery by Noah Veltman, and The UNIX Workbench by Sean Kross  I also have Xcode and Homebrew installed- you will probably need these to download Hugo. If you don‚Äôt have either but are on a mac, this link may help:\n How to install Xcode, Homebrew, Git, RVM, Ruby \u0026amp; Rails on Mac OS X  Finally, I did not want to learn more about a lot of things! For instance, the nitty gritty of static site generators and how domain names work. I am a new mom, and just in the process of writing all this up, I filled up my tea mug twice with ice cold water, and filled my water bottle with scalding hot water. So, where offered, I followed the advice of Yihui and Amber. For example:\n ‚ÄúConsidering the cost and friendliness to beginners, we currently recommend Netlify.‚Äù Sold. ‚ÄúIf you are not familiar with domain names or do not want to learn more about them, an option for your consideration is a free subdomain *.rbind.io offered by RStudio, Inc.‚Äù. Done.   3 In GitHub Go online to your GitHub account, and create a new repository (check to initialize with a README but don‚Äôt add .gitignore- this will be taken care of later). For naming your repo, consider your future deployment plan:\n If you are going to use Netlify to host the site, you can name this repository anything you want!  You can see some of the repo names used by members of the rbind organization here.    If you want to host your site as a GitHub Page, you should name your repository yourgithubusername.github.io (so mine would have been apreshill.github.io). If you are going this route, I suggest you follow Amber‚Äôs instructions instead of mine!   Screenshot above: Creating a new repository in GitHub\n Go to the main page of your new repository, and under the repository name, click the green Clone or download button.\n In the Clone with HTTPs section, click on the clipboard icon to copy the clone URL for your new repository. You‚Äôll paste this text into terminal in the next section.\n   4 In terminal Now you will clone your remote repository and create a local copy on your computer so you can sync between the two locations (using terminal or your alternative command line tool for a Windows machine).\nUse cd to navigate into the directory where you want your repo to be\n Once there, type: git clone [paste]. So my command looked like this:\n  git clone https://github.com/apreshill/apreshill.git And this is what printed to my terminal window:\nCloning into \u0026#39;apreshill\u0026#39;... remote: Counting objects: 3, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. Checking connectivity... done. Close terminal, you are done in there.   5 In RStudio Install blogdown from your RStudio console. If you already have devtools installed like I did, you can just use the second line below:  if (!requireNamespace(\u0026quot;devtools\u0026quot;)) install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;rstudio/blogdown\u0026quot;) Install Hugo using the blogdown package helper function:  blogdown::install_hugo() # or library(blogdown) install_hugo()  This is where my instructions diverge from Ed‚Äôs- he states that blogdown won‚Äôt create a website in your root folder because the README.md file is already there. I didn‚Äôt find that to be the case- I tested this with a new site as well. If one way doesn‚Äôt work for you, try the other!   Use the top menu buttons in RStudio to select File -\u0026gt; New Project -\u0026gt; Existing Directory, then browse to the directory on your computer where your GitHub repo is and click on the Create Project button.  Screenshot above: Creating a new project in an existing directory in RStudio\n Now you should be ‚Äúin‚Äù your project in RStudio. If you are using git for version control, edit your *gitignore file. This file should be viewable in your file viewer pane in RStudio. Below is what it should look like: the first four lines will automatically be in this file if you have set up your RStudio Project, but if you plan to use Netlify to deploy, you need to add the public/ line (read about here.)  .Rproj.user .Rhistory .RData .Ruserdata blogdown .DS_Store # if a windows user, Thumbs.db instead public/ # if using Netlify  6 Build your site in RStudio Now you can finally build your site using the blogdown::new_site() function. But first you should at least think about themes‚Ä¶\n6.1 Picking a theme There are over 90 Hugo themes. So I went back to the blogdown book. Thankfully, Yihui and Amber offer ‚Äúto save you some time, we list a few themes below that match our taste‚Ä¶‚Äù. Huzzah- I went with hugo-academic! Whatever theme you choose, you‚Äôll need to pick one of 3 ways to make your new site:\nIf you are happy with the default theme, which is the lithium theme, you can use:  blogdown::new_site() # default theme is lithium If you want a theme other than the default, you can specify the theme at the same time as you call the new_site function:  # for example, create a new site with the academic theme blogdown::new_site(theme = \u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE) If instead you want to add the theme later (like I did, because I didn‚Äôt see the above example until it was too late!), you can do this:  library(blogdown) new_site() # default theme is lithium # need to stop serving so can use the console again install_theme(\u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE, update_config = TRUE)  Now is a good time to re-read about blogdown::serve_site() and how LiveReload works (and how it blocks your R console by default)   I recommend setting theme_example = TRUE- some themes won‚Äôt provide an example site, but the academic theme did and I found it helpful to see. You can always delete the example content.\n 6.2 Update project options In your project in RStudio, go to the top menu bar of RStudio and select Tools -\u0026gt; Project Options and update following Yihui and Amber‚Äôs instructions.\n 6.3 Edit your configurations Relevant reading:\n blogdown book chapter on configuration Additional detail from Amber You can also view my config.toml file  Now, edit the baseurl in your config.toml file. The URL should always end with a / trailing slash. At this point, you probably haven‚Äôt deployed your site yet, so to view it locally you can use the Serve Site add-in, or run the blogdown::serve_site function. Both of these baseurls worked for me when viewing locally:\nbaseurl = \u0026quot;https://example.com/\u0026quot; baseurl = \u0026quot;/\u0026quot;  Make sure that the baseurl = listed ends with a trailing slash /!   Go ahead and edit all the other elements in the config.toml file now as you please- this is how you personalize your site!\n 6.4 Addins \u0026amp; workflow Relevant reading:\n blogdown book chapter on the RStudio IDE  Addins: use them- you won‚Äôt need the blogdown library loaded in the console if you use the Addins. My workflow in RStudio at this point (again, just viewing locally because we haven‚Äôt deployed yet) works best like this:\nOpen the RStudio project for the site Use the Serve Site add-in (only once due to the magic of LiveReload) View site in the RStudio viewer pane, and open in a new browser window while I work Select existing files to edit using the file pane in RStudio After making changes, click the save button (don‚Äôt knit!)- the console will reload, the viewer pane will update, and if you hit refresh in the browser your local view will also be updated When happy with changes, add/commit/push changes to GitHub  Having blogdown::serve_site running locally with LiveReload is especially useful as you can immediately see if you have totally screwed up. For example, in editing my about.md file, this error popped up in my console after making a change and I was able to fix the error right away:\nStarted building sites ... ERROR 2017/06/08 16:22:34 failed to parse page metadata for home/about.md: (18, 6): missing comma Error: Error building site: Errors reading pages: Error: failed to parse page metadata for home/about.md: (18, 6): missing comma for about.md The above workflow is only for editing existing files or posts, but not for creating new posts. For that, read on‚Ä¶\n 6.5 Posting Relevant reading:\n blogdown book chapter on RStudio IDE blogdown book chapter on output formats: on .md versus .Rmd posts Additional detail from Amber on adding a blog post  Bottom line:\nUse the New Post addin. But, you need the console to do this, so you have to stop blogdown::serve_site by clicking on the red Stop button first. The Addin is a Shiny interface that runs this code in your console: blogdown:::new_post_addin(). So, your console needs to be unblocked for it to run. You also need to be ‚Äúin‚Äù your RStudio project or it won‚Äôt work.\n6.5.1 Draft posts Relevant reading:\n blogdown book chapter on building a website for local preview  Whether you do a markdown or R Markdown post (see below), you should know that in the YAML front matter of your new file, you can add draft: TRUE and you will be able to preview your post using blogdown::serve_site(), but conveniently your post will not show up on your deployed site until you set it to false. Because this is a function built into Hugo, all posts (draft or not) will still end up in your GitHub repo though.\n 6.5.2 New markdown posts Pick one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: Markdown (recommended) Use the console to author a new .md post:  blogdown::new_post() blogdown::new_post(ext = \u0026#39;.md\u0026#39;) # md is the default! Here are the ?new_post arguments:\nnew_post(title, kind = \u0026quot;\u0026quot;, open = interactive(), author = getOption(\u0026quot;blogdown.author\u0026quot;), categories = NULL, tags = NULL, date = Sys.Date(), file = NULL, slug = NULL, title_case = getOption(\u0026quot;blogdown.title_case\u0026quot;), subdir = getOption(\u0026quot;blogdown.subdir\u0026quot;, \u0026quot;post\u0026quot;), ext = getOption(\u0026quot;blogdown.ext\u0026quot;, \u0026quot;.md\u0026quot;))  Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload.    6.5.3 New R Markdown (.Rmd) posts Again, you have your choice of one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: R Markdown (.Rmd) (recommended) Use the console to author a new .Rmd post:  blogdown::new_post(ext = \u0026#39;.Rmd\u0026#39;) # md is the default! After you edit your .Rmd post, in addition to saving the changes in your .Rmd file, you must use blogdown::serve_site- this is how the output html file needs to be generated.\n Do not knit your .Rmd posts- use blogdown::serve_site instead. If you happen to hit the knit button, just Serve Site again to rewrite the .html file.   Ultimately, your YAML front matter looks something like this; note that some but not all features of rmarkdown::html_document are supported in blogdown:\n--- title: \u0026quot;My Awesome Post\u0026quot; author: \u0026quot;John Doe\u0026quot; date: \u0026quot;2017-02-14\u0026quot; output: blogdown::html_page: toc: true toc_depth: 1 number_sections: true fig_width: 6 ---  Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload and your .html file is properly output.    6.5.4 Adding images to a post If you want to include an image that is not a figure created from an R chunk, the recommended method is to:\nAdd the image to your /static/img/ folder, then Reference the image using the relative file path as follows:  ![my-image](/img/my-image.png)    7 Deploy in Netlify Deploying in Netlify through GitHub is smooth. Yihui and Amber give some beginner instructions, but Netlify is so easy, I recommend that you skip dragging your public folder in and instead automate the process through GitHub.\nWhen you are ready to deploy, commit your changes and push to GitHub, then go online to Netlify. Click on the Sign Up button and sign up using your existing GitHub account (no need to create another account) Log in, and select: New site from Git -\u0026gt; Continuous Deployment: GitHub. From there, Netlify will allow you to select from your existing GitHub repositories. You‚Äôll pick the repo you‚Äôve been working from with blogdown, then you‚Äôll configure your build. This involves specifying two important things: the build command and the publish directory (this should be public).\n More about the build command from Netlify: ‚ÄúFor Hugo hosting, hugo will build and deploy with the version 0.17 of hugo. You can specify a specific hugo release like this: hugo_0.15. Currently 0.13, 0.14, 0.15, 0.16, 0.17, 0.18 and 0.19 are supported. For version 0.20 and above, you‚Äôll need to create a Build environment variable called HUGO_VERSION and set it to the version of your choice.‚Äù I opted for the former, and specified hugo_0.19.   You can check your hugo version in terminal using the command hugo version. This is what my output looked like, so I could run version 0.20 if I wanted to through Netlify, but I went with 0.19 and it works just fine.\n$ hugo version Hugo Static Site Generator v0.20.7 darwin/amd64 BuildDate: 2017-05-08T18:37:40-07:00 Screenshot above: Basic build settings in Netlify\n Netlify will deploy your site and assign you a random subdomain name of the form random-word-12345.netlify.com. Mine was particularly unfortunate, with the random word garbage-collector-janice. You should know that you can change this; I changed mine to apreshill.netlify.com.\n Anytime you change your subdomain name, you need to update the baseurl in your config.toml file (so I changed mine to baseurl = ‚Äúhttps://apreshill.netlify.com/‚Äù).   At this point, you should be up and running with blogdown, GitHub, and Netlify, but here are some ideas if you want to go further‚Ä¶\n 8 Going further 8.1 Custom CSS I like to tinker with default theme settings like colors and fonts. Every Hugo theme is structured a little differently, but if you are interested, you can check out my custom css to see how I customized the academic theme, which provides a way to link to a custom CSS file in the config.toml file:\n # Link custom CSS and JS assets # (relative to /static/css and /static/js respectively) custom_css = [\u0026quot;blue.css\u0026quot;]  8.2 Formspree I used Formspree to make a contact form, which is an online service (managed on GitHub) that allows you to add an HTML form to your static site. No registration, just use the form and confirm your email address once. I added the following code into my contact widget:\n\u0026lt;form action=\u0026quot;https://formspree.io/your@email.com\u0026quot; method=\u0026quot;POST\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;name\u0026quot;\u0026gt;Your name: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;name\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;email\u0026quot;\u0026gt;Your email: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;email\u0026quot; name=\u0026quot;_replyto\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;message\u0026quot;\u0026gt;Your message:\u0026lt;/label\u0026gt;\u0026lt;br\u0026gt; \u0026lt;textarea rows=\u0026quot;4\u0026quot; name=\u0026quot;message\u0026quot; id=\u0026quot;message\u0026quot; required=\u0026quot;required\u0026quot; class=\u0026quot;form-control\u0026quot; placeholder=\u0026quot;I can\u0026#39;t wait to read this!\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_next\u0026quot; value=\u0026quot;/html/thanks.html\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;submit\u0026quot; value=\u0026quot;Send\u0026quot; name=\u0026quot;submit\u0026quot; class=\u0026quot;btn btn-primary btn-outline\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_subject\u0026quot; value=\u0026quot;Website message\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;_gotcha\u0026quot; style=\u0026quot;display:none\u0026quot; /\u0026gt; \u0026lt;/form\u0026gt;  8.3 *.rbind.io domain names You may want a different domain name than the one provided by Netlify. I opted for a free subdomain *.rbind.io offered by RStudio. To do the same, head over to the rbind/support GitHub page and open a new issue. All you need to do is let them know what your Netlify subdomain name is (*.netlify.com), and what you want your subdomain name to be (*.rbind.io). The awesome rbind support team will help you take it from there!\n Again, you will need to update the baseurl in your config.toml file to reflect your new rbind subdomain name (so mine is baseurl = ‚Äúhttps://alison.rbind.io/‚Äù).    8.4 Have fun! Lastly, don‚Äôt forget to just have fun with it. Happy blogdowning!\n  via GIPHY   ","date":1497225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497225600,"objectID":"dbf96d61412119467fb5db386d4f4407","permalink":"https://kushagragpt99.github.io/post/2017-06-12-up-and-running-with-blogdown/","publishdate":"2017-06-12T00:00:00Z","relpermalink":"/post/2017-06-12-up-and-running-with-blogdown/","section":"post","summary":"A guide to getting up and running with blogdown, GitHub, and Netlify","tags":["blogdown"],"title":"Up \u0026 Running with blogdown","type":"post"},{"authors":["","Ahsan Barkati"],"categories":null,"content":"   This talk was part of Programming Club, IIT Kanpur\u0026rsquo;s talk on reinforcement learning (RL). It was targeted for sophomores and junior undergraduates with some statistical background on Markov process and Monte Carlo. It covered the components of an RL model, namely policy, value function and agent\u0026rsquo;s representation of the environment. Additionally, it covered the basics of Markov reward process and the Bellman expectation equation, necessary to define the update procedure of the RL agent mathematically. This talk also covered the basic algorithms of training the RL agent, namely policy and value iteration. Towards the end, it touched upon the model-free methods of RL and explained the underlying mechanism behind model-free learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"271d8fb7cd4b84326cda1763be3c7784","permalink":"https://kushagragpt99.github.io/talk/2019-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/2019-rl/","section":"talk","summary":"This talk covers the terms encountered in RL and the mathematical concepts used in RL models.","tags":["Reinforcement Learning","Markov process","Monte Carlo"],"title":"Reinforcement Learning","type":"talk"}]