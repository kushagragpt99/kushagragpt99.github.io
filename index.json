[{"authors":["Kushagra"],"categories":null,"content":"I am a masters student in the Department of Statistics, Stanford University. As far as I can remember, I have always been drawn towards the dense logical framework of mathematics. In particular, I am fascinated by how simple and intuitive concepts in statistics contribute towards developing intricate methods of solving complex problems.\nI completed my undergrad at the Indian Institute of Technology Kanpur (IITK) in Mathematics and Scientific Computing with a minor in Machine Learning. The excellent array of opportunities in IITK and my insatiable curiosity have allowed me to explore various areas in statistics. My work with Prof. Dootika Vats on output analysis of parallel Markov chain Monte Carlo has resulted in a manuscript currently under review with JMLR. I have had the privilege of developing a Bayesian framework for identifying dynamic models in climate change and healthcare under real-life assumptions like data sparsity and acute parameter interdependence. My manuscript addressing this problem with Dr. Vats and Prof. Snigdhansu Chatterjee (Dept. of Statistics, University of Minnesota) is currently under review with Bayesian Analysis.\nI\u0026rsquo;ve also worked extensively on deep learning in college from a research as well as industrial perspective. My internship experience in developing online recommendation engines and image segmentation have familiarized me with complex neural network architectures. My recent experiences on developing trading strategies for a quantitative trading firm introduced me to the power and wide applicability of statistics in real-world problems. I am always looking for opportunities to work on interesting and challenging problems to keep my brain churning.\nI have explored the field of Computer Architecture in the past few months. The aggressive hardware optimizations employed in current processors interest me. I'm\rworking with Biswa in CAR3S Group to play around and see if they can be exploited by an attacker.\nIf you're interested, I've found a Medium to present my experiences in words. I'm also in the process of compiling a fairly comprehensive set of resources to introduce interested folks to Computer Systems, the details of which are available here: Systems Reading Group --\r","date":1601683200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1601683200,"objectID":"0147088b0bad3f329b27707805a8a018","permalink":"/authors/kushagra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kushagra/","section":"authors","summary":"I am a masters student in the Department of Statistics, Stanford University. As far as I can remember, I have always been drawn towards the dense logical framework of mathematics. In particular, I am fascinated by how simple and intuitive concepts in statistics contribute towards developing intricate methods of solving complex problems.","tags":null,"title":"","type":"authors"},{"authors":["Kushagra Gupta, Dootika Vats, Snigdhansu Chatterjee"],"categories":null,"content":"","date":1610496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610496000,"objectID":"6dc778a1fb267bf3846049235fe8f1d0","permalink":"/publication/preprints/2021-bayesian-equation-selection/","publishdate":"2021-01-13T00:00:00Z","relpermalink":"/publication/preprints/2021-bayesian-equation-selection/","section":"publication","summary":"We present a Bayesian framework for discovering this system of differential equations under assumptions that align with real-life scenarios, including the availability of relatively sparse data.","tags":["Bayesian variable selection","linchpin MCMC","spike-and-slab priors","Lorenz systems"],"title":"Bayesian equation selection on sparse data for discovery of stochastic dynamical systems","type":"publication"},{"authors":["","Medha Agarwal"],"categories":null,"content":"   Markov chain Monte Carlo (MCMC) is a popular method of generating correlated samples from complex multi-dimensional distributions where i.i.d sampling is not possible. MCMC finds application in a wide variety of fields, Bayesian machine learning, optimization, and econometrics, to name a few. With the recent advancements in computing power, long runs of MCMC have become accessible to practitioners with parallel implementation of MCMC emerging as a popular choice. This trend has motivated research that answers some fundamental questions pertinent to sampling. In this talk, we will give an intuitive understanding of how and why the algorithm works and what are the best practices in MCMC. Additionally, we will talk about diagnostics that determine the quality of MCMC algorithms. Towards the end, we will address the problem of output analysis of parallel MCMC by discussing globally centered Monte Carlo error estimators.\n","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"520cadcc46a7ed90b8e0599ac526b3c7","permalink":"/talk/2020-mcmc/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/talk/2020-mcmc/","section":"talk","summary":"A talk on MCMC under the aegis of Special Interest Group in Machine Learning ([SIGML](https://www.cse.iitk.ac.in/users/sigml/)).","tags":["mcmc","statistics"],"title":"Markov Chain Monte Carlo","type":"talk"},{"authors":["Kushagra Gupta, Dootika Vats"],"categories":null,"content":"","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"9114e0c8b74b3dd9e11e5171294c75fe","permalink":"/publication/preprints/2020-replicated-batch-means/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/publication/preprints/2020-replicated-batch-means/","section":"publication","summary":"We propose a multivariate replicated batch means (RBM) estimator that utilizes information across multiple chains in order to estimate the asymptotic covariance matrix.","tags":["replicated batch means","MCMC","Output Analysis","Operations Research"],"title":"Estimating Monte Carlo variance from multiple Markov chains","type":"publication"},{"authors":["Kushagra Gupta","Dootika Vats","Snigdhansu Chatterjee"],"categories":["stats"],"content":"Often the underlying system of differential equations driving a stochastic dynamical system is assumed to be known, with inference conditioned on this assumption. We present a Bayesian framework for discovering this system of differential equations under assumptions that align with real-life scenarios, including the availability of relatively sparse data. Further, we discuss computational strategies that are critical in teasing out the important details about the dynamical system and algorithmic innovations to solve for acute parameter interdependence in the absence of rich data. This gives a complete Bayesian pathway for model identification via a variable selection paradigm and parameter estimation of the corresponding model using only the observed data. We present detailed computations and analysis of the Lorenz-96, Lorenz-63, and the Orstein-Uhlenbeck system using the Bayesian framework we propose.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"a9a5238f3973eeaa7b0105c15bb7b783","permalink":"/project/comp_model/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/comp_model/","section":"project","summary":"A Bayesian framework for discovering this system of differential equations under assumptions that align with real-life scenarios.","tags":["stats"],"title":"Bayesian equation selection with spike and slab priors","type":"project"},{"authors":[],"categories":[],"content":"A few weeks ago, I wrapped up teaching tidymodels for the third time. We say third time\u0026rsquo;s the charm, right? Even during a global pandemic? I don\u0026rsquo;t know, but this time around was fun in new ways and hard in new ways, so I wanted to take the time to write some thoughts down about how it went.\nBut why? Why teach tidymodels virtually? I\u0026rsquo;ve already taught it as a 2-day workshop at rstudio::conf(2020). I had already agreed to teach intro to machine learning with tidymodels as a full-day workshop for the Cascadia R Conf (which unfortunately was cancelled due to COVID), and the R / Medicine conference (still on, and 100% virtual!).\nI had three main goals:\n First, I wanted to do a good job for the R / Medicine conference workshop in August. This seemed like an interesting teaching challenge. One of the best compliments I\u0026rsquo;ve ever gotten from a colleague is that I \u0026ldquo;teach with heart\u0026rdquo; — so the challenge was, can a virtual workshop have a ❤️?    Second, given current events, I saw an opportunity to document a good system (tooling + pedagogy + logistics) around virtual workshops. I don\u0026rsquo;t see them going away anytime soon. My colleague, Greg Wilson, had also been giving our RStudio certified instructor workshops virtually for over a year, so I trusted that he could help me navigate.\n  Third, help out the R / Medicine conference organizers. My partner-in-crime for this particular workshop, Dr. Stephan Kadauke, wanted to join me for some reconnaissance work. He is one of the conference organizers and will be leading his own workshop there, so he wanted to test out the tooling and the pedagogy too.\n  So we thought, three birds, one stone: I get to pilot a much shorter version of my conf workshop materials, we get to test doing it virtually, and the R / Medicine conference organizers learn how this could all work in August.\nHow we planned it Starting out, I knew I had two main hurdles:\n Shaving two days of workshop content into X days Logistics (like figuring out what X should be!)  The first decision based on conversations with Greg was to offer the workshop across two half-days. This is how the RStudio instructor training is timed, and it works well because there are few people who can (or want to) be tied up for a full day, especially if under stay-at-home orders. We asked Stephan if his group at the Children\u0026rsquo;s Hospital of Philadelphia (CHOP) would be up for two 4-hour sessions. Stephan\u0026rsquo;s feedback was yes, but given that these folks at CHOP typically have weekly schedules, having two consecutive days would not work. So we opted for:\n Two 4-hour sessions that were\u0026hellip; exactly one week apart.  I\u0026rsquo;m in Oregon, they were in Philly, so we decided to start at 9am my time (1pm for them) and wrap up at 1pm my time (5pm for them). After making this call, and again with Greg\u0026rsquo;s sage advice to take a break every hour, I started working on a rough schedule. I had about 8 hours total to work with; about half of my conf workshop. We decided on:\n 50 minute chunks, and 10 minute breaks at 10 till the hour every single hour.  I didn\u0026rsquo;t try to make sure my materials for each session filled exactly 50 minutes. Instead, I promised the group that I would break wherever I was at the same time, and after each break we just picked up where left off.\nHere was the new topic outline:\nDay 1   Session 00: Intro (include a tooling tour- orient to Google doc + Zoom)  Session 01: Build a model (mainly parsnip package)  Session 02: Resample a model (add rsample package, plus tune package for fit_resamples())  Session 03: Build a better training set (add recipes and workflows packages)  Day 2   Session 04: Build an ensemble model (back to parsnip, now with model arguments)  Session 05: Tune a model (heavy tune package)  Session 06: The Great Model-Off (a Kaggle-like group activity)  The final decision was about tooling. Luckily, Greg had advice here too. We went with:\n Google doc as the \u0026ldquo;home page\u0026rdquo; + chat (no workshop website!) RStudio Cloud for all exercises Zoom for video  Pre-workshop launch list  Make shared Google doc and prepopulate with: Zoom link, RStudio Cloud link, bulleted list of participant names (be sure to make this editable for anyone!) Email everyone with a Google calendar invite that includes the Zoom link and a link to a shared Google doc (be sure to make this editable for anyone!) Ask everyone to fill in their 2-sentence bios ahead of time to ensure that you\u0026rsquo;ve done this correctly!  The Zoom link and the Google doc link should be the only links that attendees see ahead of time. Then, the Google doc is the one true source for everything. Too many links at first leads to confusion later. I also prepopulated the doc with my session outline with HTML links to each session\u0026rsquo;s slide deck.\nActual launch In Zoom, I set it up to mute everyone as they joined. We started with an orientation of the tools, the schedule, and the general plan for how they would work and interact with each other. We tried to keep questions per session in the Google doc, which my TA Stephan fielded in real time, which now is a great resource for me as I prepare for this again in August.\nWhat I should have done:\n Have a Code of Conduct. I will next time. We didn\u0026rsquo;t have any issues, but what came up later was that I asked participants to turn on their cameras. I wished that part of my Code of Conduct was that we would not videotape or take screenshots at any point to protect the privacy of all participants. Asked folks to make sure they had an updated version of Zoom. In particular, some newer security issues have been addressed recently, so this is nice for everyone to take advantage of. Plus the interface looked different for some. Locked down the Zoom room. After a few minutes of starting, it was super distracting to have late arrivals who kept sending me personal chats asking for the links. In addition to locking down after the first 10-15 minutes, I also should have assigned my TA as a co-host, so that he could have helped me manage that. We did this on day 2 and it worked great.  How did it go? So, I\u0026rsquo;m not going to lie here. The first session of teaching spooked me a bit. This was because all attendees left their videos off and I felt like:\nIt was eerie to teach to a silent void. On day 2, I asked for two volunteers to turn on their cameras for a single 50 minute block each because it really helped me to see faces. Participants probably kept their own view as \u0026ldquo;speaker only\u0026rdquo;, but for me it really helped to be able to have some human feedback, even if they were muted. Much love to the head nodders out there. This conversation on day 2 broke my heart though, because several participants indicated they felt they couldn\u0026rsquo;t turn on their cameras because they had young children at home. It is hard.\nInterestingly, the participants didn\u0026rsquo;t sense my discomfort at all. In fact, I heard from several that it was nice to see me up close and so personal. It actually felt more personal than a large in-person workshop, to my surprise.\nOn day 1, I started by asking participants to use Zoom reactions (like thumbs up) to answer questions, give me progress updates, etc. I ended up retiring this- it was distracting and the reactions disappear so it wasn\u0026rsquo;t actually useful. Instead, I asked people to use the Zoom chat to indicate \u0026ldquo;done\u0026rdquo; or give quick one-word answers (a or b, yes or no).\nStephan also had a great idea for the breaks. On day 2, I started using Garrick Aden-Buie\u0026rsquo;s countdown app to show the 10 minute break countdown full-screen. I used the hosted version here.\nWhat can I do better? Logistically, I got really frustrated because I kept losing my Zoom meeting controls. Later I found out about this accessibility setting, which would have helped!\nMore substantively, as I mentioned, I would have a Code of Conduct at the very beginning. I also think virtual workshops offer a unique opportunity to include some more creative exercise types. Here are a few I brainstormed with Greg Wilson after the fact—expect to see these at R / Medicine if you attend with me!\n  Spot the bug- do in groups\n  Unscramble code\n  Predict what is going to happen\n  Fill in the blank with the flair package\n  Verdict Transitioning from primarily teaching in person to teaching virtually is hard, and I\u0026rsquo;m in awe of all the instructors I know who have had to do this on very short notice. But, can it be done with heart? Yes, I think so ❤️\nThanks Thanks to the participants, who were the loveliest guinea pigs. It is a hard time to learn and a hard time to find time. I appreciate that you took time out of your lives to spend 8 hours with me.\nThanks also to Greg Wilson for his support, and Stephan Kadauke for being an awesome co-pilot. Extra special thanks to Desirée De Leon, who has the biggest heart of all. Knowing that I didn\u0026rsquo;t have time for creativity with my slides, she surprised me with the most beautiful xaringan slide deck theme based on tidymodels.org. I merged in her PR with glee and delight, I know the participants felt those same emotions too (while learning about machine learning, no less!) 🌼\n      Feedback If you are curious, here is some of the feedback I collected at the bottom of our Google doc:\nTwo half days?  worked well for me. A full day would be tough. Worked well for me also and I also think having all in one day would be a lot. worked well. worked well for me too.  Separated by one week?  seemed fine. I liked this! good. Enough time to digest the previous session yes.  Pace: too fast, too slow?  good pace I liked the pace. It felt like we covered a lot of ground quickly, but also like we have great resources to come back to for refreshing on what we learned. good pace, except at “data leakage”. I still have hard time to understand that part. good pace.  Scope: too small, too big?  nice scope Nice scope very nice and practical. nice scope.  Timing in 50 min chunks: too many or too few breaks?  The 50 min chunks were great! I thought perfect length. Perfect break up. And the 10 min breaks gave enough time to make tea, grab a snack, etc. I am fine with that. works well.  Timed code exercises: too easy, too hard?  good Neither, I thought they were appropriate good. good.  In-between homework/reading?  maybe some light reading or practice In theory I think I would have liked some ‘homework’, but the week turned out to be so busy that I don’t know if I would’ve made time to complete it before the workshop. will be helpful to give some reading about different model descriptions in-between.  Final take-home project (+/- feedback)?  optional I think this could be fun if we had access to a few different datasets to choose from. With maybe some pointers on what to look out for as we explore the data. Is it unbalanced? Could any two variables be collinear? Etc. i would vote for take-home project to play with.  Zoom: video on/off?  I find video distracting while working/listening/learning. I like being able to see the presenter/speaker’s video. I tend to choose “Speaker View” so that I don’t see all the participant videos (which I would find distracting). i am fine with either. I know, from the speaker’s perspective, it will be good to have video on. I think it was nice and respectful to ask volunteers to turn video on part time! (Kudos to Stephan for having it on the whole time!)  Google doc as our “home page” and “discussion forum”?  Great! Yes! I thought it was a really helpful tool and ‘homebase’ to come to for all of the things (slides, R Studio Cloud project, etc.) very good! Much easier to track. works!  Rstudio.cloud?  pretty good except for crashing at end. ditto. I really enjoyed using R Studio Cloud. It makes it easier when I don’t have to worry about pre-installing packages or updating my version of R in order to go into a workshop. Having everything already set up for you in there is super convenient. I like Rstudio cloud generally. Just not suitable for complicated model training.  ","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"e55fbb1e1fbfd9c7d301525253850c35","permalink":"/post/2020-06-02-tidymodels-virtually/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/post/2020-06-02-tidymodels-virtually/","section":"post","summary":"Some reflections on my third time teaching tidymodels for machine learning, and doing it virtually.","tags":["rmarkdown"],"title":"How I Taught Tidymodels, Virtually","type":"post"},{"authors":[],"categories":[],"content":"\rAh, R Markdown. I love teaching R Markdown. But this was not a love at first sight story. When I first started teaching with R at Oregon Health \u0026amp; Science University many moons ago, my students used R Markdown for their homework assignments and in-class labs. This is a great way to start for beginners, to be honest- just use it! I found that most people are not very disoriented by a file that mixes text and code; a concern I had at first and one that I’ve heard voiced by other educators. It seems natural to use if it is your first exposure to all the data science things, especially if you’ve never used an R script.\nI did notice, though, that over the course of even just a few weeks, many of my students would reach unbearable levels of R Markdown curiosity. I would see signs of growing pains in their assignments, like how can I turn off these messages and warnings when I load packages? This is so long- how can I make it easier to find things? How can I make it look better?\nFor all learners, there comes an uncomfortable point where you simply cannot just continue to use this awesome tool without learning how to use it. And I realized when I was teaching that ultimately it was unfair of me to assume that they would just learn an entire ecosystem while I stood idly by, clutching the secrets of being an “R Markdown Whisperer” close to my chest.\nLike, why can’t you just find the piñata, kid? But more importantly, what happens when you can’t? I think the risk is that learners either won’t like or won’t appreciate a tool that I genuinely think would make their working lives better. If I believed it was worth their time to use it, it was worth my time to teach it.\nSo like any good educator, I evolved. I decided that the next year, I would teach R Markdown. And I did. I went from:\n\r“Just use it”\n\rTo:\n\r“Just tell them how to use it already”\n\rI waited until a few weeks into a quarter, and I spent a full lab period walking students through “Hey- this bit where you put the title of each assignment and your name? That’s called YAML! Here is what it does…” I showed them how to use bootswatch themes, make a floating table of contents, control their output using the power of knitr code chunks and chunk options.\nGuess what happened? The next batch of assignment my TA team and I received were…stunning. Creative. Beautiful. Thoughtful and thought-provoking. Students had taken care with their markdown headers, they played with flatly/yeti/united, they cared about not printing the giant output of a code chunk. It was a thing of beauty. I remember one of my beloved TAs, Grace Lawley, commenting that one star student’s homework “brought tears to her eyes” (and yes, I trained Grace too as she was also my research assistant, so she was already baptized into the R Markdown family).\nBut I have to admit, the first time I taught it, I had a hard time teaching it! I realized that I lacked vocabulary around things I used everyday, but had never really talked about out loud with anyone in words.\nNow, in my role at RStudio, I’ve devoted a lot of time and energy trying to figure out how we can make R Markdown easier- easier to discover, easier to debug, easier to use, and easier to talk about.\nSo without further ado, here are some of my guiding principles when introducing R Markdown to beginners, for those who are ready to go beyond casual knitter:\nMake it. Make it again. Knit early. Knit often. That means starting with a pre-filled Rmd document usually that you know will knit. How do you motivate repeated knitting and make it satisfying? Teach the basics of output formats and options by editing mainly the YAML. Your goal is to show how small effort –\u0026gt; high polish. For single docs, I love html_document() with a floating table of contents and a theme (like this), switching quickly to bookdown::html_document2() and distill::distill_article(). The latter two also enable automated figure numbering, which for scientific audiences is quite nice!\nYou can do something like this with #rmarkdown. Ask students to knit to HTML, then use the bootswatch themes https://t.co/J9ucdRWnIl\nBonus: you get the prettiest #rstats homeworks after this! 💐🖍️\n---\ntitle: \u0026quot;Habits\u0026quot; output: html_document: theme: flatly\n--- https://t.co/wMQ3Uqhc7s\n\u0026mdash; Alison Presmanes Hill (@apreshill) March 2, 2019  Bonus when using a theme with html_document(): show off the code_download output option!\nTIL you can embed a \u0026quot;code download\u0026quot; button in an HTML #rmarkdown doc so that users can click to download your source .Rmd from the rendered HTML version...without GitHub 🤩 #rstats\nYAML:\n---\noutput:\nhtml_document:\ncode_download: true\n---\nTest: https://t.co/bp7w7XKF8b pic.twitter.com/uMQK0mvYcF\n\u0026mdash; Alison Presmanes Hill (@apreshill) March 22, 2019  \rMake it pretty. The starting Rmd should have a nice ggplot2 graphic in there, and maybe a pretty gt table too. This is a motivational, aspirational document! I also try to use local data sets, so they can see how that actually works, as opposed to using a data package.\nNever underestimate the power of being able to make pretty #rstats things. ⬆️ polish/effort ratio ➡️ happier users 🦚 https://t.co/LMbFRUiuLa\n\u0026mdash; Alison Presmanes Hill (@apreshill) March 2, 2019  \rMake it snappy. I aim to get to a shareable link in the first 20 minutes (at most!). I like to use Netlify Drop for this. No account sign-up needed, and everyone knows how to drag-and-drop (see video below). It is very satisfying to get a link they can share with their mom/best friend/arch nemesis (kidding). I like to have everyone drop their links in a chat too, like a Slack, Google Doc, or a Gitter channel if doing a workshop. My favorite motto: “if it knits, it ships” 🚢\n\rMake it real. Teach folks what they need to know to actually use the tool productively in real life. If you are an avid R Markdown user, this means that you know without a doubt that file paths will eventually be painful, for example. At the end of an intro, I go back and highlight things I just used to make sure they notice them like R Projects and the here package for data importing. I also love a good “Your Turn” exercise where you get a fresh data dump and all they have to do is re-knit. As in, “Surprise! Now instead of 3 sites, you have data from all 6 sites- what do you do?”\n\rMake it easy. People will only keep using R Markdown if they see it making their life easier. So show them how. For example, the RStudio IDE has some very nice built-in features that make it much easier to be an R Markdown user. I point out things having a global setup chunk, and IDE features like:\n\r\rWhat do I save until later?\nRendering with render(). I think knitting in the RStudio IDE can get you very far- this I consider an intermediate to advanced concept that is confusing if introduced too early. I’ve never heard someone say “Well there is this simple button, but how can I do the same thing from the command line?”\n\rParameterized reports. I actually do use parameters though! I’ll have at least one or two often in one of my Rmds, and mention them briefly in my wrap-up. I teach parameters explicitly with actual exercises when I have \u0026gt; 1 hour.\n\rMultiple Rmd output formats. I tend to start with single output formats first. If I have \u0026gt; 1.5 hours and it matches my learning objectives, then I happily oblige- I love teaching bookdown, blogdown, and distill websites. But to start, I stay single.\n\rMarkdown. I skim markdown, and always provide a link to an interactive tutorial. This one is my favorite. Teaching markdown can be…dry. I show some bits on the slides because I cannot count on everyone doing the interactive tutorial ahead of time, but I do not linger much.\n\r\rIf you are curious to see some of the materials I’ve used to teach R Markdown, click on the rmarkdown button just below this post! 🚀\nBut remember: there is no one way to learn R Markdown, and no one way to teach it either. I love seeing the creativity of the community when introducing the R Markdown family- so keep them coming!\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590723957,"objectID":"726d799b72309e718780e6fc80d1b9f4","permalink":"/post/2020-05-28-how-i-teach-r-markdown/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/post/2020-05-28-how-i-teach-r-markdown/","section":"post","summary":"A handful of guiding principles for introducing beginners to the R Markdown family of packages.","tags":["rmarkdown"],"title":"How I Teach R Markdown","type":"post"},{"authors":null,"categories":["machine learning","tidymodels"],"content":"A few years ago, I did a talk called \u0026ldquo;Take a Sad Plot \u0026amp; Make it Better,\u0026quot; where I showed how I took a single sad plot and tried to make it better. The process of making that plot better taught me a lot about data visualization, and about the ggplot2 package.\nFast-forward to 2019 when I started learning tidymodels, and I have accumulated some pretty sad predictive modeling scripts! And my sad plots are not so lonely anymore. Specifically, my old scripts for doing cross-validation with tidymodels are particularly sad. But, I\u0026rsquo;ve been able to make them better (one might even call them happy), primarily due to changes in the tune package and the addition of the fit_resamples() function. The process of making these scripts better taught me a lot about predictive modeling, and about the (evolving) tidymodels ecosystem. So, why write a blog post with outdated code?\n I want to remember that I did this \u0026ldquo;by hand.\u0026rdquo; I want to remember how I did this \u0026ldquo;by hand.\u0026rdquo; The code still works, even if there is now a happier path to doing the same thing. I want to share cute penguin art and gifs.  Let\u0026rsquo;s start with some cute penguin art by Rohan Chakravarty\u0026hellip;\n\nMy objective here is not to provide an introduction to using tidymodels, cross-validation, or to machine learning. If that is what you came for, check out the project button at the top of this post for my workshop materials for learners, and my associated blog post on the RStudio education site.\n Bottom line: If you are stumbling upon this blog post in the year 2020 or beyond, know that there is a better way!   A sad script symphony 🎻 🎷 🎹 I\u0026rsquo;m not the first person to write sad tidymodels scripts- there are many out in the wild. Here were the blog posts that I found most helpful when trying to solve this particular coding conundrum:\n   Modelling with Tidymodels and Parsnip: A Tidy Approach to a Classification Problem by Diego Usai\n   A tutorial on tidy cross-validation with R by Bruno Rodrigues\n   Modeling with parsnip and tidymodels by Benjamin Sorensen\n  Packages library(tidyverse)\rlibrary(tidymodels)\rlibrary(rpart) # for decision tree\rlibrary(ranger) # for random forest\r Data I\u0026rsquo;m going to use data that Allison Horst helped me source on penguins from the Palmer Station (Antarctica) Long Term Ecological Research Network.\n \u0026ldquo;sooo now I\u0026rsquo;m just looking at penguin pictures\u0026rdquo;\n  Allison Horst after slacking me this penguin data    Update! We have bundled the Palmer Station penguins data into an R data package named palmerpenguins. Enjoy 🐧 Here is the package website: https://allisonhorst.github.io/palmerpenguins/   install.packages(\u0026quot;remotes\u0026quot;) # to install from github\rremotes::install_github(\u0026quot;allisonhorst/palmerpenguins\u0026quot;)\r After you\u0026rsquo;ve installed the package, load it and read about the variables with ?penguins. We\u0026rsquo;ll modify this dataset lightly by:\n casting all characters as factors, dropping any observations with missing data, and dropping the island variable.  library(palmerpenguins)\rtidypenguins \u0026lt;- penguins %\u0026gt;% select(-island) %\u0026gt;% drop_na()\rglimpse(tidypenguins)\r#\u0026gt; Rows: 333\r#\u0026gt; Columns: 7\r#\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, A...\r#\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 3...\r#\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 2...\r#\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198,...\r#\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3...\r#\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, female, male, female, male,...\r#\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2...\r Penguins Figure 1: Artwork by @allisonhorst\n\rThis data included structural size measurements of penguins like their bill length, flipper length, and body mass. It also included each penguin\u0026rsquo;s species and sex. I\u0026rsquo;m going to use this data to try to predict penguin body mass. Sadly, we only have data for three distinct penguin species:\ntidypenguins %\u0026gt;% count(species)\r#\u0026gt; # A tibble: 3 x 2\r#\u0026gt; species n\r#\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r#\u0026gt; 1 Adelie 146\r#\u0026gt; 2 Chinstrap 68\r#\u0026gt; 3 Gentoo 119\r Here is a lineup:\nFrom: https://www.bas.ac.uk/about/antarctica/wildlife/penguins/\nLooks like we have data for 3 of the smaller penguin species (of those pictured here).\nFirst, let\u0026rsquo;s build a simple linear regression model to predict body mass from flipper length.\nggplot(tidypenguins, aes(x = flipper_length_mm, y = body_mass_g)) +\rgeom_point(color = \u0026quot;salmon\u0026quot;, size = 3, alpha = .9) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rtheme_penguin()\r Not bad! Looks promising. To actually fit a linear regression model, you might be used to something like this in R:\npenguin_mod \u0026lt;- lm(body_mass_g ~ flipper_length_mm, data = tidypenguins)\rsummary(penguin_mod)\r#\u0026gt; #\u0026gt; Call:\r#\u0026gt; lm(formula = body_mass_g ~ flipper_length_mm, data = tidypenguins)\r#\u0026gt; #\u0026gt; Residuals:\r#\u0026gt; Min 1Q Median 3Q Max #\u0026gt; -1057.33 -259.79 -12.24 242.97 1293.89 #\u0026gt; #\u0026gt; Coefficients:\r#\u0026gt; Estimate Std. Error t value Pr(\u0026gt;|t|) #\u0026gt; (Intercept) -5872.09 310.29 -18.93 \u0026lt;2e-16 ***\r#\u0026gt; flipper_length_mm 50.15 1.54 32.56 \u0026lt;2e-16 ***\r#\u0026gt; ---\r#\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r#\u0026gt; #\u0026gt; Residual standard error: 393.3 on 331 degrees of freedom\r#\u0026gt; Multiple R-squared: 0.7621,\tAdjusted R-squared: 0.7614 #\u0026gt; F-statistic: 1060 on 1 and 331 DF, p-value: \u0026lt; 2.2e-16\r But we aren\u0026rsquo;t going to stick with this. We are going to use tidymodels, with the goal of generating accurate predictions for future, yet-to-be-seen penguins.\ntidymodels 101 The code provided in the section below is not particularly sad 🐧. If you are embarking on learning tidymodels, you\u0026rsquo;ll need to use this same kind of code as the building blocks for any predictive modeling pipeline.\nParsnip: build the model This step is really three, using only the parsnip package:\nlm_spec \u0026lt;- linear_reg() %\u0026gt;% # pick model\rset_engine(\u0026quot;lm\u0026quot;) %\u0026gt;% # set engine\rset_mode(\u0026quot;regression\u0026quot;) # set mode\rlm_spec\r#\u0026gt; Linear Regression Model Specification (regression)\r#\u0026gt; #\u0026gt; Computational engine: lm\r Things that are missing: data (we haven\u0026rsquo;t touched it yet) and a formula (no data, no variables, no twiddle ~). This is an abstract model specification. See other possible parsnip models here.\nRecipe: not happening here, folks This is where you would normally insert some code for feature engineering using the recipes package. But previously this required functions named prep(), bake(), juice()- so I\u0026rsquo;m willfully ignoring that for now. There will be no recipes involving penguins.\nRsample: initial split We\u0026rsquo;ll use the rsample package to split (ayee! I promise no penguins were hurt in the writing of this blog post) the penguins up into two datasets: training and testing. If you are unfamiliar with this practice, read up on the holdout method.\npenguin_split \u0026lt;- initial_split(tidypenguins, strata = species)\rpenguin_train \u0026lt;- training(penguin_split)\rpenguin_test \u0026lt;- testing(penguin_split)\r Fitting the model once Fitting a single model once is\u0026hellip;not exactly the hardest part.\nThis is essentially the workflow from this early blog post.\nset.seed(0)\rlm_spec %\u0026gt;% # train: get fitted model\rfit(body_mass_g ~ ., data = penguin_train) %\u0026gt;% # test: get predictions\rpredict(new_data = penguin_test) %\u0026gt;% # compare: get metrics\rbind_cols(penguin_test) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred)\r#\u0026gt; # A tibble: 1 x 3\r#\u0026gt; .metric .estimator .estimate\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 291.\r Fitting the model with a function If you squint, you might see that I could make this into a function like below:\nget_rmse \u0026lt;- function(model_spec, split) {\rmodel_spec %\u0026gt;% # train: get fitted model\rfit(body_mass_g ~ ., data = training(split)) %\u0026gt;% # test: get predictions\rpredict(new_data = testing(split)) %\u0026gt;% # compare: get metrics\rbind_cols(testing(split)) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred)\r}\r And I could use it to fit a linear regression model:\nset.seed(0)\rget_rmse(model_spec = lm_spec, split = penguin_split)\r#\u0026gt; # A tibble: 1 x 3\r#\u0026gt; .metric .estimator .estimate\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 291.\r I could also build up a tibble that includes the results, if I wanted to save the predicted values, for example:\nget_preds \u0026lt;- function(model_spec, split){\r# train: get fitted model\rfit_model \u0026lt;- model_spec %\u0026gt;% fit(body_mass_g ~ ., data = training(split))\r# test: get predictions\rpreds \u0026lt;- fit_model %\u0026gt;% predict(new_data = testing(split)) %\u0026gt;% bind_cols(testing(split) %\u0026gt;% select(body_mass_g, species))\rpreds\r}\rset.seed(0)\rpenguin_preds \u0026lt;- get_preds(model_spec = lm_spec, split = penguin_split)\r Then I can work with the predicted values, like plotting the fitted body mass estimates against the residuals.\nggplot(penguin_preds, aes(x = .pred, y = (.pred - body_mass_g))) +\rgeom_point(aes(colour = species), size = 3, alpha = .8) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rtheme_penguin() +\rscico::scale_colour_scico_d(end = .8) +\rggtitle(\u0026quot;Residuals vs Fitted\u0026quot;)\r#\u0026gt; `geom_smooth()` using formula 'y ~ x'\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\r#\u0026gt; found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\r#\u0026gt; font family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r#\u0026gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\r#\u0026gt; family not found in Windows font database\r # compare: get metrics\rpenguin_preds %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred)\r#\u0026gt; # A tibble: 1 x 3\r#\u0026gt; .metric .estimator .estimate\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 291.\r Or I could fit a regression tree model with a new model spec:\n# regression tree model spec\rrt_spec \u0026lt;-\rdecision_tree() %\u0026gt;% set_engine(\u0026quot;rpart\u0026quot;) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;)\r# get rmse\rset.seed(0)\rget_preds(model_spec = rt_spec, split = penguin_split) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred)\r#\u0026gt; # A tibble: 1 x 3\r#\u0026gt; .metric .estimator .estimate\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 314.\r Or a random forest:\n# random forest model spec\rrf_spec \u0026lt;-\rrand_forest() %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;)\r# get rmse\rset.seed(0)\rget_preds(model_spec = rf_spec, split = penguin_split) %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred)\r#\u0026gt; # A tibble: 1 x 3\r#\u0026gt; .metric .estimator .estimate\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 297.\r But, unfortunately, I shouldn\u0026rsquo;t be predicting with the test set over and over again like this. It isn\u0026rsquo;t good practice to predict with the test set \u0026gt; 1 time. What is a good predictive modeler to do? I should be saving (holding out) the test set and use it to generate predictions exactly once, at the very end — after I\u0026rsquo;ve compared different models, selected my features, and tuned my hyperparameters. How do you do this? You do cross-validation with the training set, and you leave the testing set for the very last fit you do.\nHey Jude, don\u0026rsquo;t make it sad 🎶 Now, for the 😭 part- let\u0026rsquo;s add cross-validation! To do this, we\u0026rsquo;ll use a function called rsample::vfold_cv().\n# add the cv step here\rset.seed(0)\rpenguin_folds \u0026lt;- vfold_cv(data = penguin_train, strata = \u0026quot;species\u0026quot;)\rpenguin_folds\r#\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 2\r#\u0026gt; splits id #\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01\r#\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02\r#\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03\r#\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04\r#\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05\r#\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06\r#\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07\r#\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08\r#\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09\r#\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10\r The process of training, testing, and computing metrics gets a lot harder when you need to do this across 10 folds, each with a different data split. I eventually worked out three approaches, which I show below. All require some level of comfort with iteration using the purrr package.\nFunction with minimal purrr-ing This approach is essentially a mega-function, that we then use purrr to map across each fold.\nI\u0026rsquo;m going to change a few things from my previous get_preds() function:\n training(split) -\u0026gt; analysis(split) testing(split) -\u0026gt; assessment(split) I also added the rsample::add_resample_id() function to keep track of the fold number. I saved the predictions now as a list column.  To build up this function, my strategy was to figure out how to work with one fold, then I knew I\u0026rsquo;d be able to use purrr::map_df() to apply it across multiple folds.\n# Figure it out for one fold\rget_fold_results \u0026lt;- function(model_spec, split){\r# train: get fitted model for each fold\rfits \u0026lt;- model_spec %\u0026gt;% fit(body_mass_g ~ ., data = analysis(split))\r# test: get predictions on for each fold\rpreds \u0026lt;- fits %\u0026gt;% predict(new_data = assessment(split)) %\u0026gt;% bind_cols(assessment(split)) # compare: compute metric for each fold\rrmse \u0026lt;- assessment(split) %\u0026gt;% summarize(rmse = rmse_vec(truth = body_mass_g, estimate = preds$.pred))\rrmse %\u0026gt;% # add fold identifier column\rrsample::add_resample_id(split = split) %\u0026gt;% as_tibble() %\u0026gt;% # add predictions\rmutate(preds = list(preds))\r}\r I tried this function with a single fold first:\nset.seed(0)\rget_fold_results(\rsplit = penguin_folds$splits[[1]], model_spec = rt_spec\r)\r#\u0026gt; # A tibble: 1 x 3\r#\u0026gt; rmse id preds #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 290. Fold01 \u0026lt;tibble [26 x 8]\u0026gt;\r Next, I used purrr- but just once. The function get_fold_results is doing most of the work for us, but I needed purrr to map it across each fold.\nset.seed(0)\rkfold_results \u0026lt;- map_df(\rpenguin_folds$splits, ~get_fold_results(.x, model = rt_spec))\rkfold_results\r#\u0026gt; # A tibble: 10 x 3\r#\u0026gt; rmse id preds #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 290. Fold01 \u0026lt;tibble [26 x 8]\u0026gt;\r#\u0026gt; 2 395. Fold02 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 3 367. Fold03 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 4 310. Fold04 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 5 260. Fold05 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 6 274. Fold06 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 7 333. Fold07 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 8 259. Fold08 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 9 343. Fold09 \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 10 314. Fold10 \u0026lt;tibble [25 x 8]\u0026gt;\r Here we are still left with 10 RMSE values- one for each of the 10 folds. We don\u0026rsquo;t care too much about by fold- the power is in the aggregate. Specifically, we mainly care about the central tendency and spread of these RMSE values. Let\u0026rsquo;s finish by combining (or aggregating) these metrics.\nkfold_results %\u0026gt;% summarize(mean_rmse = mean(rmse), sd_rmse = sd(rmse))\r#\u0026gt; # A tibble: 1 x 2\r#\u0026gt; mean_rmse sd_rmse\r#\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 314. 45.7\r So, this works. But, can you imagine doing it again? Without errors? Can you imagine teaching it?\nPurrr-to-the-max This approach is purrr::map() (and friends) on steriods. We use vanilla map(), map2(), and map2_dbl() here. We also use anonymous functions as a formula, and the pipe operator within those anonymous functions.\nset.seed(0)\rpenguin_res \u0026lt;- penguin_folds %\u0026gt;% mutate(\r# train: get fitted model for each fold\rtrain_set = map(splits, analysis),\rfit_models = map(train_set, ~rt_spec %\u0026gt;% fit(body_mass_g ~ ., data = .x)),\r# test: get predictions for each fold\rtest_set = map(splits, assessment),\restimates = map2(fit_models, test_set, ~.x %\u0026gt;% predict(.y)),\r# compare: compute metric for each fold\rrmse = map2_dbl(test_set, estimates, ~rmse_vec(truth = .x$body_mass_g, estimate = .y$.pred))\r)\rpenguin_res\r#\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 7\r#\u0026gt; splits id train_set fit_models test_set estimates rmse\r#\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 \u0026lt;split [225~ Fold01 \u0026lt;tibble [225~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26~ \u0026lt;tibble [26 ~ 290.\r#\u0026gt; 2 \u0026lt;split [226~ Fold02 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 395.\r#\u0026gt; 3 \u0026lt;split [226~ Fold03 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 367.\r#\u0026gt; 4 \u0026lt;split [226~ Fold04 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 310.\r#\u0026gt; 5 \u0026lt;split [226~ Fold05 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 260.\r#\u0026gt; 6 \u0026lt;split [226~ Fold06 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 274.\r#\u0026gt; 7 \u0026lt;split [226~ Fold07 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 333.\r#\u0026gt; 8 \u0026lt;split [226~ Fold08 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 259.\r#\u0026gt; 9 \u0026lt;split [226~ Fold09 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 343.\r#\u0026gt; 10 \u0026lt;split [226~ Fold10 \u0026lt;tibble [226~ \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25~ \u0026lt;tibble [25 ~ 314.\rpenguin_res %\u0026gt;% summarise(mean_rmse = mean(rmse), sd_rmse = sd(rmse))\r#\u0026gt; # A tibble: 1 x 2\r#\u0026gt; mean_rmse sd_rmse\r#\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 314. 45.7\r The purrr mash-up Another way I worked out was largely after reviewing Max\u0026rsquo;s slides from previous workshops. This is basically a mash-up of my previous two approaches, where we write laser-focused functions that each do one thing, then use purrr to apply those functions across the folds. This way is nice(r) for showing in slides as you can incrementally build up the results table. Let\u0026rsquo;s see this sad script in action\u0026hellip;\nRound 1 set.seed(0) # for reproducibility\r# train: get fitted model for a split\rget_fits \u0026lt;- function(split, model_spec){\rmodel_spec %\u0026gt;% fit(body_mass_g ~ ., data = analysis(split))\r}\r# train: get fitted models across folds\rpenguin_purrr \u0026lt;- penguin_folds %\u0026gt;% mutate(rt_fits = map(splits, get_fits, rt_spec))\rpenguin_purrr\r#\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 3\r#\u0026gt; splits id rt_fits #\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt;\r#\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt;\r Round 2 # test: get predictions for a split\rget_preds \u0026lt;- function(split, fit_df) {\rfit_df %\u0026gt;% predict(new_data = assessment(split)) %\u0026gt;% bind_cols(assessment(split))\r}\r# test: get predictions across folds\rpenguin_purrr \u0026lt;- penguin_purrr %\u0026gt;% mutate(rt_preds = map2(splits, rt_fits, get_preds))\rpenguin_purrr\r#\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 4\r#\u0026gt; splits id rt_fits rt_preds #\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 x 8]\u0026gt;\r#\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r#\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt;\r aaaand Round 3 # compare: compute metric for a split\rget_rmse \u0026lt;- function(pred_df) {\rpred_df %\u0026gt;% rmse(truth = body_mass_g, estimate = .pred) %\u0026gt;% pluck(\u0026quot;.estimate\u0026quot;)\r}\r# compare: compute metric across folds\rpenguin_purrr \u0026lt;- penguin_purrr %\u0026gt;% mutate(rt_rmse = map_dbl(rt_preds, get_rmse))\rpenguin_purrr\r#\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 5\r#\u0026gt; splits id rt_fits rt_preds rt_rmse\r#\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 x 8]\u0026gt; 290.\r#\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 395.\r#\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 367.\r#\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 310.\r#\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 260.\r#\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 274.\r#\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 333.\r#\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 259.\r#\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 343.\r#\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 314.\r Finally, summarizing as I did before:\npenguin_purrr %\u0026gt;% summarize(mean_rmse = mean(rt_rmse), sd_rmse = sd(rt_rmse))\r#\u0026gt; # A tibble: 1 x 2\r#\u0026gt; mean_rmse sd_rmse\r#\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 314. 45.7\r In practice, if you did all these at once instead of incrementally, it would look like:\nset.seed(0)\rpenguin_folds %\u0026gt;% # train: get fitted model for a split\rmutate(rt_fits = map(splits, get_fits, rt_spec)) %\u0026gt;% # test: get predictions on for each fold\rmutate(rt_preds = map2(splits, rt_fits, get_preds)) %\u0026gt;% # compare: compute metric for each fold\rmutate(rt_rmse = map_dbl(rt_preds, get_rmse))\r#\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 5\r#\u0026gt; splits id rt_fits rt_preds rt_rmse\r#\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [26 x 8]\u0026gt; 290.\r#\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 395.\r#\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 367.\r#\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 310.\r#\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 260.\r#\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 274.\r#\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 333.\r#\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 259.\r#\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 343.\r#\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;fit[+]\u0026gt; \u0026lt;tibble [25 x 8]\u0026gt; 314.\r When you put it like that, it doesn\u0026rsquo;t look like so much work! But, this way hides how much work it takes to write those 3 custom functions: get_fits(), get_preds(), and get_rmse(). And we still had to use vanilla map(), map2(), and map2_dbl().\nMake it better I kept a learning log while working through the all the above code, and I wrote down these notes to myself:\n  It is very easy to do the wrong thing; it is very hard to do the right thing.\n  I lost sight many times of what the code I was writing was doing, because I was using up so much cognitive energy on getting the code to just work.\n  I thought I knew how to use purrr\u0026hellip;\n  If you have made it this far, I\u0026rsquo;m pretty sure I don\u0026rsquo;t need to convince you that a better way to do cross-validation using tidymodels would be more pleasant to do more than once. It would also be less prone to error due to me copying-and-pasting repeatedly, and making stupid mistakes that would be difficult to spot with so much cluttered code. Luckily, tune::fit_resamples() came along to take a sad script and make it better:\npenguin_party \u0026lt;-\rtune::fit_resamples(\rrt_spec,\rbody_mass_g ~ .,\rresamples = penguin_folds\r)\r Here is the beautiful output from that function:\npenguin_party\r#\u0026gt; # Resampling results\r#\u0026gt; # 10-fold cross-validation using stratification #\u0026gt; # A tibble: 10 x 4\r#\u0026gt; splits id .metrics .notes #\u0026gt; \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; #\u0026gt; 1 \u0026lt;split [225/26]\u0026gt; Fold01 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 2 \u0026lt;split [226/25]\u0026gt; Fold02 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 3 \u0026lt;split [226/25]\u0026gt; Fold03 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 4 \u0026lt;split [226/25]\u0026gt; Fold04 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 5 \u0026lt;split [226/25]\u0026gt; Fold05 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 6 \u0026lt;split [226/25]\u0026gt; Fold06 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 7 \u0026lt;split [226/25]\u0026gt; Fold07 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 8 \u0026lt;split [226/25]\u0026gt; Fold08 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 9 \u0026lt;split [226/25]\u0026gt; Fold09 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r#\u0026gt; 10 \u0026lt;split [226/25]\u0026gt; Fold10 \u0026lt;tibble [2 x 3]\u0026gt; \u0026lt;tibble [0 x 1]\u0026gt;\r Now, to see all the stuff inside this penguin_party, we can use tune\u0026rsquo;s collect_* functions.\npenguin_party %\u0026gt;% collect_metrics()\r#\u0026gt; # A tibble: 2 x 5\r#\u0026gt; .metric .estimator mean n std_err\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 rmse standard 314. 10 14.5 #\u0026gt; 2 rsq standard 0.849 10 0.0172\r To see the predictions, we need to add use control_resamples():\npenguin_party \u0026lt;-\rtune::fit_resamples(\rrt_spec,\rbody_mass_g ~ .,\rresamples = penguin_folds,\rcontrol = control_resamples(save_pred = TRUE) # add this line\r)\r Then we collect the predictions.\npenguin_party %\u0026gt;% collect_predictions()\r#\u0026gt; # A tibble: 251 x 4\r#\u0026gt; id .pred .row body_mass_g\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r#\u0026gt; 1 Fold01 3446. 4 3625\r#\u0026gt; 2 Fold01 3446. 11 3400\r#\u0026gt; 3 Fold01 4015. 14 3800\r#\u0026gt; 4 Fold01 3446. 41 2850\r#\u0026gt; 5 Fold01 3446. 54 3550\r#\u0026gt; 6 Fold01 4015. 65 4450\r#\u0026gt; 7 Fold01 3446. 70 3725\r#\u0026gt; 8 Fold01 4015. 71 4725\r#\u0026gt; 9 Fold01 4015. 76 3900\r#\u0026gt; 10 Fold01 3446. 85 3350\r#\u0026gt; # ... with 241 more rows\r Now, isn\u0026rsquo;t that better?\n","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582761600,"objectID":"3f6d21755f6210ccc450575422f58ff0","permalink":"/post/2020-02-27-better-tidymodels/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/post/2020-02-27-better-tidymodels/","section":"post","summary":"Taking a sad script and making it better for model cross-validation.","tags":["tidymodels"],"title":"Take a Sad Script \u0026 Make it Better: Tidymodels Edition","type":"post"},{"authors":["","Aadil Hayat"],"categories":null,"content":"   This talk was part of Programming Club, IIT Kanpur\u0026rsquo;s talk on machine learning frameworks. This talk was targeted for sophomores and junior undergraduates with some background on deep learning. It explained dataflow graphs (which represent TensorFlow computations) and its basic terminologies and functions. Next, it covered basic implementation in TensorFlow through a toy problem on MNIST dataset, including the neural network and the dataflow graph for the problem. It also covered the installation procedure of the GPU and the CPU-only version.\n","date":1578096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578096000,"objectID":"ab75b4604f9d86033f76addc3de83d38","permalink":"/talk/2019-tensorflow/","publishdate":"2020-01-04T00:00:00Z","relpermalink":"/talk/2019-tensorflow/","section":"talk","summary":"This talk introduces tensorflow for deep learning  and explains its underlying computation methodology.","tags":["tensorflow","deep learning","dataflow graphs"],"title":"Introduction to tensorflow","type":"talk"},{"authors":[],"categories":[],"content":"\rI’m excited to be teaching a new workshop at the upcoming rstudio::conf in January called “Introduction to Machine Learning with the Tidyverse”, with my colleague Garrett Grolemund. Our workshop just sold out over the weekend! 🎉\nIt is always hard to develop an entirely new workshop, especially if you are doing it at the same time as learning how to use a new API. It is even harder when that API is under active development like the tidymodels ecosystem! I’ve been so lucky to be able to work with the tidymodels team at RStudio, Max Kuhn and Davis Vaughan, to help shape how we tell the tidymodels story to ML beginners. But my favorite part of developing a new workshop like this has been studying how others teach machine learning. Spoiler alert: there are a lot of materials intended for learners that make things seem harder than they actually are! Below, I’m sharing my bookmarked resources, organized roughly in the order I think they are most helpful for beginners.\nMachine Learning for Everyone. In simple words. With real-world examples. Yes, again. In my experience, the biggest hurdle to getting started is sifting through both the hype and the math. This is a readable illustrated introduction to key concepts that will help you start building your own mental model of this space. For example, “the only goal of machine learning is to predict results based on incoming data. That’s it.” There you go! Start here.\n\n\rA Visual Introduction to Machine Learning by r2d3. This is a wonderful two-part series (that I wish would be extended!):\n\rPart I: A Decision Tree\rPart II: Model Tuning and the Bias-Variance Tradeoff \r\rSupervised Machine Learning course by Julia Silge Taught with R and the caret package (the precursor to the in-development tidymodels ecosystem), this is a great next step in your machine learning journey as you’ll start doing ML right away in your browser using an innovative course delivery platform. You’ll also get to play with data that is not iris, titanic, or AmesHousing. This will be sweet relief because you’ll find the rest of my recommended resources all basically build models to predict home prices in Ames, Iowa.\n\n\rHands-on Machine Learning with R by Bradley Boehmke \u0026amp; Brandon Greenwell. Another great way to learn concepts plus code, although another one that focuses on the caret package (pre-tidymodels). Each chapter maps onto a new learning algorithm, and provides a code-through with real data from building to tuning. The authors also offer practical advice for each algorithm, and the “final thoughts” sections at the end of each chapter will help you tie it all together.\n\nDon’t skip the “Fundamentals” section, even if you feel like you’ve got that down by now. The second chapter on the modeling process is especially good.\n\n\rInterpretable Machine Learning: A Guide for Making Black Box Models Explainable by Christoph Molnar. If you only have time to read a single chapter, skip ahead to Chapter 4: Interpretable Models. I also appreciated the introduction section on terminology. But the whole book is excellent and well-written.\n\n\rModel evaluation, model selection, and algorithm selection in machine learning- a 4-part series by Sebastian Raschka. I found this to be a great evidence-based, thorough overview of the methods for machine learning. I especially liked how he walks you step-by-step from the simplest methods like the holdout method up to nested cross-validation:\n\rPart I: The Basics\rPart II: Bootstrapping \u0026amp; uncertainties\rPart III: Cross-validation and hyperparameter tuning\rPart IV: Comparing the performance of machine learning models and algorithms using statistical tests and nested cross-validation\r\r\rAt this point, if you can read through the above resources and you are no longer feeling awash in new terminology, I think your vocabulary and mental model are in pretty good shape! That means you are ready for the next step, which is to read Max Kuhn and Kjell Johnson’s new book Feature Engineering and Selection: A Practical Approach for Predictive Models\n\nIn my experience, the later chapters in this book filled in a lot of lingering questions I had about certain methods, like whether to use factor or dummy variables in tree-based models. But also don’t miss the section on “important concepts” at the beginning- this should feel like a nice review if you’ve gotten this far!\n\rElements of Statistical Learning. The entire PDF of the book is available online. A great resource for those with a strong statistics background, and for those looking for more math and formulas.\n\r\rOther note-worthy resources\r\rFor the highly visual learner, you may want to cue up some YouTube videos from Udacity’s “Machine Learning for Trading” course. I found these illustrations especially helpful:\n\rCross-validation\rOverfitting\rEnsemble learners\rBootstrap aggregating (bagging)\rBoosting\r\r\rChris Albon’s Machine Learning Flashcards ($12)\n\rShirin Elsinghorst’s blog (free! and so good).\n\nI love her sketchnotes.\n\rI also found Rafael Irizarry’s “Introduction to Machine Learning”, a chapter from his Introduction to Data Science book, to have some helpful discussion.\n\rMachine Learning: A primer by Danilo Bzdok, Martin Krzywinski \u0026amp; Naomi Altman, from the Nature Methods Points of Significance collection- this collection in general is always straight-forward with great visuals. Start with the primer, then skim these:\n\rStatistics versus machine learning\rMachine learning: supervised methods\rClassification and regression trees (decision trees are the “base learner” for many ensemble methods - this is a good intro)\rEnsemble methods: bagging and random forests\r\r\r\rThat’s all for now- if you are taking my workshop in January I look forward to meeting you in person! If not, rest assured that all code and materials will be shared openly after the workshop. Until then, happy learning 🤖\n\r","date":1577059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577119901,"objectID":"416351d525a4abd88ecf05ec88076d80","permalink":"/post/2019-12-23-learning-to-teach-machines-to-learn/","publishdate":"2019-12-23T00:00:00Z","relpermalink":"/post/2019-12-23-learning-to-teach-machines-to-learn/","section":"post","summary":"I’m excited to be teaching a new workshop at the upcoming rstudio::conf in January called “Introduction to Machine Learning with the Tidyverse”, with my colleague Garrett Grolemund. Our workshop just sold out over the weekend!","tags":[],"title":"Learning to Teach Machines to Learn","type":"post"},{"authors":["","Raunak Shah"],"categories":null,"content":"   This talk was part of Programming Club, IIT Kanpur\u0026rsquo;s talk on introduction to deep learning. This talk was targeted for freshers and sophomores (both UG and PG) who are starting with machine learning. It covered topics like the most popular convex cost functions, activation functions and gradient descent, along with hands-on coding experience of these topics. It served as a medium of introducing deep learning to the campus community and as a starting step to explore more complex topics.\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"97855ba07b4bb6c611bebddb47a00173","permalink":"/talk/2019-basic_dl/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/talk/2019-basic_dl/","section":"talk","summary":"Part of introduction to deep learning lecture series, covering topics like convex cost functions and gradient descent.","tags":["deep learning"],"title":"Introduction to Deep Learning","type":"talk"},{"authors":["Kushagra Gupta","Dootika Vats"],"categories":["stats"],"content":"Sufficient research has been done on estimating the asymptotic covariance matrix in a Markov chain central limit theorem for applications in Markov chain Monte Carlo (MCMC). However, almost all of it, including the efficient batch means (BM) estimator, focuses on a single-chain MCMC run. We demonstrate that simply averaging covariance matrix estimators from different chains (average BM) can yield critical underestimates of the variance in small sample sizes, especially for slow mixing Markov chains. We propose a multivariate replicated batch means (RBM) estimator that utilizes information across all chains in order to estimate the asymptotic covariance matrix, thereby correcting for the underestimation. Under weak conditions on the mixing rate of the process, strong consistency of the RBM estimator follows from the strong consistency of the BM estimators. Further, we show that the large-sample bias and variance of the RBM estimator mimics that of the average BM estimator. Thus, for large MCMC runs, the RBM and average BM yield equivalent performance, but for small MCMC runs, the RBM estimator can be dramatically superior. This superiority is demonstrated through a variety of examples, including a two-variable Gibbs sampler for a bivariate Gaussian target distribution. Here we obtain a closed-form expression for the asymptotic covariance matrix of the Monte Carlo estimator, a result vital in itself, as it allows for benchmarking implementations in the future.\n","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"bf6ccd8edd51a1f62595b1b7212a01a4","permalink":"/project/rbm/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/project/rbm/","section":"project","summary":"Replicated batch means estimator for covariance estimation from multiple Markov chains.","tags":["stats"],"title":"Replicated Batch Means for Parallel MCMC","type":"project"},{"authors":["Kushagra Gupta","Aditya Gulati","Vikrant Malik","Aviral Aggarwal"],"categories":["misc"],"content":"  -- Fully Homomorphic Encryption (FHE) allows computation directly on the encrypted data, generating an encrypted result that matches the result of the computation on decrypted data. FHE removes the insecurity introduced by the requirement of decrypting data to perform computations on it. We build a levelled FHE library (which can evaluate circuits with a bounded depth) based on the GSW scheme using libtorch and CUDA. Our library is capable of performing parallel computations and automatic differentiation, making it significantly fast and easy to use, particularly for machine learning models.\n","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"5ffe1686c8e696f3f8c4595597b72ab7","permalink":"/project/fhe/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/project/fhe/","section":"project","summary":"Implemented a C++ FHE library based on GSW encryption system using libtorch with CUDA to enable parallel computation with Automatic Differentiation.","tags":["misc"],"title":"Fully Homomorphic Encryptio Library","type":"project"},{"authors":["Kushagra Gupta","Arshad Rahman"],"categories":["stats"],"content":"In Fall 2019, I was working with Prof.Vats and Prof. Arshad Rahman (in Economics department) on developing Bayesian models for interval data. I was introduced to interval regression in my summer project on Bayesian estimation of Weibull distribution while reading Prof.Kundu\u0026rsquo;s paper on interval-censored data with Weibull lifetime. I researched on various paradigms that solve interval regression, covering quantile regression (QR), meta-heuristic algorithms, information theory, convex analysis, and set arithmetic linear models. I found QR to be a promising approach as traditional QR settings can extend to interval data in the following way - we can modify the quantile loss function to have three parts based on the partition by the interval endpoints, as opposed to a binary partition by traditional QR, with certain added assumptions. The change is that the dot product of parameters and covariates lying in the interval has a zero loss as the dependent variable can lie anywhere in the interval. The assumptions can be modelled in a Bayesian setting with appropriate priors. As part of my project, I built Bayesian frameworks for various paradigms of interval regression.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"b76bcc6ebceb6f5bd69068769f9917f2","permalink":"/project/qr/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/project/qr/","section":"project","summary":"Developed Bayesian models in multiple paradigms for interval regression.","tags":["stats"],"title":"Interval Regression using Bayesian Inference","type":"project"},{"authors":["Kushagra Gupta","Medha Agarwal","Debasis Kundu"],"categories":["stats"],"content":"In the summer of 2019, I (in a team of 2) was working with Prof.Kundu on developing Bayesian methodologies for parameter estimation of a 3-Weibull distribution \\cite{weibull}. Maximum likelihood estimators are inconsistent and inefficient for this problem when the shape parameter is less than one due to the likelihood function exploding to infinity. However, the Bayesian setting with gamma priors solves this problem as the posterior breaks into a product of proper densities, which is convenient for MCMC sampling. We found the posterior to be log-concave and similar to a gamma distribution and, thus, approximated the parameters and confidence intervals using this observation. Our Bayesian implementation reduced the mean square errors significantly as compared to the maximum likelihood method.\n","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"5a4b4031cc7e0e43e7ad77f75cb1a97c","permalink":"/project/weibull/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/project/weibull/","section":"project","summary":"Developed algorithm for parameter estimation of 3-Weibull distribution using bayesian inference.","tags":["stats"],"title":"Bayesian Inference on 3 parameter Weibull Distribution","type":"project"},{"authors":["Kushagra Gupta","Utsav Singh"],"categories":["ml"],"content":" I worked on multi-agent self-play in atari games in collaborative and competitive settings. I used variational autoencoders to disentangle multiple near-optimal policies extracted using latent code. Our initial results on the model gave win probability of 72%, which is close to 80% SOTA values, and much better than the human score of 40% in multi-agent CTF. I worked on developing a generative model for InfoRL to maintain unsupervised setting for latent code generation to allow all standard MARL algorithms to be used with InfoRL.  ","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"6bd13f3caa1ba927eb609b13da077e28","permalink":"/project/mrl/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/project/mrl/","section":"project","summary":"Multi-agent self-play in atari games in collaborative and competetive settings.","tags":["ml"],"title":"Multi-Agent Reinforcement Learning","type":"project"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"\r\r#1: Update Hugo\r#2: Change the baseurl\r#3: Netlify drag-and-drop\r#4: Torch public/\r#5: Peruse public/\r#6: Back to the future\r\r\r\r“Just a spoonful of Hugo helps the blog go down.”\r- me, only somewhat kidding\n\rIn this series, I’m sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated). You can read the previous posts about my “Spoonful of Hugo” series about Hugo archetypes, Hugo versions, and Hugo page bundles.\nThe following are a few steps that I always start with to troubleshoot any blogdown/Hugo/Netlify problems. These steps would solve what I would anecdotally estimate as ~50% of blogdown problems that I see posted in the GitHub repository and on the community site.\n#1: Update Hugo\r\rFigure 1: Don’t be like this\r\rIf things have gone south and you are getting Hugo errors when you use the “Serve Site” Addin locally, it is possible that you need to update your version of Hugo. From R, you can check your Hugo version with blogdown:\nblogdown::hugo_version()\rThen you can reference your Hugo theme to find the minimum version of Hugo required by your theme:\n\rFigure 2: Check your theme’s minimum Hugo version\r\rYou can go higher than the minimum version though, so it’s good practice to update your Hugo, again from within R:\nblogdown:: update_hugo()\rCheck your version again post-update:\nblogdown::hugo_version()\r## [1] \u0026#39;0.73.0\u0026#39;\rIf you are using Netlify to build your site using Hugo, you’ll want this version to match that- the best way to do that is with a netlify.toml file.\n\r#2: Change the baseurl\rOpen up your config.toml file and look for the baseurl field, usually pretty close to the top. Here is mine1:\nbaseurl = \u0026quot;https://alison.rbind.io\u0026quot;\rNow if you are just starting with Hugo and don’t actually have a domain name yet, try taking the advice that blogdown automatically prints out for you:\nWarning: You should change the \u0026quot;baseurl\u0026quot; option in config.toml from https://example.org to your actual domain; if you do not have a domain, set \u0026quot;baseurl\u0026quot; to \u0026quot;/\u0026quot;\rBut be careful here- you shouldn’t leave it as “/”- once you do have your domain name you should update the baseurl as “/” is a not a valid URL.\nCare to know more?\rHere is a quote from the person who writes the Hugo docs:\n\r“…the only purpose for the baseurl field in the config is to define the full base URL of your website for deployment purposes.” - @rdwatters\n\rThe main error that would happen without the trailing slash in the past is that you would end up with a site where the theme’s CSS would be all wrong. This was probably because the theme designer used code like this buried in a layout file:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ .Site.BaseURL }}css/style.css\u0026quot;/\u0026gt;\rNow, if you set baseurl = \"http://mysite.com\" but only rendered locally, things would look just peachy, because the default local server already included the trailing slash. So, the link in the html file would be2:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;http://localhost:1313/css/style.css\u0026quot;\u0026gt;\rBut, at build, the link in the html file would turn into:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;http://mysite.comcss/style.css\u0026quot;\u0026gt;\rWhich creates sites that look like this:\n\rFigure 3: Hugo tranquil peak theme\r\rGitHub issue #369\n\rFigure 4: Hugo universal theme\r\rGitHub issue #131\nGitHub issue #114\nHowever, Hugo authors and theme developers have largely been moving towards using relative URLs instead of the baseurl to build paths. This was based on public advice voiced by the Hugo authors on the discourse forum. For example:\n\r“The recommended way to reference resources is to use either relURL or absURL template funcs, which handles the slash issues.”- @bep\n\rFollowing that advice, a more up-to-date theme would have code that looks like this buried in a layout file:\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ \u0026quot;css/style.css\u0026quot; | relURL }}\u0026quot;/\u0026gt; → \rBottom line? If your theme uses relURL or absURL to link to site resources like CSS, JavaScript, or static images, then whether or not you include a trailing slash in your baseurl should not matter at all.\nAnd here is some tough love about your theme: if the most recent version does still require the trailing slash in the baseurl to “work” out of the box, I would seriously consider switching themes. This is a pretty good “canary in the coal mine” test regarding how up-to-date the theme author is, and how well the theme you have chosen adheres to Hugo templating best practices. If you are having pain with this now, it is likely not the only thing that will be painful about working with your theme.\n\r\r#3: Netlify drag-and-drop\rIf you can render your site locally but your published site looks different, try the drag-and-drop method:\nUse the “Serve Site” Addin, then drag-and-drop the public/ folder straight into Netlify. What does this do? You can now see your public site…that you built…with your local version of Hugo. Netlify is doing none of the site building here.\nOne of the first benefits of this approach is that it ensures that you are able to actually generate a public/ folder locally! I have seen folks struggle to deploy the wrong repo. This simple step can force you to make sure to use the “Serve Site” Addin to generate the public/ folder, and that the repo you are trying to link to Netlify actually contains a Hugo site because you must physically move the public/ folder. But this method can also help you diagnose other problems too.\nIf your public/ folder does not render on Netlify, you have work to do locally. I can’t tell you what it is as it can be a number of things, but you can be sure that your problem is not just the Netlify build- it is your local build too.\nIf your public/ folder does render perfectly on Netlify, but you are getting a Netlify build error, then you likely have a Hugo version problem. It might be that the version you are running locally is more recent than the version run by Netlify by default to actually build your site. The good news is there is a quick fix for this! The solution is to upgrade the Hugo version Netlify is using- see my advice here for how to do that.\nIf you are happy with how your site looks but you are missing content and/or seeing old deleted content, then you may need the next few strategies to troubleshoot.\n\r#4: Torch public/\rWhen you are seeing very weird things locally, try deleting your local public/ folder. Then serve site again. Sometimes it can get “junked up”. I’ve found that sometimes deleted content can be a little sticky. As recommended in the blogdown book:\n\r“you are strongly recommended to delete the /public/ directory before you rebuild the site for publishing every time, because Hugo never deletes it”\n\rAlso, this has a bonus of reinforcing for you exactly what the “Serve Site” Addin does - it regenerates the public/ folder. This is also the folder that, if you are using Netlify to build your site, is in your .gitignore file because Netlify (+ Hugo) generates this file “fresh” with each push to your GitHub repository.\n\r#5: Peruse public/\rWhen you notice weird things, try actually looking inside public/- don’t be afraid to spelunk around in there! If you are seeing something wrong with your site, try to figure out how blogdown/Hugo is processing and rendering your content. This folder can tell you a lot! Keep in mind that your local public/ folder will still contain future/draft/expired content if you used the “Serve Site” Addin.\n\r#6: Back to the future\r\rFigure 5: Where are my posts?\r\rIf your site renders beautifully locally, and your drag-and-drop site from public/ looks the same, but you are missing key content when you actually deploy to Netlify using a Hugo build, you may have inadvertently stumbled into a Hugo date time warp. This is a fairly common gotcha. Try using the drag-and-drop method again, this time first delete public/, then instead of using the “Serve Site” Addin, run this in your console:\nblogdown::build_site(local = FALSE)\rPlop this new public folder in Netlify to see what your site will look like when it is actually published. What does this show you? Your local Hugo build (read: your public/ folder generated by “Serve Site”) differs by design in 3 important ways from your deployed site built by Netlify/Hugo. By default, Hugo will not publish:\n\rContent with a future publishDate value\n\rContent with draft: true status\n\rContent with a past expiryDate value\n\r\rYou can see that these are defaults. The behavior of the “Serve Site” Addin is also documented in the blogdown book:\n\r“This is for you to preview draft and future posts locally.”\n\rBlogdown’s build_site(local = FALSE) differs from the “Serve Site” Addin in that it will not render draft, future, or expired content. So your public/ folder from build_site(local = FALSE) shows you exactly what Netlify should publish. Seeing it can help you troubleshoot why some content was showing up locally but not when you publish.\nThe defaults are pretty sensible and nice to have, as you can still put these kinds of content under version control, and hence collaborate with other team members on the content without having the content publish (or expire) until you say so.\nTo show content that Hugo was hiding, you’ll want to edit some YAML fields in the individual offending content files. For example, in the YAML of an individual content file (like a blog post), if you want to un-draft it, add or change this key/value:\ntitle: \u0026#39;A Spoonful of Hugo: Troubleshooting your Build\u0026#39;\rauthor: \u0026quot;Alison Hill\u0026quot;\rdate: \u0026#39;2019-03-04\u0026#39;\rdraft: false\rAlternatively, if you want to date something in the future (like to advertise the date of an upcoming talk) but publish now, you can use the publishDate field. The publishDate field is a newer addition to Hugo (\u0026gt;= v0.54.0) which, if left unset, will default to the date field, which means in the individual content file YAML you can do:\ntitle: \u0026#39;A Spoonful of Hugo: Get excited!!\u0026#39;\rauthor: \u0026quot;Alison Hill\u0026quot;\rdate: \u0026#39;2025-03-04\u0026#39;\rpublishDate: \u0026#39;2019-03-04\u0026#39;\rHopefully these 6 things can help you get unstuck. If not, the RStudio community forums are a great place to ask questions!\n\r\rYes that’s right, I don’t have a trailing slash- read on for why I can get away with this.↩︎\n\rhttps://discourse.gohugo.io/t/how-not-to-specify-url-site/5691/5↩︎\n\r\r\r","date":1551657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551657600,"objectID":"0ba13389195341f445a9a6be1766a7ec","permalink":"/post/2019-03-04-hugo-troubleshooting/","publishdate":"2019-03-04T00:00:00Z","relpermalink":"/post/2019-03-04-hugo-troubleshooting/","section":"post","summary":"A few troubleshooting strategies to save your sanity","tags":["blogdown"],"title":"A Spoonful of Hugo: Troubleshooting Your Build","type":"post"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"\r\r“Just a spoonful of Hugo helps the blog go down.”\r- me, only somewhat kidding\n\rIn this series, I’m sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated). You can read the previous posts about my “Spoonful of Hugo” series about Hugo archetypes and Hugo versions.\nThis is my third post in this series and it is breaking news.\nHugo Page Bundles\rWell, not really breaking news, but you still may not know about it! Hugo v0.32 introduced a new feature called Page Bundles, as a way to organize the content files. Blogdown users rejoice that Davis Vaughn posted an issue on the rstudio/blogdown repo to enable this option, which Yihui added shortly before rstudio::conf 2019 🎉. Here is the snippet from the NEWS.md:\n\r“One benefit of using a page bundle instead of a normal page is that you can put resource files associated with the post (such as images) under the same directory of the post itself. This means you no longer have to put them under the static/ directory, which has been quite confusing to Hugo beginners.”\n\rWhat does a blogdown/Hugo site begin to look like without page bundles? I think here is a representative example from tidyverse.org (sorry tidyverse team- it’s not you, it’s the old Hugo).\nFor this team, they need an image for every post, which gets out of control pretty fast. Also, some ended up in static/ too, organized by post (which I have done on my own blog, though not well or consistently).\nWhat would it look like to use page bundles?\ncontent/\r├── about\r│ ├── index.md\r├── posts\r│ ├── 2015-07-23-hi-world\r│ │ ├── bakers.csv\r│ │ ├── image1.jpg\r│ │ ├── image2.png\r│ │ └── index.Rmd\r│ └── 2015-07-24-bye-world\r│ └── index.Rmd\rOne could call this bundled file structure “tidier” 🍱.\nIn the above, after serving site, index.html files also get added to the bundle. In Hugo’s terms, these are leaf bundles. The resource files allowed in a bundle include page and non-page items like images, pdf, .csv files, etc.\nThis is instead of:\ncontent/\r├── about\r│ ├── index.md\r├── posts\r│ ├── 2015-07-23-hi-world.Rmd\r│ ├── bakers.csv\r│ ├── image1.jpg\r│ ├── image2.png\r│ └── 2015-07-24-bye-world.Rmd\rWhen you create a new bundled post, the actual content of the post goes in the index file of a page bundle. So:\n# not bundled post\rpost/2015-07-23-hi-world.Rmd\r# bundled post\rpost/2015-07-24-bye-world/index.Rmd\r\rBundle Me, blogdown!\rFirst, read the previous post on setting up a netlify.toml file. Since using Hugo page bundles depends on Hugo v0.32 or higher, you should go ahead and update hugo then update your netlify.toml with your updated version:\nblogdown::update_hugo()\rblogdown::hugo_version()\rNow, let’s use the usethis package.\nProject-specific .Rprofile\rFirst, I’m going to demo here how to create a project-specific .Rprofile file- but know that you can do a user-level .Rprofile file too.\n# install.packages(\u0026quot;usethis\u0026quot;) # uncomment this to install\rusethis::edit_r_profile(scope = \u0026quot;project\u0026quot;)\rThese helpful messages should print to your console: please note the “restart” reminder…\n\u0026gt; usethis::edit_r_profile(scope = \u0026quot;project\u0026quot;)\r● Restart R for changes to take effect\r✔ Setting active project to \u0026#39;/Users/alison/rprojs/alison.rbind.io\u0026#39;\r● Modify \u0026#39;.Rprofile\u0026#39;\rNow you could add this to your file:\n# in .Rprofile of the website project\rif (file.exists(\u0026quot;~/.Rprofile\u0026quot;)) {\rbase::sys.source(\u0026quot;~/.Rprofile\u0026quot;, envir = environment())\r}\roptions(blogdown.new_bundle = TRUE)\rThe first code chunk above is from the blogdown book, where we describe a workaround for loading both user and project .Rprofile files (since R technically only reads one startup profile file).\nIf you don’t want this, you could add the blogdown options to your user .Rprofile instead using:\nusethis::edit_r_profile(scope = \u0026quot;user\u0026quot;)\rHeck, while you are at it, you could set a bunch of options to make your blogdown life easier:\n# in .Rprofile of the website project\rif (file.exists(\u0026quot;~/.Rprofile\u0026quot;)) {\rbase::sys.source(\u0026quot;~/.Rprofile\u0026quot;, envir = environment())\r}\roptions(\rblogdown.author = \u0026quot;Alison Hill\u0026quot;,\rblogdown.ext = \u0026quot;.Rmd\u0026quot;,\rblogdown.subdir = \u0026quot;post\u0026quot;,\rblogdown.yaml.empty = TRUE,\rblogdown.new_bundle = TRUE,\rblogdown.title_case = TRUE\r)\rFor the blogdown-specific options, any of these prepopulate content in your “New Post” Addin (I told you to use this here). There is a handy table from the blogdown book, summarized here:\n\rblogdown.author = author of new posts\rblogdown.ext = default extension of new posts (can also be “.md” or “.Rmarkdown”)\rblogdown.subdir = theme-specific, you need to know your theme and content folder here\rblogdown.yaml.empty = I told you to do that here\rblogdown.new_bundle = what this whole post is about!\rblogdown.title_case = “nEed More coFFee” –\u0026gt; “Need More Coffee” (it tidies all your post titles to title case)\r\r\rThe Newline Thing\rHere is a massive .Rprofile gotcha: this file must end with a blank line. So make sure you add an empty line at the end of the file, then save it, and restart your R session.\nWant to make your general R life easier in the future? Follow Yihui’s advice and do this in RStudio to ensure that all source files end with a newline:\n\r\rUse Bundles\rAfter restarting R, try using the “New Post” Addin, this time with feeling. There is still one more gotcha though. Use the Addin to create your new bundled post. The only catch is that once you are looking at your exciting new post, you should delete the slug in the YAML (I posted an issue about this here).\nThe reason is that you want the link to your post to be:\nhttp://alison.rbind.io/post/2019-02-21-hugo-page-bundles/\nIf you include the slug, the link to your post will be:\nhttp://alison.rbind.io/post/2019-02-21-hugo-page-bundles/hugo-page-bundles\nAnother option is to update your config.toml file with permalinks like Yihui suggests (but beware: this will change all your past links as well, requiring some Netlify redirects):\n[permalinks]\rpost = \u0026quot;/:year/:month/:day/:slug/\u0026quot;\rThe default here from Hugo was /post/:year-:month-:day-:slug/:slug/.\nA small note: if you want to add relative links from a blog post to another post in your same blog. So [this](/post/2019-02-19-hugo-archetypes/) becomes this.\nNow, add images and data files to your ❤️’s content! But you may want to do one more thing…\n\rUpdate Metadata\rIf you are anything like me, you may draft a blog post then come back to it later. For example, I started this post 2 days ago, but want to publish it today, 2021-11-05. The cool thing that was already built-in to blogdown is the “Update Metadata” Addin. With your blog post open (it should be called index.Rmd)1, click on Addins and select “Update Metadata”. You should see a window like this:\nCheck the box to rename the file if the date has changed. RStudio will tell you your file has been deleted- which is technically true since the folder was renamed, but don’t panic!\nClick YES. The index.Rmd file that is now open should have an updated date field in the YAML. In your RStudio file viewer, you may want to click on “content” at this point then navigate back to view your post- then you will then see that the folder name now has an updated date too.\n\r\rIf no post is open, you will get an error: Warning message: The current document does not seem to contain YAML metadata↩︎\n\r\r\r","date":1550707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550707200,"objectID":"57794aaa1dd7911236dd044032a33bf8","permalink":"/post/2019-02-21-hugo-page-bundles/","publishdate":"2019-02-21T00:00:00Z","relpermalink":"/post/2019-02-21-hugo-page-bundles/","section":"post","summary":"Why (and how) you should use Hugo's new page bundles feature","tags":["blogdown"],"title":"A Spoonful of Hugo: Page Bundles","type":"post"},{"authors":["alison"],"categories":["blogdown","netlify","hugo"],"content":"\r\r“Just a spoonful of Hugo helps the blog go down.”\r- me, only somewhat kidding\n\rYou can read the previous post about my “Spoonful of Hugo” series here. In this series, I’m sharing small spoonfuls of Hugo that I have learned that hopefully can help you get your site UP (and even better- more efficient, more streamlined, more automated).\nThis is my second post in this series, and it is a relatively quick one. Just do this. This one is a no-brainer.\n Thanks to Mara Averick for alerting me that with Hugo version 0.54.0 and onward, there is a trailing zero at the end of Hugo versions now. So for versions before 0.54.0, use the format: 0.53; for later versions use 0.54.0 (0.54 will not work).   Use Netlify to Deploy\rFirst, you’ll need to use Netlify! I am a very happy Netlify user and currently have approximately 33 sites deployed. To setup a new account, navigate to Netlify and click on the Sign Up link.\nSign up with GitHub to connect your GitHub and Netlify accounts (as shown below).\nIf you use a different version control service, select GitLab or BitBucket instead.\nThe last step is to use the Netlify UI in browser do New Site from Git \u0026gt; pick your repo. You’ll be prompted to fill in these fields, they are probably already filled in correctly for you:\nThe next part is the advanced build settings:\nSee that pro tip about the netlify.toml? Let’s do that! You can leave these fields as is.\n\rWhy netlify.toml?\rIn their Build Gotchas:\n\r“If your build works locally, the next debugging step is to ensure the package versions we use to build match yours. You can find the settings for these in the Build Settings doc. That’s the leading cause of build failure.”\n\rYes that is right- package version mismatches are the leading cause of build failure with Netlify. What does this look like for blogdown users? This means that you are running a version of Hugo locally that doesn’t match the version that Netlify is using to build your site. Most of the time, you are using a more recent version of Hugo than the one Netlify uses. This means that the files your theme relies on may be using newer Hugo functions that were introduced in later Hugo versions- functions that Netlify won’t be able to find working from an older Hugo version. You’ll get all the build errors.\nYou can check your local Hugo version by running this code in your R console:\nblogdown::hugo_version()\r## [1] \u0026#39;0.73.0\u0026#39;\rNow, we want Netlify to use this same version of Hugo when it builds your site. You can do this two ways:\nDo this in your browser (👎)\rDo this in your project root directory in a netlify.toml file (👍)\r\r\rAdd the netlify.toml File\rAdding this file means that team members can see for themselves what version of Hugo you are running- if it is buried in the Netlify UI, you can’t see that information unless you sift through the public build logs (no thanks). Making the file as plain text in the root of your blogdown project directory means that:\n\rit is version controlled (yay!) and\rother people who use/learn from/contribute to your blog can actually reproduce your site with the same site configuration.\rBonus: you can set the Hugo versions for branch deploys too.\r\rHere is an example from my own netlify.toml file1:\n[build] publish = \u0026quot;public\u0026quot;\rcommand = \u0026quot;hugo\u0026quot;\r[context.production.environment]\rHUGO_VERSION = \u0026quot;0.54.0\u0026quot; # if older, use format: 0.53 (no trailing zero)\rHUGO_ENV = \u0026quot;production\u0026quot;\rHUGO_ENABLEGITINFO = \u0026quot;true\u0026quot;\r[context.branch-deploy.environment]\rHUGO_VERSION = \u0026quot;0.54.0\u0026quot; # if older, use format: 0.53 (no trailing zero)\r[context.deploy-preview.environment]\rHUGO_VERSION = \u0026quot;0.54.0\u0026quot;\rYou can leave off the last two chunk if you don’t want to use branch deploys or preview deploys, but I ❤️ these two Netlify features and encourage you to try them out. I’ve starting drafting individual blog posts and tutorials in branches, and then I can see them rendered and share them for feedback without asking collaborators to clone and build the repository locally. It is lovely. Every branch and pull request gets a link 🎉.\nSo add this file to your blogdown site repo and push to GitHub.\nNote that, according to the Netlify docs:\n\r“During a build, the following ordering determines which context covers a particular deploy: UI settings are overridden if a netlify.toml file is present in the root folder of the repo and there exists a setting for the same property/redirect/header in the toml file.”\n\rIf you look in your site’s Netlify deploy log, you should see entries like this:\n7:47:13 PM: Found netlify.toml. Overriding site configuration\r7:47:13 PM: Starting build script\r7:47:13 PM: Installing dependencies\r7:47:14 PM: Started restoring cached node version\r7:47:17 PM: Finished restoring cached node version\r7:47:18 PM: v8.15.0 is already installed.\r7:47:19 PM: Now using node v8.15.0 (npm v6.4.1)\r7:47:19 PM: Attempting ruby version 2.3.6, read from environment\r7:47:20 PM: Using ruby version 2.3.6\r7:47:20 PM: Using PHP version 5.6\r7:47:20 PM: Installing Hugo 0.54.0\rSuccess!\n\r\rthe leading zero matters for Hugo versions, so 0.53 works but .53 will not. For versions \u0026gt;= 0.54.0, the trailing zero also matters, so 0.54.0 works but 0.54 will not.↩︎\n\r\r\r","date":1550620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550620800,"objectID":"8a82d0bfce9f49b7c489323259c476b4","permalink":"/post/2019-02-19-hugo-netlify-toml/","publishdate":"2019-02-20T00:00:00Z","relpermalink":"/post/2019-02-19-hugo-netlify-toml/","section":"post","summary":"Why you should use a netlify.toml file in your blogdown site","tags":["blogdown","netlify","hugo"],"title":"A Spoonful of Hugo: The netlify.toml File","type":"post"},{"authors":["alison"],"categories":["hugo","blogdown"],"content":"\r\r“Just a spoonful of Hugo helps the blog go down.”\r- me, only somewhat kidding\n\rAs a happy blogdown user, a common story I hear from other #rstats users is that you try to change one little thing in Hugo, and the whole site breaks. Here be dragons for folks who aren’t web developers.\nI’m here to tell you that there are small spoonfuls of Hugo that can help you get your site UP (and even better- more efficient, more streamlined, more automated), even if you are not in the least bit interested in transitioning into a career in web development 😏.\nMy Project\rThe education team at RStudio needs a website and we have a short wishlist:\n\rWe want something we can maintain ourselves,\rWe want to look consistent with other RStudio sites on the outside, and\rWe want to be consistent on the inside so that we can get help if/when we need it.\r\rThis led me to the current tidyverse.org blogdown site. I wanted to make a copy of the site then customize for the education team, but I noticed that the source code for the site didn’t make it easy for me to copy the structure of the site and edit only the content of the site. This is one of the real strengths of Hugo, so I embarked on a learning adventure.\n\r\r\rvia GIPHY\r\rAs a result, I have been living and breathing Hugo lately. As in, my husband now recognizes Mike Dane’s voice. You may not have have met Mike yet, but he appears in all the video tutorials in the Hugo docs. His screencasts have been really helpful to me, like this one on templating. I’ve also spent a lot of time actually reading the docs (which are pretty good!), reading posts and answers on the Hugo discourse community site, and spelunking around inside the actual source code for two very well structured Hugo sites:\nThe actual Hugo site: https://github.com/gohugoio/hugoDocs\rThe rOpenSci site: https://github.com/ropensci/roweb2\r\rI’ll be using this post and other later posts to share some of the things I’ve learned about Hugo along the way. Mainly breadcrumbs to myself, but I hope these help other people too.\nFor reference, I’m using Hugo via the blogdown R package, and within the RStudio IDE. These are my blogdown and Hugo versions:\npackageVersion(\u0026quot;blogdown\u0026quot;)\r## [1] \u0026#39;0.20\u0026#39;\rblogdown::hugo_version()\r## [1] \u0026#39;0.73.0\u0026#39;\r\rtl;dr: A Teaspoon of Archetypes\rAdd custom archetypes as .md files to your project root directory (do not touch the archetypes folder in your themes/archetypes folder).\n\rIf you don’t have that as an empty folder in your project root, make one, then add your archetype files to it.\rIf you are making a new blogdown site, I recommend using these options to keep your empty directories1:\r\rlibrary(blogdown)\rnew_site(theme = \u0026quot;jpescador/hugo-future-imperfect\u0026quot;, sample = TRUE, theme_example = TRUE, empty_dirs = TRUE, # this!\rto_yaml = TRUE)\r\rFigure 1: Using the RStudio Project Wizard\r\rUse the “New Post” Addin in RStudio to create any and all new content for your site (not just posts!). Be sure to use the handy dropdown menu to select from all the possible archetypes. Also, careful about the subdirectory here- some themes use blog, others use news, articles, or posts.\n\rYour archetypes, while only markdown files, can include R code. When you use the Addin, be sure to choose R Markdown (.Rmd) as the format so that you can run the code.\n\rDon’t miss this great blog post by my friend and the great educator Leo Collado-Torres on archetypes.\r\r\r\rA Tablespoon of Archetypes\rOne of the easiest things you can do for yourself is customize your site’s archetypes. From the Hugo docs:\n\r“Archetypes are templates used when creating new content.”\n\rRight away when I cloned the tidyverse site, I noticed that there were instructions for how to contribute a new article (or blog post) in the README.md and in a separate CONTRIBUTING.md file. Then I noticed this open GitHub issue from Mara Averick (the tidyverse developer advocate) titled “Fix README/CONTRIBUTING so there’s one source of mechanical info?”.\nI also noticed that there was no project root folder called archetypes, which is where you would store your custom site archetype files as .md files. In fact, there is no theme folder as you might expect either, which is where you could view the default theme archetypes. Let’s look at some from other Hugo themes:\nThe default Hugo theme for blogdown, Lithium, has just one archetype: default.md\n---\rtitle: \u0026#39;\u0026#39;\rdate: \u0026#39;\u0026#39;\r---\rIn contrast, the Hugo Academic theme has A LOT: https://github.com/gcushen/hugo-academic/tree/master/archetypes; here is the content of the one for new posts:\n+++\rtitle = \u0026quot;{{ replace .Name \u0026quot;-\u0026quot; \u0026quot; \u0026quot; | title }}\u0026quot;\rsubtitle = \u0026quot;\u0026quot;\r# Add a summary to display on homepage (optional).\rsummary = \u0026quot;\u0026quot;\rdate = {{ .Date }}\rdraft = false\r# Authors. Comma separated list, e.g. `[\u0026quot;Bob Smith\u0026quot;, \u0026quot;David Jones\u0026quot;]`.\rauthors = []\r# Tags and categories\r# For example, use `tags = []` for no tags, or the form `tags = [\u0026quot;A Tag\u0026quot;, \u0026quot;Another Tag\u0026quot;]` for one or more tags.\rtags = []\rcategories = []\r# Projects (optional).\r# Associate this post with one or more of your projects.\r# Simply enter your project\u0026#39;s folder or file name without extension.\r# E.g. `projects = [\u0026quot;deep-learning\u0026quot;]` references # `content/project/deep-learning/index.md`.\r# Otherwise, set `projects = []`.\r# projects = [\u0026quot;internal-project\u0026quot;]\r# Featured image\r# To use, add an image named `featured.jpg/png` to your page\u0026#39;s folder. [image]\r# Caption (optional)\rcaption = \u0026quot;\u0026quot;\r# Focal point (optional)\r# Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight\rfocal_point = \u0026quot;\u0026quot;\r+++\r\rA quick note: you may have noticed differences in both the content between these two files but also the structure. The first is a YAML file, the second is a TOML file. For blogdown users, you may want to use YAML. This is also why I recommend when you set up your site to use the to_yaml = TRUE option (in the Project Wizard from figure 1, check the “Convert all metadata to YAML” box; otherwise, the exampleSite will contain TOML instead of YAML)2.\nIf you read the original tidyverse CONTRIBUTING.md file, the instructions include a fair bit of R code that I would guess means a lot of copying and pasting into new posts. For example, the R Markdown setup chunk and the code for using usethis::use_tidy_thanks() for package releases. I studied the contributing guidelines, and parsed three different “kinds” of articles that are commonly contributed, each with a different archetype:\nThe default.md- this is just for plain old markdown posts and basically sets up the YAML of the post to be the same as it is now (currently, there is no archetype dictating the content- it is pulling from a project-level .Rprofile).\n\rA default-rmarkdown.md which should only be used with an R Markdown post and provides only the setup chunk at the top.\n\rA package-release.md which also should only be used with an R Markdown post and adds the usethis::use_tidy_thanks() code chunk (this is pseudo-code so the default chunk option is set to eval = FALSE).\n\r\rSo I drafted a pull request that adds these three archetypes to the GitHub repository for the tidyverse.org. Here is the “after” Addin view:\nHere’s hoping Hugo archetypes make some things about adding new content to your site easier. There is no Hugo involved, other than realizing that Hugo will look first in your themes/\u0026lt;THEME-NAME\u0026gt;/archetypes/ folder, then in your project root archetypes/ folder next. DO NOT TOUCH any files in your themes/ directory.3\nYou may want to set up archetypes for your blogdown site if you have a “signature” R setup chunk that loads your preferred knitr chunk options, common libraries you always load at setup like tidyverse, ggplot2 themes you prefer (theme_minimal() FTW), etc. This may be especially helpful if you have multiple team members contributing to a single site and you want their posts to have a uniform setup. Then archetypes can be a real time- and sanity-saver. Get more ideas from Leo’s blog post on archetypes. You can also make directory based archetypes if you use Hugo page bundles, which is a topic of a future post.\n\r\rThese setup options are newish to the blogdown package: https://github.com/rstudio-education/arm-workshop-rsc2019/issues/8↩︎\n\rIf you end up with TOML in your content files, run this R code: hugo_convert(to = \"YAML\", unsafe = TRUE)↩︎\n\rTrust me on this one- if you ever want to update your site this will make that process way harder.↩︎\n\r\r\r","date":1550534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550534400,"objectID":"a160260aa45ac1699baf5f119b4c4e60","permalink":"/post/2019-02-19-hugo-archetypes/","publishdate":"2019-02-19T00:00:00Z","relpermalink":"/post/2019-02-19-hugo-archetypes/","section":"post","summary":"Why you should use Hugo archetypes in your blogdown site","tags":["blogdown"],"title":"A Spoonful of Hugo: Archetypes","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"bd516ebe91dc31b2b1f084d003009111","permalink":"/accolades/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/accolades/","section":"","summary":"The highs in my journey","tags":null,"title":"Accolades","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"How to get in touch","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"/experience/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"My journey so far","tags":null,"title":"Experience","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3960dd3bdc6f629fb800d1d2aaa7224f","permalink":"/resume/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/resume/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks","type":"widget_page"},{"authors":["Kushagra Gupta","Aditya Gulati","Esha Gupta","Abhinav Airan"],"categories":["misc"],"content":"Our team won the silver medal in the competition. The problem involved data analysis and inference from a dataset for a globular star cluster. The datasets had the coordinates, color index (for different wavelengths) and magnitude of every star in the cluster. The task involved using the datasets appropriately to analyze the globular cluster to determine some parameters and peculiarities of the same.\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"e379e491aa91ff7245148d6c5f4cfe21","permalink":"/project/techmeet/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/project/techmeet/","section":"project","summary":"Data science hackathon on analysing globular star clusters.","tags":["misc"],"title":"Data Science Hackathon, Inter IIT Tech Meeth 2018","type":"project"},{"authors":["Kushagra Gupta","Harish Rajagopal","Harsh Sinha","Apurv Gupta"],"categories":["ml"],"content":"   The problem was part of the machine learning competition in inter IIT tech meet, 2018, where we won the silver medal in the competition. The task was to classify all pixels in a given satellite image, as belonging to one of nine given classes. Our approach towards solving this challenge involved three approaches, with each approach performing better than the last one. The ﬁrst model involved the implementation of a basic U-Net architecture with custom metrics designed for this problem. The second approach involved a P-Net architecture with extensive hyperparameter tuning to improve model accuracy. The third approach utilizes nine distinct U-Net architectures for the segmentation, giving results comparable to the state of the art model for multi-class image segmentation.\nMy contributions to the project :\n I designed and implemented a U-Net architecture for image segmentation of high-quality satellite images by using context-based representations. I improved the existing accuracy from 84% to 91% on just 25 images by developing a new algorithm based on 9 U-Nets using \u0026lsquo;one vs all\u0026rsquo; classification approach. I used localized optimization of parameters with high frequency to break the bottleneck of a small dataset.  ","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"5e988f48affaca82772d2d76feabc6bc","permalink":"/project/eits/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/project/eits/","section":"project","summary":"Designed and implemented a U-Net architecture for image segmentation of high quality satellite images by using context-based representations.","tags":["ml"],"title":"Eye in the Sky","type":"project"},{"authors":["alison"],"categories":["readr","readxl","data import"],"content":"\r A shorter version of this blog post now appears as an article vignette for the readxl package, thank you to Jenny Bryan for the invitation!   A problem I run up against a lot when working with other people’s data is having multiple header rows in the source data file. I like to use readr functions to read in rectangular data like .csv and .tsv files, but if you skip rows at import using the skip argument, you lose the header row as well, which usually has column names. The problem I often have is that the header row has column names that I want to keep, but I’d like to skip the second row (or more), which has some junk in it. Usually this row is some kind of data dictionary inserted between the row of column names and the actual data.\nIn this post, I’ll walk through a solution to this problem, using the readr package. You can also watch along in the video.\n  Warning!: I made a mistake when I said readr uses the first 100 rows of your data to predict column types- it uses the first 1000 rows.\n\rDownload stickers.csv\rBeing sticker rich\rThis dataset is from an article published in PLOS ONE called “Being Sticker Rich: Numerical Context Influences Children’s Sharing Behavior”. In this study, children (ages 3–11) received a small (12, “sticker poor”) or large (30, “sticker rich”) number of stickers, and were then given the opportunity to share their windfall with either one or multiple anonymous recipients. This type of experimental design is a version of the Dictator Game.\nThe main research questions the authors explored were: do the number of available resources and/or the number of potential recipients alter the likelihood of a child donating and/or the amount they donate? But, in order to answer this question, we have to be able to read in the data! Luckily, these lovely developmental psychologists opted to share their data on the Harvard Dataverse as a tab-delimited file.\nIf you download the file, you can open it up in a plain text editor.\rYou can also open it with Microsoft Excel.\r\rRead in the file\rLet’s start by creating a variable called link to store the link to the data file.\n# create variable to store url\rlink \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot;\rThe file has a .tab extension, so we know it is tab-delimited. This means that the right readr function for reading this file is read_tsv. Since we stored our link already as a character string, that is the only argument to the read_tsv function.\n#install.packages(\u0026quot;readr\u0026quot;)\rlibrary(readr) # load the readr package\rstickers \u0026lt;- read_tsv(link) # spec()\rNow, we know the second row of data is wonky, but how can we see that in R? There are a number of ways we can go spelunking around into our data file. The easiest to print it. Since we used readr, we have a tibble, which nicely prints to screen.\nstickers\r# # A tibble: 402 x 18\r# SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths\r# \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r# 1 [Included Sa~ 1=12:1; ~ 1=12; 2=30 1=1 recipient;~ 1=fem~ NA\r# 2 1 1 1 1 1 36\r# 3 2 1 1 1 2 36\r# 4 3 1 1 1 2 36\r# 5 4 1 1 1 1 36\r# 6 5 1 1 1 2 36\r# 7 6 1 1 1 2 36\r# 8 7 2 1 2 1 36\r# 9 8 2 1 2 2 36\r# 10 9 3 2 1 2 36\r# # ... with 392 more rows, and 12 more variables: Ageyears \u0026lt;dbl\u0026gt;,\r# # Agegroups \u0026lt;chr\u0026gt;, `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;,\r# # RightEnvelope \u0026lt;chr\u0026gt;,\r# # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;,\r# # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;,\r# # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;,\r# # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt;\rUnfortunately, dplyr::glimpse can’t help us much, because we have one variable name that is ridiculously long (absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)). We’ll fix that with dplyr::rename.\nlibrary(dplyr)\rglimpse(stickers)\r# Rows: 402\r# Columns: 18\r# $ SubjectNumber \u0026lt;chr\u0026gt; ...\r# $ Condition \u0026lt;chr\u0026gt; ...\r# $ NumberStickers \u0026lt;chr\u0026gt; ...\r# $ NumberEnvelopes \u0026lt;chr\u0026gt; ...\r# $ Gender \u0026lt;chr\u0026gt; ...\r# $ Agemonths \u0026lt;dbl\u0026gt; ...\r# $ Ageyears \u0026lt;dbl\u0026gt; ...\r# $ Agegroups \u0026lt;chr\u0026gt; ...\r# $ `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt; ...\r# $ LeftEnvelope \u0026lt;chr\u0026gt; ...\r# $ RightEnvelope \u0026lt;chr\u0026gt; ...\r# $ `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt; ...\r# $ `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt; ...\r# $ Giveornot \u0026lt;chr\u0026gt; ...\r# $ LargerEnvelopeabs \u0026lt;chr\u0026gt; ...\r# $ LargeEnvelopepercent \u0026lt;chr\u0026gt; ...\r# $ SmallerEnvelopeabs \u0026lt;chr\u0026gt; ...\r# $ SmallEnvelopepercent \u0026lt;chr\u0026gt; ...\rMore options:\nhead(stickers)\r# # A tibble: 6 x 18\r# SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths\r# \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r# 1 [Included Sa~ 1=12:1; ~ 1=12; 2=30 1=1 recipient;~ 1=fem~ NA\r# 2 1 1 1 1 1 36\r# 3 2 1 1 1 2 36\r# 4 3 1 1 1 2 36\r# 5 4 1 1 1 1 36\r# 6 5 1 1 1 2 36\r# # ... with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;,\r# # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;,\r# # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;,\r# # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;,\r# # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;,\r# # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt;\rtail(stickers)\r# # A tibble: 6 x 18\r# SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths\r# \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r# 1 396 1 1 1 2 136\r# 2 397 4 2 2 1 136\r# 3 398 1 1 1 1 137\r# 4 399 1 1 1 2 137\r# 5 400 4 2 2 2 139\r# 6 401 3 2 1 1 143\r# # ... with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;,\r# # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;,\r# # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;,\r# # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;,\r# # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;,\r# # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt;\rnames(stickers)\r# [1] \u0026quot;SubjectNumber\u0026quot; # [2] \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; # [4] \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; # [6] \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; # [8] \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; # [10] \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; # [12] \u0026quot;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026quot;\r# [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; # [14] \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; # [16] \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; # [18] \u0026quot;SmallEnvelopepercent\u0026quot;\r# View()\rNow we are ready to diagnose the problem!\nProblem: the first row is not really data. It is metadata about the variables, and it is screwing up readr’s ability to predict our column types.\nSolution: we’ll use readr and the read_tsv() function to read in the data twice. In Step 1, we’ll create a character vector of the column names only. In Step 2, we’ll read in the actual data and skip the multiple header rows at the top. When we do this, we lose the column names, so we use the character vector of column names we created in Step 1 instead.\n\rRead in the file (again)\rStep 1\rGoal: we want to read in the first row only and save it as a character vector called sticker_names. This row contains the correct column names that we’ll need in Step 2.\nsticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE\rrename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names()\rsticker_names\r# [1] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; \u0026quot;stickersgiven\u0026quot; # [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; \u0026quot;SmallEnvelopepercent\u0026quot;\rglimpse(sticker_names)\r# chr [1:18] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; \u0026quot;NumberStickers\u0026quot; \u0026quot;NumberEnvelopes\u0026quot; ...\r\rStep 2\rGoal: we want to read in all the rows except for the first two rows, which contained the variable names and variable descriptions. We want to save this as stickers, and set the column names to the sticker_names object we created in Step 1.\nstickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names)\rglimpse(stickers)\r# Rows: 401\r# Columns: 18\r# $ SubjectNumber \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...\r# $ Condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3,...\r# $ NumberStickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,...\r# $ NumberEnvelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1,...\r# $ Gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1,...\r# $ Agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, 36,...\r# $ Ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,...\r# $ Agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\r# $ `Subject\u0026#39;sEnvelope` \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 30, ...\r# $ LeftEnvelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18, 1...\r# $ RightEnvelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, N...\r# $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18, 1...\r# $ `PercentGiven(Outof100percent)` \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.33,...\r# $ Giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,...\r# $ LargerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA, N...\r# $ LargeEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, ...\r# $ SmallerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, N...\r# $ SmallEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, ...\r\r\rFin!\rAll together now: the final solution!\n# load packages\rlibrary(readr)\rlibrary(dplyr)\r# create variable to store url\rlink \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot;\r# read in column names only\rsticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE\rrename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names()\r# read in data, set column names\rstickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names)\r\rAddendum\rFor good measure, I would add a final step to everything above and use janitor::clean_names() to put all the variable names into snake case. So my final final solution is here:\n# load packages\rlibrary(readr)\rlibrary(dplyr)\rlibrary(janitor)\r# create variable to store url\rlink \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot;\r# read in column names only\rsticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE\rrename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names()\r# read in data, set column names\rstickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names) %\u0026gt;% clean_names()\rstickers\r# # A tibble: 401 x 18\r# subject_number condition number_stickers number_envelopes gender agemonths\r# \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r# 1 1 1 1 1 1 36\r# 2 2 1 1 1 2 36\r# 3 3 1 1 1 2 36\r# 4 4 1 1 1 1 36\r# 5 5 1 1 1 2 36\r# 6 6 1 1 1 2 36\r# 7 7 2 1 2 1 36\r# 8 8 2 1 2 2 36\r# 9 9 3 2 1 2 36\r# 10 10 3 2 1 2 36\r# # ... with 391 more rows, and 12 more variables: ageyears \u0026lt;dbl\u0026gt;,\r# # agegroups \u0026lt;dbl\u0026gt;, subjects_envelope \u0026lt;dbl\u0026gt;, left_envelope \u0026lt;dbl\u0026gt;,\r# # right_envelope \u0026lt;dbl\u0026gt;, stickersgiven \u0026lt;dbl\u0026gt;,\r# # percent_given_outof100percent \u0026lt;dbl\u0026gt;, giveornot \u0026lt;dbl\u0026gt;,\r# # larger_envelopeabs \u0026lt;dbl\u0026gt;, large_envelopepercent \u0026lt;dbl\u0026gt;,\r# # smaller_envelopeabs \u0026lt;dbl\u0026gt;, small_envelopepercent \u0026lt;dbl\u0026gt;\rglimpse(stickers)\r# Rows: 401\r# Columns: 18\r# $ subject_number \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12...\r# $ condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3...\r# $ number_stickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2...\r# $ number_envelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1...\r# $ gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1...\r# $ agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, 36, 3...\r# $ ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3...\r# $ agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r# $ subjects_envelope \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 30, 12...\r# $ left_envelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18, 18,...\r# $ right_envelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, NA,...\r# $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18, 18,...\r# $ percent_given_outof100percent \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.33, 0...\r# $ giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0...\r# $ larger_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA, NA,...\r# $ large_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, 1....\r# $ smaller_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, NA,...\r# $ small_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, 0....\r\rBonus data dictionary\rAs an extra bonus, when you do have extra header rows, you can create a data dictionary using the gather() function from the tidyr package.\nlibrary(tidyr)\rstickers_dict \u0026lt;- read_tsv(link, n_max = 1) %\u0026gt;% rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% clean_names() %\u0026gt;% gather(variable_name, variable_description)\rstickers_dict\r# # A tibble: 18 x 2\r# variable_name variable_description # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; # 1 subject_number [Included Sample Only] # 2 condition 1=12:1; 2=12:2, 3=30:1, 4=30:2 # 3 number_stickers 1=12; 2=30 # 4 number_envelopes 1=1 recipient; 2=2 recipients # 5 gender 1=female; 2=male # 6 agemonths \u0026lt;NA\u0026gt; # 7 ageyears \u0026lt;NA\u0026gt; # 8 agegroups 1=3-4yrs; 2=5-6yrs; 3=7-8yrs; 4=9-11yrs # 9 subjects_envelope How many stickers did the child keep for themselves ~\r# 10 left_envelope 1 recipient conditions: How many stickers the subjec~\r# 11 right_envelope 1 recipient conditions: N/A; 2 recipient conditions:~\r# 12 stickersgiven Regardless of condition, the number of stickers the ~\r# 13 percent_given_outof100~ Regardless of condition, the proportion of stickers ~\r# 14 giveornot 1=Donated 1 or more stickers to the recipient(s); 0=~\r# 15 larger_envelopeabs Raw number of stickers (out of 30: Condition 2 or 4 ~\r# 16 large_envelopepercent Proportion of stickers (out of 100%; Condition 2 or ~\r# 17 smaller_envelopeabs Raw number of stickers (out of 30: Condition 2 or 4 ~\r# 18 small_envelopepercent Proportion of stickers (out of 100%; Condition 2 or ~\r\rUseful resources\r\rGreat blog post from Lisa DeBruine using readxl to read in data with multiple header rows (including those with merged cells!): https://debruine.github.io/multirow_headers.html\rThis GitHub issue with Hadley’s response that solved all my problems:\rhttps://github.com/tidyverse/readr/issues/179\rMy original tweet when I discovered this trick!\r\rNeat #rstats #readr #tidyverse solution to read data when 1st row is header + 2nd row is junk, thanks @hadleywickham https://t.co/5TuH7vNaID pic.twitter.com/woZ3HuECge\n\u0026mdash; Alison Presmanes Hill (@apreshill) September 4, 2017  \r","date":1531008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531008000,"objectID":"c18d02feb96be2ee747ed7aa57c7bcc4","permalink":"/post/2018-02-23-read-multiple-header-rows/","publishdate":"2018-07-08T00:00:00Z","relpermalink":"/post/2018-02-23-read-multiple-header-rows/","section":"post","summary":"Using the readr package to sidestep a common problem","tags":null,"title":"Read Data with Multiple Header Rows into R","type":"post"},{"authors":["Kushagra Gupta","Avik Pal"],"categories":["ml"],"content":"   Deep learning has revolutionized many areas of machine learning, and it is poised to do so with recommender systems as well. This project shows how deep autoencoders can be successfully trained even on relatively small amounts of data by using both well established (dropout) and scaled exponential linear units deep learning techniques. We used iterative output re-feeding - a technique which allows dense updates in collaborative ﬁltering, increases the learning rate and further improves generalization performance of the model.\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"a06bd57ad79c53a7a81a511ee7d9a39f","permalink":"/project/kvpy/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/project/kvpy/","section":"project","summary":"Developed online collaborative filtering based deep learning algorithm recommender based on AutoEncoder using tensorflow.","tags":["ml"],"title":"Autoencoder Recommendation Engine","type":"project"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":["Kushagra Gupta","Avik Pal","Ahsan Barkati","Yatin Dandi"],"categories":["ml"],"content":"Sub-project : ​​An online recommendation system based on collaborative filtering for implicit data using sentiment and frequency dependent weighting schemes. Technical details :\n Implemented a state of the art algorithm for online collaborative filtering based on Fast Matrix Factorization for Online Recommendation with Implicit Feedback (He et al.) using Numpy. Integrated element-wise Alternating Least Squares (eALS) based incremental update strategy for online learning. Developed an online collaborative filtering based deep recommender algorithm based on AutoEncoder in tensorflow. Used the VADER model in NLTK for sentiment analysis of comments. Improved results of algorithm by using interaction count and sentiment dependent weighting scheme for the observed data and a frequency aware weighting scheme for the missing data. Built multiple Kafka consumers and producer for parallely consuming real time interaction data of comments, likes and views to produce online recommendations for users. Used locust to simulate parallel user interaction to test recommendation algorithm. Used an eventually consistent engagement database (Couchbase) for storing user and item based data.  Sub-Project: ​​Identification and Classification of toxic comments. Technical Details:\n Implemented a Bidirectional LSTM based model using Keras for flagging toxic comments based on six metrics. Built Kafka consumer and producer data-pipelines for recording and processing new comments.  ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"3b21211a6efc492e8e09c5e163210a65","permalink":"/project/nyo/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/project/nyo/","section":"project","summary":"Developed collaborative filtering based online recommendation engine and a toxic comment classifier for a logistics platform.","tags":["ml"],"title":"Machine Learning for Large Scale Logistics Platform","type":"project"},{"authors":["alison"],"categories":["rladies","xaringan"],"content":"\rSo, you are doing an R-Ladies presentation…that’s awesome!\nThe short version\rI made an R-Ladies theme for xaringan slides. My original tweet about it:\nif you want to use @xieyihui\u0026#39;s awesome #xaringan package for #rstats slides but want more #Rladies flavor, there is now a built-in theme for that (with code highlighting)! Thanks to the awesome @RLadiesGlobal starter kit. Update the CSS in your YAML to use 🧙🏽‍♀️🧞‍♀️ pic.twitter.com/YnlGSVAMsl\n\u0026mdash; Alison Presmanes Hill (@apreshill) November 29, 2017  The way to use the theme is to update the YAML like so:\noutput:\rxaringan::moon_reader:\rcss: [\u0026quot;default\u0026quot;, \u0026quot;rladies\u0026quot;, \u0026quot;rladies-fonts\u0026quot;]\rMake sure your version of xaringan is up-to-date.\nBelow is a demo slide deck using the theme.\n\r(view the source .Rmd on GitHub)\n\rThe longer story\rI recommend Yihui’s xaringan package for slides. This is an R package, available through GitHub, for creating slideshows with remark.js through R Markdown. This means that you can:\n\rwrite all your slides in Markdown text\rinclude chunks of R code and rendered output like plots, results, tables, etc. in your slides\ruse git for version control and share your GitHub repository\r\rThis makes xaringan ideal for an R-Ladies presentation!1\nTo use the package, you’ll need the devtools package installed so that you can use the install_github function. Then do:\ndevtools::install_github(\u0026#39;yihui/xaringan\u0026#39;)\rAs Yihui points out in the documentation, if you use RStudio, you can use the menu to navigate to File -\u0026gt; New File -\u0026gt; R Markdown -\u0026gt; From Template -\u0026gt; Ninja Presentation, and you will see an R Markdown example.\nI first used xaringan a few months ago. I was working with Yihui on the blogdown book, and had signed up to lead a workshop for the Portland R User group. Obviously, such a workshop could not have powerpoint slides, so it seemed like the perfect time to learn xaringan.\nFor my workshop, I made a simple website for the newly founded R-Ladies PDX using blogdown (Thanks to Augustina and Deeksha, our fearless organizers). So naturally, my slides needed more purple.\nLuckily, the R-Ladies run a tight ship- they have a starter kit on GitHub that details all the pretty purples they like.\nAbout a month after I did the R-Ladies blogdown workshop, I saw this blog post by Yihui:\nFirst, I thought this was such a cool idea and I hope more people make and submit themes. Then I realized, I had already made a theme! I submitted a pull request2, Yihui helped me make some edits to the CSS files to make them more parsimonious with the default theme, I electronically signed a contributor agreement, and now the theme is there for you all to enjoy and use! You use the theme by editing the YAML:\noutput:\rxaringan::moon_reader:\rcss: [\u0026quot;default\u0026quot;, \u0026quot;rladies\u0026quot;, \u0026quot;rladies-fonts\u0026quot;]\rIf you use the theme and you are on twitter, I’d love to see it- please mention me on twitter!\nExamples!\n\rMy blogdown workshop slides: “Up and running with blogdown” (view the source .Rmd on GitHub)\r\r\r\rJessica Minnier’s slides for “Building Shiny Apps: With Great Power Comes Great Responsibility”\r\r\r\r\rIf you are new to xaringan, don’t miss the wiki!↩︎\n\rYihui’s technical instructions for contributors section of that blog post has been revised and is very detailed↩︎\n\r\r\r","date":1513555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513555200,"objectID":"4bae6dbc2d30b9c31fdd99c5dcdd6a81","permalink":"/post/2017-12-18-r-ladies-presentation-ninja/","publishdate":"2017-12-18T00:00:00Z","relpermalink":"/post/2017-12-18-r-ladies-presentation-ninja/","section":"post","summary":"A guide to using the R-Ladies xaringan slide theme","tags":["xaringan"],"title":"R-Ladies Presentation Ninja","type":"post"},{"authors":["alison"],"categories":["blogdown","hugo","netlify"],"content":"\r\r1 Read up on blogdown\r2 Caveats, disclaimers, etc.\r3 In GitHub\r4 In terminal\r5 In RStudio\r6 Build your site in RStudio\r7 Deploy in Netlify\r8 Going further\r\r\r1 Read up on blogdown\rBefore you start, I recommend reading the following:\n\rblogdown: Creating Websites with R Markdown by Yihui Xie and Amber Thomas\rMaking a Website Using blogdown, Hugo, and GitHub pages also by Amber Thomas\r\rI also found this comment by Eric Nantz, the creator of the R-Podcast, in the rbind/support issues section on GitHub to be helpful:\n\rhttps://github.com/rbind/support/issues/12\r\r\r2 Caveats, disclaimers, etc.\rEven with all the great resources I listed above, getting myself up and running took a few tries, so in this post I’m passing along what ended up working for me. Everyone’s mileage may vary, though, depending on your operating system and your approach. About me: I am a macOS user, and I use R, RStudio, Git (usually via GitLab, sometimes via GitHub), and terminal regularly, so I’m assuming familiarity here with all of these. If that is not you, here are some places to get started:\n\rFor Git: Happy Git with R by Jenny Bryan et al.\rFor RStudio: DataCamp’s Working with the RStudio IDE (free) by Garrett Grolemund\rFor Terminal: The Command Line Murder Mystery by Noah Veltman, and The UNIX Workbench by Sean Kross\r\rI also have Xcode and Homebrew installed- you will probably need these to download Hugo. If you don’t have either but are on a mac, this link may help:\n\rHow to install Xcode, Homebrew, Git, RVM, Ruby \u0026amp; Rails on Mac OS X\r\rFinally, I did not want to learn more about a lot of things! For instance, the nitty gritty of static site generators and how domain names work. I am a new mom, and just in the process of writing all this up, I filled up my tea mug twice with ice cold water, and filled my water bottle with scalding hot water. So, where offered, I followed the advice of Yihui and Amber. For example:\n\r“Considering the cost and friendliness to beginners, we currently recommend Netlify.” Sold.\r“If you are not familiar with domain names or do not want to learn more about them, an option for your consideration is a free subdomain *.rbind.io offered by RStudio, Inc.”. Done.\r\r\r3 In GitHub\rGo online to your GitHub account, and create a new repository (check to initialize with a README but don’t add .gitignore- this will be taken care of later). For naming your repo, consider your future deployment plan:\n\rIf you are going to use Netlify to host the site, you can name this repository anything you want!\r You can see some of the repo names used by members of the rbind organization here.   \rIf you want to host your site as a GitHub Page, you should name your repository yourgithubusername.github.io (so mine would have been apreshill.github.io). If you are going this route, I suggest you follow Amber’s instructions instead of mine!\r\r\rScreenshot above: Creating a new repository in GitHub\n\rGo to the main page of your new repository, and under the repository name, click the green Clone or download button.\n\rIn the Clone with HTTPs section, click on the clipboard icon to copy the clone URL for your new repository. You’ll paste this text into terminal in the next section.\n\r\r\r4 In terminal\rNow you will clone your remote repository and create a local copy on your computer so you can sync between the two locations (using terminal or your alternative command line tool for a Windows machine).\nUse cd to navigate into the directory where you want your repo to be\n\rOnce there, type: git clone [paste]. So my command looked like this:\n\r\rgit clone https://github.com/apreshill/apreshill.git\rAnd this is what printed to my terminal window:\nCloning into \u0026#39;apreshill\u0026#39;...\rremote: Counting objects: 3, done.\rremote: Compressing objects: 100% (2/2), done.\rremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\rUnpacking objects: 100% (3/3), done.\rChecking connectivity... done.\rClose terminal, you are done in there.\r\r\r5 In RStudio\rInstall blogdown from your RStudio console. If you already have devtools installed like I did, you can just use the second line below:\r\rif (!requireNamespace(\u0026quot;devtools\u0026quot;)) install.packages(\u0026quot;devtools\u0026quot;)\rdevtools::install_github(\u0026quot;rstudio/blogdown\u0026quot;)\rInstall Hugo using the blogdown package helper function:\r\rblogdown::install_hugo()\r# or\rlibrary(blogdown)\rinstall_hugo()\r This is where my instructions diverge from Ed’s- he states that blogdown won’t create a website in your root folder because the README.md file is already there. I didn’t find that to be the case- I tested this with a new site as well. If one way doesn’t work for you, try the other!   Use the top menu buttons in RStudio to select File -\u0026gt; New Project -\u0026gt; Existing Directory, then browse to the directory on your computer where your GitHub repo is and click on the Create Project button.\r\rScreenshot above: Creating a new project in an existing directory in RStudio\n\rNow you should be “in” your project in RStudio. If you are using git for version control, edit your *gitignore file. This file should be viewable in your file viewer pane in RStudio. Below is what it should look like: the first four lines will automatically be in this file if you have set up your RStudio Project, but if you plan to use Netlify to deploy, you need to add the public/ line (read about here.)\r\r.Rproj.user\r.Rhistory\r.RData\r.Ruserdata\rblogdown\r.DS_Store # if a windows user, Thumbs.db instead\rpublic/ # if using Netlify\r\r6 Build your site in RStudio\rNow you can finally build your site using the blogdown::new_site() function. But first you should at least think about themes…\n6.1 Picking a theme\rThere are over 90 Hugo themes. So I went back to the blogdown book. Thankfully, Yihui and Amber offer “to save you some time, we list a few themes below that match our taste…”. Huzzah- I went with hugo-academic! Whatever theme you choose, you’ll need to pick one of 3 ways to make your new site:\nIf you are happy with the default theme, which is the lithium theme, you can use:\r\rblogdown::new_site() # default theme is lithium\rIf you want a theme other than the default, you can specify the theme at the same time as you call the new_site function:\r\r# for example, create a new site with the academic theme\rblogdown::new_site(theme = \u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE)\rIf instead you want to add the theme later (like I did, because I didn’t see the above example until it was too late!), you can do this:\r\rlibrary(blogdown)\rnew_site() # default theme is lithium\r# need to stop serving so can use the console again\rinstall_theme(\u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE, update_config = TRUE)\r Now is a good time to re-read about blogdown::serve_site() and how LiveReload works (and how it blocks your R console by default)   I recommend setting theme_example = TRUE- some themes won’t provide an example site, but the academic theme did and I found it helpful to see. You can always delete the example content.\n\r6.2 Update project options\rIn your project in RStudio, go to the top menu bar of RStudio and select Tools -\u0026gt; Project Options and update following Yihui and Amber’s instructions.\n\r6.3 Edit your configurations\rRelevant reading:\n\rblogdown book chapter on configuration\rAdditional detail from Amber\rYou can also view my config.toml file\r\rNow, edit the baseurl in your config.toml file. The URL should always end with a / trailing slash. At this point, you probably haven’t deployed your site yet, so to view it locally you can use the Serve Site add-in, or run the blogdown::serve_site function. Both of these baseurls worked for me when viewing locally:\nbaseurl = \u0026quot;https://example.com/\u0026quot;\rbaseurl = \u0026quot;/\u0026quot;\r Make sure that the baseurl = listed ends with a trailing slash /!   Go ahead and edit all the other elements in the config.toml file now as you please- this is how you personalize your site!\n\r6.4 Addins \u0026amp; workflow\rRelevant reading:\n\rblogdown book chapter on the RStudio IDE\r\rAddins: use them- you won’t need the blogdown library loaded in the console if you use the Addins. My workflow in RStudio at this point (again, just viewing locally because we haven’t deployed yet) works best like this:\nOpen the RStudio project for the site\rUse the Serve Site add-in (only once due to the magic of LiveReload)\rView site in the RStudio viewer pane, and open in a new browser window while I work\rSelect existing files to edit using the file pane in RStudio\rAfter making changes, click the save button (don’t knit!)- the console will reload, the viewer pane will update, and if you hit refresh in the browser your local view will also be updated\rWhen happy with changes, add/commit/push changes to GitHub\r\rHaving blogdown::serve_site running locally with LiveReload is especially useful as you can immediately see if you have totally screwed up. For example, in editing my about.md file, this error popped up in my console after making a change and I was able to fix the error right away:\nStarted building sites ...\rERROR 2017/06/08 16:22:34 failed to parse page metadata for home/about.md: (18, 6): missing comma\rError: Error building site: Errors reading pages: Error: failed to parse page metadata for home/about.md: (18, 6): missing comma for about.md\rThe above workflow is only for editing existing files or posts, but not for creating new posts. For that, read on…\n\r6.5 Posting\rRelevant reading:\n\rblogdown book chapter on RStudio IDE\rblogdown book chapter on output formats: on .md versus .Rmd posts\rAdditional detail from Amber on adding a blog post\r\rBottom line:\nUse the New Post addin. But, you need the console to do this, so you have to stop blogdown::serve_site by clicking on the red Stop button first. The Addin is a Shiny interface that runs this code in your console: blogdown:::new_post_addin(). So, your console needs to be unblocked for it to run. You also need to be “in” your RStudio project or it won’t work.\n6.5.1 Draft posts\rRelevant reading:\n\rblogdown book chapter on building a website for local preview\r\rWhether you do a markdown or R Markdown post (see below), you should know that in the YAML front matter of your new file, you can add draft: TRUE and you will be able to preview your post using blogdown::serve_site(), but conveniently your post will not show up on your deployed site until you set it to false. Because this is a function built into Hugo, all posts (draft or not) will still end up in your GitHub repo though.\n\r6.5.2 New markdown posts\rPick one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: Markdown (recommended)\rUse the console to author a new .md post:\r\rblogdown::new_post()\rblogdown::new_post(ext = \u0026#39;.md\u0026#39;) # md is the default!\rHere are the ?new_post arguments:\nnew_post(title, kind = \u0026quot;\u0026quot;, open = interactive(),\rauthor = getOption(\u0026quot;blogdown.author\u0026quot;), categories = NULL, tags = NULL,\rdate = Sys.Date(), file = NULL, slug = NULL,\rtitle_case = getOption(\u0026quot;blogdown.title_case\u0026quot;),\rsubdir = getOption(\u0026quot;blogdown.subdir\u0026quot;, \u0026quot;post\u0026quot;),\rext = getOption(\u0026quot;blogdown.ext\u0026quot;, \u0026quot;.md\u0026quot;))\r Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload.   \r6.5.3 New R Markdown (.Rmd) posts\rAgain, you have your choice of one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: R Markdown (.Rmd) (recommended)\rUse the console to author a new .Rmd post:\r\rblogdown::new_post(ext = \u0026#39;.Rmd\u0026#39;) # md is the default!\rAfter you edit your .Rmd post, in addition to saving the changes in your .Rmd file, you must use blogdown::serve_site- this is how the output html file needs to be generated.\n Do not knit your .Rmd posts- use blogdown::serve_site instead. If you happen to hit the knit button, just Serve Site again to rewrite the .html file.   Ultimately, your YAML front matter looks something like this; note that some but not all features of rmarkdown::html_document are supported in blogdown:\n---\rtitle: \u0026quot;My Awesome Post\u0026quot;\rauthor: \u0026quot;John Doe\u0026quot;\rdate: \u0026quot;2017-02-14\u0026quot;\routput:\rblogdown::html_page:\rtoc: true\rtoc_depth: 1\rnumber_sections: true\rfig_width: 6\r---\r Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload and your .html file is properly output.   \r6.5.4 Adding images to a post\rIf you want to include an image that is not a figure created from an R chunk, the recommended method is to:\nAdd the image to your /static/img/ folder, then\rReference the image using the relative file path as follows:\r\r![my-image](/img/my-image.png)\r\r\r\r7 Deploy in Netlify\rDeploying in Netlify through GitHub is smooth. Yihui and Amber give some beginner instructions, but Netlify is so easy, I recommend that you skip dragging your public folder in and instead automate the process through GitHub.\nWhen you are ready to deploy, commit your changes and push to GitHub, then go online to Netlify.\n\rClick on the Sign Up button and sign up using your existing GitHub account (no need to create another account)\n\rLog in, and select: New site from Git -\u0026gt; Continuous Deployment: GitHub.\n\rFrom there, Netlify will allow you to select from your existing GitHub repositories. You’ll pick the repo you’ve been working from with blogdown, then you’ll configure your build. This involves specifying two important things: the build command and the publish directory (this should be public).\n\rMore about the build command from Netlify: “For Hugo hosting, hugo will build and deploy with the version 0.17 of hugo. You can specify a specific hugo release like this: hugo_0.15. Currently 0.13, 0.14, 0.15, 0.16, 0.17, 0.18 and 0.19 are supported. For version 0.20 and above, you’ll need to create a Build environment variable called HUGO_VERSION and set it to the version of your choice.” I opted for the former, and specified hugo_0.19.\r\r\rYou can check your hugo version in terminal using the command hugo version. This is what my output looked like, so I could run version 0.20 if I wanted to through Netlify, but I went with 0.19 and it works just fine.\n$ hugo version\rHugo Static Site Generator v0.20.7 darwin/amd64 BuildDate: 2017-05-08T18:37:40-07:00\rScreenshot above: Basic build settings in Netlify\n\rNetlify will deploy your site and assign you a random subdomain name of the form random-word-12345.netlify.com. Mine was particularly unfortunate, with the random word garbage-collector-janice. You should know that you can change this; I changed mine to apreshill.netlify.com.\n Anytime you change your subdomain name, you need to update the baseurl in your config.toml file (so I changed mine to baseurl = “https://apreshill.netlify.com/”).   At this point, you should be up and running with blogdown, GitHub, and Netlify, but here are some ideas if you want to go further…\n\r8 Going further\r8.1 Custom CSS\rI like to tinker with default theme settings like colors and fonts. Every Hugo theme is structured a little differently, but if you are interested, you can check out my custom css to see how I customized the academic theme, which provides a way to link to a custom CSS file in the config.toml file:\n # Link custom CSS and JS assets\r# (relative to /static/css and /static/js respectively)\rcustom_css = [\u0026quot;blue.css\u0026quot;]\r\r8.2 Formspree\rI used Formspree to make a contact form, which is an online service (managed on GitHub) that allows you to add an HTML form to your static site. No registration, just use the form and confirm your email address once. I added the following code into my contact widget:\n\u0026lt;form action=\u0026quot;https://formspree.io/your@email.com\u0026quot; method=\u0026quot;POST\u0026quot;\u0026gt;\r\u0026lt;label for=\u0026quot;name\u0026quot;\u0026gt;Your name: \u0026lt;/label\u0026gt;\r\u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;name\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt;\r\u0026lt;label for=\u0026quot;email\u0026quot;\u0026gt;Your email: \u0026lt;/label\u0026gt;\r\u0026lt;input type=\u0026quot;email\u0026quot; name=\u0026quot;_replyto\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt;\r\u0026lt;label for=\u0026quot;message\u0026quot;\u0026gt;Your message:\u0026lt;/label\u0026gt;\u0026lt;br\u0026gt;\r\u0026lt;textarea rows=\u0026quot;4\u0026quot; name=\u0026quot;message\u0026quot; id=\u0026quot;message\u0026quot; required=\u0026quot;required\u0026quot; class=\u0026quot;form-control\u0026quot; placeholder=\u0026quot;I can\u0026#39;t wait to read this!\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt;\r\u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_next\u0026quot; value=\u0026quot;/html/thanks.html\u0026quot; /\u0026gt;\r\u0026lt;input type=\u0026quot;submit\u0026quot; value=\u0026quot;Send\u0026quot; name=\u0026quot;submit\u0026quot; class=\u0026quot;btn btn-primary btn-outline\u0026quot;\u0026gt;\r\u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_subject\u0026quot; value=\u0026quot;Website message\u0026quot; /\u0026gt;\r\u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;_gotcha\u0026quot; style=\u0026quot;display:none\u0026quot; /\u0026gt;\r\u0026lt;/form\u0026gt;\r\r8.3 *.rbind.io domain names\rYou may want a different domain name than the one provided by Netlify. I opted for a free subdomain *.rbind.io offered by RStudio. To do the same, head over to the rbind/support GitHub page and open a new issue. All you need to do is let them know what your Netlify subdomain name is (*.netlify.com), and what you want your subdomain name to be (*.rbind.io). The awesome rbind support team will help you take it from there!\n Again, you will need to update the baseurl in your config.toml file to reflect your new rbind subdomain name (so mine is baseurl = “https://alison.rbind.io/”).   \r8.4 Have fun!\rLastly, don’t forget to just have fun with it. Happy blogdowning!\n\r\rvia GIPHY\r\r\r","date":1497225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497225600,"objectID":"dbf96d61412119467fb5db386d4f4407","permalink":"/post/2017-06-12-up-and-running-with-blogdown/","publishdate":"2017-06-12T00:00:00Z","relpermalink":"/post/2017-06-12-up-and-running-with-blogdown/","section":"post","summary":"A guide to getting up and running with blogdown, GitHub, and Netlify","tags":["blogdown"],"title":"Up \u0026 Running with blogdown","type":"post"},{"authors":["","Ahsan Barkati"],"categories":null,"content":"   This talk was part of Programming Club, IIT Kanpur\u0026rsquo;s talk on reinforcement learning (RL). It was targeted for sophomores and junior undergraduates with some statistical background on Markov process and Monte Carlo. It covered the components of an RL model, namely policy, value function and agent\u0026rsquo;s representation of the environment. Additionally, it covered the basics of Markov reward process and the Bellman expectation equation, necessary to define the update procedure of the RL agent mathematically. This talk also covered the basic algorithms of training the RL agent, namely policy and value iteration. Towards the end, it touched upon the model-free methods of RL and explained the underlying mechanism behind model-free learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"271d8fb7cd4b84326cda1763be3c7784","permalink":"/talk/2019-rl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/2019-rl/","section":"talk","summary":"This talk covers the terms encountered in RL and the mathematical concepts used in RL models.","tags":["Reinforcement Learning","Markov process","Monte Carlo"],"title":"Reinforcement Learning","type":"talk"}]