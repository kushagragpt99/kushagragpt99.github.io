<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>All Talks &amp; Workshops | Kushagra Gupta</title>
    <link>/talk/</link>
      <atom:link href="/talk/index.xml" rel="self" type="application/rss+xml" />
    <description>All Talks &amp; Workshops</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Kushagra Gupta 2021</copyright><lastBuildDate>Sat, 03 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/avatar.jpeg</url>
      <title>All Talks &amp; Workshops</title>
      <link>/talk/</link>
    </image>
    
    <item>
      <title>Markov Chain Monte Carlo</title>
      <link>/talk/2020-mcmc/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/talk/2020-mcmc/</guid>
      <description>




  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/talk/2020-mcmc/rosenbrock_huf89aabc5a006130cd2261868bbadf44c_985214_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/talk/2020-mcmc/rosenbrock_huf89aabc5a006130cd2261868bbadf44c_985214_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1918&#34; height=&#34;905&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Markov chain Monte Carlo (MCMC) is a popular method of generating correlated samples from complex multi-dimensional distributions where i.i.d sampling is not possible. MCMC finds application in a wide variety of fields, Bayesian machine learning, optimization, and econometrics, to name a few. With the recent advancements in computing power, long runs of MCMC have become accessible to practitioners with parallel implementation of MCMC emerging as a popular choice. This trend has motivated research that answers some fundamental questions pertinent to sampling. In this talk, we will give an intuitive understanding of how and why the algorithm works and what are the best practices in MCMC. Additionally, we will talk about diagnostics that determine the quality of MCMC algorithms. Towards the end, we will address the problem of output analysis of parallel MCMC by discussing globally centered Monte Carlo error estimators.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to tensorflow</title>
      <link>/talk/2019-tensorflow/</link>
      <pubDate>Sat, 04 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/talk/2019-tensorflow/</guid>
      <description>




  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/talk/2019-tensorflow/tensors_flowing_hu0bd88d939c7ea5a767fc2ea7039b1f27_443056_2000x2000_fit_lanczos.gif&#34; &gt;


  &lt;img data-src=&#34;/talk/2019-tensorflow/tensors_flowing_hu0bd88d939c7ea5a767fc2ea7039b1f27_443056_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;252&#34; height=&#34;448&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This talk was part of Programming Club, IIT Kanpur&amp;rsquo;s talk on machine learning frameworks. This talk was targeted for sophomores and junior undergraduates with some background on deep learning. It explained dataflow graphs (which represent TensorFlow computations) and its basic terminologies and functions. Next, it covered basic implementation in TensorFlow through a toy problem on MNIST dataset, including the neural network and the dataflow graph for the problem. It also covered the installation procedure of the GPU and the CPU-only version.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Deep Learning</title>
      <link>/talk/2019-basic_dl/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/talk/2019-basic_dl/</guid>
      <description>














&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;basic_ml.png&#34; &gt;


  &lt;img src=&#34;basic_ml.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This talk was part of Programming Club, IIT Kanpur&amp;rsquo;s talk on introduction to deep learning. This talk was targeted for freshers and sophomores (both UG and PG) who are starting with machine learning. It covered topics like the most popular convex cost functions, activation functions and gradient descent, along with hands-on coding experience of these topics. It served as a medium of introducing deep learning to the campus community and as a starting step to explore more complex topics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>/talk/2019-rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/talk/2019-rl/</guid>
      <description>




  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/talk/2019-rl/RL_hu28e342dfb08d6419167a58ebb9883a60_21215_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/talk/2019-rl/RL_hu28e342dfb08d6419167a58ebb9883a60_21215_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;800&#34; height=&#34;382&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This talk was part of Programming Club, IIT Kanpur&amp;rsquo;s talk on reinforcement learning (RL). It was targeted for sophomores and junior undergraduates with some statistical background on Markov process and Monte Carlo. It covered the components of an RL model, namely policy, value function and agent&amp;rsquo;s representation of the environment. Additionally, it covered the basics of Markov reward process and the Bellman expectation equation, necessary to define the update procedure of the RL agent mathematically. This talk also covered the basic algorithms of training the RL agent, namely policy and value iteration. Towards the end, it touched upon the model-free methods of RL and explained the underlying mechanism behind model-free learning.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
