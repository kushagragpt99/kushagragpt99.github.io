<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kushagra Gupta</title>
    <link>/authors/kushagra-gupta/</link>
      <atom:link href="/authors/kushagra-gupta/index.xml" rel="self" type="application/rss+xml" />
    <description>Kushagra Gupta</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Kushagra Gupta 2020</copyright><lastBuildDate>Sun, 01 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/avatar.jpeg</url>
      <title>Kushagra Gupta</title>
      <link>/authors/kushagra-gupta/</link>
    </image>
    
    <item>
      <title>Replicated Batch Means for Parallel MCMC</title>
      <link>/project/rbm/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/rbm/</guid>
      <description>&lt;p&gt;Sufficient research has been done on estimating the asymptotic covariance matrix in a Markov chain central limit theorem for applications in Markov chain Monte Carlo (MCMC). However, almost all of it, including the efficient batch means (BM) estimator, focuses on a single-chain MCMC run. We demonstrate that simply averaging covariance matrix estimators from different chains (average BM) can yield critical underestimates of the variance in small sample sizes, especially for slow mixing Markov chains.  We propose a multivariate replicated batch means (RBM) estimator that utilizes information across all chains in order to estimate the asymptotic covariance matrix, thereby correcting for the underestimation. Under weak conditions on the mixing rate of the process, strong consistency of the RBM estimator follows from the strong consistency of the BM estimators. Further, we show that the large-sample bias and variance of the RBM estimator mimics that of the average BM estimator. Thus, for large MCMC runs, the RBM and average BM yield equivalent performance, but for small MCMC runs, the RBM estimator can be dramatically superior. This superiority is demonstrated through a variety of examples, including a two-variable Gibbs sampler for a bivariate Gaussian target distribution. Here we obtain a closed-form expression for the asymptotic covariance matrix of the Monte Carlo estimator, a result vital in itself, as it allows for benchmarking implementations in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fully Homomorphic Encryptio Library</title>
      <link>/project/fhe/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/fhe/</guid>
      <description>&lt;!-- 














&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;unet.PNG&#34; &gt;


  &lt;img src=&#34;unet.PNG&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;
 --&gt;
&lt;p&gt;Fully Homomorphic Encryption (FHE) allows computation directly on the encrypted data, generating an encrypted result that matches the result of the computation on decrypted data. FHE removes the insecurity introduced by the requirement of decrypting data to perform computations on it. We build a levelled FHE library (which can evaluate circuits with a bounded depth) based on the 
&lt;a href=&#34;https://eprint.iacr.org/2013/340&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GSW scheme&lt;/a&gt; using libtorch and CUDA. Our library is capable of performing parallel computations and automatic differentiation, making it significantly fast and easy to use, particularly for machine learning models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interval Regression using Bayesian Inference</title>
      <link>/project/qr/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/project/qr/</guid>
      <description>&lt;p&gt;In Fall 2019, I was working with Prof.Vats and Prof. Arshad Rahman (in Economics department) on developing Bayesian models for interval data. I was introduced to interval regression in my summer project on Bayesian estimation of Weibull distribution while reading Prof.Kundu&amp;rsquo;s paper on interval-censored data with Weibull lifetime. I researched on various paradigms that solve interval regression, covering quantile regression (QR), meta-heuristic algorithms, information theory, convex analysis, and set arithmetic linear models. I found QR to be a promising approach as traditional QR settings can extend to interval data in the following way - we can modify the quantile loss function to have three parts based on the partition by the interval endpoints, as opposed to a binary partition by traditional QR, with certain added assumptions. The change is that the dot product of parameters and covariates lying in the interval has a zero loss as the dependent variable can lie anywhere in the interval. The assumptions can be modelled in a Bayesian setting with appropriate priors. As part of my project, I built Bayesian frameworks for various paradigms of interval regression.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Inference on 3 parameter Weibull Distribution</title>
      <link>/project/weibull/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>/project/weibull/</guid>
      <description>&lt;p&gt;In the summer of 2019, I (in a team of 2) was working with Prof.Kundu on developing Bayesian methodologies for parameter estimation of a 3-Weibull distribution \cite{weibull}. Maximum likelihood estimators are inconsistent and inefficient for this problem when the shape parameter is less than one due to the likelihood function exploding to infinity. However, the Bayesian setting with gamma priors solves this problem as the posterior breaks into a product of proper densities, which is convenient for MCMC sampling. We found the posterior to be log-concave and similar to a gamma distribution and, thus, approximated the parameters and confidence intervals using this observation. Our Bayesian implementation reduced the mean square errors significantly as compared to the maximum likelihood method.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Agent Reinforcement Learning</title>
      <link>/project/mrl/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/project/mrl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;I worked on multi-agent self-play in atari games in collaborative and competitive settings.&lt;/li&gt;
&lt;li&gt;I used variational autoencoders to disentangle multiple near-optimal policies extracted using latent code.&lt;/li&gt;
&lt;li&gt;Our initial results on the model gave win probability of 72%, which is close to 80% SOTA values, and much better than the human score of 40% in multi-agent CTF.&lt;/li&gt;
&lt;li&gt;I worked on developing a generative model for InfoRL to maintain unsupervised setting for latent code generation to allow all standard MARL algorithms to be used with InfoRL.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Data Science Hackathon, Inter IIT Tech Meeth 2018</title>
      <link>/project/techmeet/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/project/techmeet/</guid>
      <description>&lt;p&gt;Our team won the silver medal in the competition. The problem involved data analysis and inference from a dataset for a globular star cluster. The datasets had the coordinates, color index (for different wavelengths) and magnitude of every star in the cluster. The task involved using the datasets appropriately to analyze the globular cluster to determine some parameters and peculiarities of the same.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Eye in the Sky</title>
      <link>/project/eits/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/project/eits/</guid>
      <description>&lt;p&gt;




  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/project/eits/unet_hu1dbdc71f780f03386782ef847930dbf0_50546_2000x2000_fit_lanczos_2.PNG&#34; &gt;


  &lt;img data-src=&#34;/project/eits/unet_hu1dbdc71f780f03386782ef847930dbf0_50546_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;671&#34; height=&#34;468&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

The problem was part of the machine learning competition in inter IIT tech meet, 2018, where we won the silver medal in the competition. The task was to classify all pixels in a given satellite image, as belonging to one of nine given classes.  Our approach towards solving this challenge involved three approaches, with each approach performing better than the last one. The ﬁrst model involved the implementation of a basic U-Net architecture with custom metrics designed for this problem. The second approach involved a P-Net architecture with extensive hyperparameter tuning to improve model accuracy. The third approach utilizes nine distinct U-Net architectures for the segmentation, giving results comparable to the state of the art model for multi-class image segmentation.&lt;/p&gt;
&lt;p&gt;My contributions to the project :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I designed and implemented a U-Net architecture for image segmentation of high-quality satellite images by using context-based representations.&lt;/li&gt;
&lt;li&gt;I improved the existing accuracy from 84% to 91% on just 25 images by developing a new algorithm based on 9 U-Nets using &amp;lsquo;one vs all&amp;rsquo; classification approach.&lt;/li&gt;
&lt;li&gt;I used localized optimization of parameters with high frequency to break the bottleneck of a small dataset.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Autoencoder Recommendation Engine</title>
      <link>/project/kvpy/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/project/kvpy/</guid>
      <description>&lt;p&gt;




  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/project/kvpy/autoencoder_hueadd5cc5474c4e1f858717e76734844b_32234_2000x2000_fit_lanczos_2.PNG&#34; &gt;


  &lt;img data-src=&#34;/project/kvpy/autoencoder_hueadd5cc5474c4e1f858717e76734844b_32234_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;549&#34; height=&#34;352&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

Deep learning has revolutionized many areas of machine learning, and it is poised to do so with recommender systems as well. This project shows how deep autoencoders can be successfully trained even on relatively small amounts of data by using both well established (dropout) and scaled exponential linear units deep learning techniques. We used iterative output re-feeding - a technique which allows dense updates in collaborative ﬁltering, increases the learning rate and further improves generalization performance of the model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Large Scale Logistics Platform</title>
      <link>/project/nyo/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/project/nyo/</guid>
      <description>&lt;p&gt;Sub-project :  ​​An online recommendation system based on collaborative filtering for implicit data using sentiment and frequency dependent weighting schemes. &lt;br&gt;
Technical details :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implemented a state of the art algorithm for online collaborative filtering based on Fast Matrix Factorization for Online Recommendation with Implicit Feedback (He et al.) using Numpy.&lt;/li&gt;
&lt;li&gt;Integrated element-wise Alternating Least Squares (eALS) based incremental update strategy for online learning.&lt;/li&gt;
&lt;li&gt;Developed an online collaborative filtering based deep recommender algorithm based on AutoEncoder in tensorflow.&lt;/li&gt;
&lt;li&gt;Used the VADER model in NLTK for sentiment analysis of comments.&lt;/li&gt;
&lt;li&gt;Improved results of algorithm by using interaction count and sentiment dependent weighting scheme for the observed data and a frequency aware weighting scheme for the missing data.&lt;/li&gt;
&lt;li&gt;Built multiple Kafka consumers and producer for parallely consuming real time interaction data of comments, likes and views to produce online recommendations for users.&lt;/li&gt;
&lt;li&gt;Used locust to simulate parallel user interaction to test recommendation algorithm.&lt;/li&gt;
&lt;li&gt;Used an eventually consistent engagement database (Couchbase) for storing user and item based data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sub-Project: ​​Identification and Classification of toxic comments.
Technical Details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implemented  a Bidirectional LSTM based model using Keras for flagging toxic comments based on six metrics.&lt;/li&gt;
&lt;li&gt;Built Kafka consumer and producer data-pipelines for recording and processing new comments.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
