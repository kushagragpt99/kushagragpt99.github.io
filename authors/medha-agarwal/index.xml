<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Medha Agarwal | Kushagra Gupta</title>
    <link>/authors/medha-agarwal/</link>
      <atom:link href="/authors/medha-agarwal/index.xml" rel="self" type="application/rss+xml" />
    <description>Medha Agarwal</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Kushagra Gupta 2020</copyright><lastBuildDate>Sat, 03 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/avatar.jpeg</url>
      <title>Medha Agarwal</title>
      <link>/authors/medha-agarwal/</link>
    </image>
    
    <item>
      <title>Markov Chain Monte Carlo</title>
      <link>/talk/2020-mcmc/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/talk/2020-mcmc/</guid>
      <description>




  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/talk/2020-mcmc/rosenbrock_huf89aabc5a006130cd2261868bbadf44c_985214_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/talk/2020-mcmc/rosenbrock_huf89aabc5a006130cd2261868bbadf44c_985214_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1918&#34; height=&#34;905&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Markov chain Monte Carlo (MCMC) is a popular method of generating correlated samples from complex multi-dimensional distributions where i.i.d sampling is not possible. MCMC finds application in a wide variety of fields, Bayesian machine learning, optimization, and econometrics, to name a few. With the recent advancements in computing power, long runs of MCMC have become accessible to practitioners with parallel implementation of MCMC emerging as a popular choice. This trend has motivated research that answers some fundamental questions pertinent to sampling. In this talk, we will give an intuitive understanding of how and why the algorithm works and what are the best practices in MCMC. Additionally, we will talk about diagnostics that determine the quality of MCMC algorithms. Towards the end, we will address the problem of output analysis of parallel MCMC by discussing globally centered Monte Carlo error estimators.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Inference on 3 parameter Weibull Distribution</title>
      <link>/project/weibull/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>/project/weibull/</guid>
      <description>&lt;p&gt;In the summer of 2019, I (in a team of 2) was working with Prof.Kundu on developing Bayesian methodologies for parameter estimation of a 3-Weibull distribution \cite{weibull}. Maximum likelihood estimators are inconsistent and inefficient for this problem when the shape parameter is less than one due to the likelihood function exploding to infinity. However, the Bayesian setting with gamma priors solves this problem as the posterior breaks into a product of proper densities, which is convenient for MCMC sampling. We found the posterior to be log-concave and similar to a gamma distribution and, thus, approximated the parameters and confidence intervals using this observation. Our Bayesian implementation reduced the mean square errors significantly as compared to the maximum likelihood method.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
