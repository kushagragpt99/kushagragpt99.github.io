<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Utsav Singh | Kushagra Gupta</title>
    <link>https://kushagragpt99.github.io/authors/utsav-singh/</link>
      <atom:link href="https://kushagragpt99.github.io/authors/utsav-singh/index.xml" rel="self" type="application/rss+xml" />
    <description>Utsav Singh</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Kushagra Gupta 2020</copyright><lastBuildDate>Mon, 01 Apr 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kushagragpt99.github.io/img/pom-card.png</url>
      <title>Utsav Singh</title>
      <link>https://kushagragpt99.github.io/authors/utsav-singh/</link>
    </image>
    
    <item>
      <title>Multi-Agent Reinforcement Learning</title>
      <link>https://kushagragpt99.github.io/project/mrl/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://kushagragpt99.github.io/project/mrl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;I worked on multi-agent self-play in atari games in collaborative and competitive settings.&lt;/li&gt;
&lt;li&gt;I used variational autoencoders to disentangle multiple near-optimal policies extracted using latent code.&lt;/li&gt;
&lt;li&gt;Our initial results on the model gave win probability of 72%, which is close to 80% SOTA values, and much better than the human score of 40% in multi-agent CTF.&lt;/li&gt;
&lt;li&gt;I worked on developing a generative model for InfoRL to maintain unsupervised setting for latent code generation to allow all standard MARL algorithms to be used with InfoRL.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
