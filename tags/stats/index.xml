<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stats | Kushagra Gupta</title>
    <link>/tags/stats/</link>
      <atom:link href="/tags/stats/index.xml" rel="self" type="application/rss+xml" />
    <description>stats</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Kushagra Gupta 2020</copyright><lastBuildDate>Sun, 01 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/pom-card.png</url>
      <title>stats</title>
      <link>/tags/stats/</link>
    </image>
    
    <item>
      <title>Replicated Batch Means for Parallel MCMC</title>
      <link>/project/rbm/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/rbm/</guid>
      <description>&lt;p&gt;Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating expectations concerning a target distribution. Output analysis of these estimates requires estimating the MCMC central limit theorem asymptotic variance. We study a steady-state simulation estimator called replicated batch means (RBM) and their lugsail counterparts, designed explicitly for producing accurate estimates for small sample settings for parallel Markov chains. We prove that the RBM class of estimators show strong consistency and link its conditions to strong consistency of single-chain batch means estimators. We obtain the bias and variance expressions for the replicated estimators and study their asymptotic properties. We also derive the exact form of the asymptotic covariance matrix for a bivariate normal Gibbs sampler. Finite sample superiority of RBM over other estimators is illustrated through a highly correlated bivariate normal Gibbs sampler.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interval Regression using Bayesian Inference</title>
      <link>/project/qr/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/project/qr/</guid>
      <description>&lt;p&gt;In Fall 2019, I was working with Prof.Vats and Prof. Arshad Rahman (in Economics department) on developing Bayesian models for interval data. I was introduced to interval regression in my summer project on Bayesian estimation of Weibull distribution while reading Prof.Kundu&amp;rsquo;s paper on interval-censored data with Weibull lifetime. I researched on various paradigms that solve interval regression, covering quantile regression (QR), meta-heuristic algorithms, information theory, convex analysis, and set arithmetic linear models. I found QR to be a promising approach as traditional QR settings can extend to interval data in the following way - we can modify the quantile loss function to have three parts based on the partition by the interval endpoints, as opposed to a binary partition by traditional QR, with certain added assumptions. The change is that the dot product of parameters and covariates lying in the interval has a zero loss as the dependent variable can lie anywhere in the interval. The assumptions can be modelled in a Bayesian setting with appropriate priors. As part of my project, I built Bayesian frameworks for various paradigms of interval regression.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Inference on 3 parameter Weibull Distribution</title>
      <link>/project/weibull/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>/project/weibull/</guid>
      <description>&lt;p&gt;In the summer of 2019, I (in a team of 2) was working with Prof.Kundu on developing Bayesian methodologies for parameter estimation of a 3-Weibull distribution \cite{weibull}. Maximum likelihood estimators are inconsistent and inefficient for this problem when the shape parameter is less than one due to the likelihood function exploding to infinity. However, the Bayesian setting with gamma priors solves this problem as the posterior breaks into a product of proper densities, which is convenient for MCMC sampling. We found the posterior to be log-concave and similar to a gamma distribution and, thus, approximated the parameters and confidence intervals using this observation. Our Bayesian implementation reduced the mean square errors significantly as compared to the maximum likelihood method.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
